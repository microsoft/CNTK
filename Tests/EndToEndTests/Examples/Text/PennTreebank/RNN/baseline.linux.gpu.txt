=== Running /home/alrezni/src/cntk_git/build/release/bin/cntk configFile=/home/alrezni/src/cntk_git/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN/../../../../../../Examples/Text/PennTreebank/Config/rnn.cntk currentDirectory=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data RunDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu DataDir=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data ConfigDir=/home/alrezni/src/cntk_git/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN/../../../../../../Examples/Text/PennTreebank/Config OutputDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu DeviceId=0 timestamping=true initOnCPUOnly=true command=writeWordAndClassInfo:train:test train=[SGD=[maxEpochs=3]] train=[epochSize=2048]] test=[SGD=[maxEpochs=3]] train=[epochSize=2048]]
-------------------------------------------------------------------
Build info: 

		Built time: Apr 14 2016 14:53:37
		Last modified date: Tue Apr  5 16:01:37 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: alrezni/examples_text
		Build SHA1: f10be05b126cd5e74268ba4d2219087ebc0e9dc1 (modified)
		Built by alrezni on atleneu04
		Build Path: /home/alrezni/src/cntk_git
-------------------------------------------------------------------
Changed current directory to /home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data
04/15/2016 13:15:32: -------------------------------------------------------------------
04/15/2016 13:15:32: Build info: 

04/15/2016 13:15:32: 		Built time: Apr 14 2016 14:53:37
04/15/2016 13:15:32: 		Last modified date: Tue Apr  5 16:01:37 2016
04/15/2016 13:15:32: 		Build type: release
04/15/2016 13:15:32: 		Build target: GPU
04/15/2016 13:15:32: 		With 1bit-SGD: no
04/15/2016 13:15:32: 		Math lib: acml
04/15/2016 13:15:32: 		CUDA_PATH: /usr/local/cuda-7.0
04/15/2016 13:15:32: 		CUB_PATH: /usr/local/cub-1.4.1
04/15/2016 13:15:32: 		CUDNN_PATH: /usr/local/cudnn-4.0
04/15/2016 13:15:32: 		Build Branch: alrezni/examples_text
04/15/2016 13:15:32: 		Build SHA1: f10be05b126cd5e74268ba4d2219087ebc0e9dc1 (modified)
04/15/2016 13:15:32: 		Built by alrezni on atleneu04
04/15/2016 13:15:32: 		Build Path: /home/alrezni/src/cntk_git
04/15/2016 13:15:32: -------------------------------------------------------------------

04/15/2016 13:15:32: Running on localhost at 2016/04/15 13:15:32
04/15/2016 13:15:32: Command line: 
/home/alrezni/src/cntk_git/build/release/bin/cntk  configFile=/home/alrezni/src/cntk_git/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN/../../../../../../Examples/Text/PennTreebank/Config/rnn.cntk  currentDirectory=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data  RunDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu  DataDir=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data  ConfigDir=/home/alrezni/src/cntk_git/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN/../../../../../../Examples/Text/PennTreebank/Config  OutputDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu  DeviceId=0  timestamping=true  initOnCPUOnly=true  command=writeWordAndClassInfo:train:test  train=[SGD=[maxEpochs=3]]  train=[epochSize=2048]]  test=[SGD=[maxEpochs=3]]  train=[epochSize=2048]]



04/15/2016 13:15:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
04/15/2016 13:15:32: RootDir = ".."
ConfigDir = "$RootDir$/Config"
DataDir   = "$RootDir$/Data"
OutputDir = "$RootDir$/Output"
ModelDir  = "$OutputDir$/Models"
deviceId = "auto"
command = writeWordAndClassInfo:train:test:write
precision  = "float"
traceLevel = 1
modelPath  = "$ModelDir$/rnn.dnn"
numCPUThreads = 1
confVocabSize = 10000
confClassSize = 50
trainFile = "ptb.train.txt"
validFile = "ptb.valid.txt"
testFile  = "ptb.test.txt"
writeWordAndClassInfo = [
    action = "writeWordAndClass"
    inputFile = "$DataDir$/$trainFile$"
    beginSequence = "</s>"
    endSequence   = "</s>"
    outputVocabFile = "$ModelDir$/vocab.txt"
    outputWord2Cls  = "$ModelDir$/word2cls.txt"
    outputCls2Index = "$ModelDir$/cls2idx.txt"
    vocabSize = "$confVocabSize$"
    nbrClass = "$confClassSize$"
    cutoff = 0
    printValues = true
]
train = [
    action = "train"
    traceLevel = 1
epochSize = 0               
    SimpleNetworkBuilder = [
rnnType = "CLASSLSTM"   
recurrentLayer = 1      
        trainingCriterion = "classCrossEntropyWithSoftmax"
        evalCriterion     = "classCrossEntropyWithSoftmax"
        initValueScale = 6.0
        uniformInit = true
        layerSizes = "$confVocabSize$:150:200:10000"
defaultHiddenActivity = 0.1 
        addPrior = false
        addDropoutNodes = false
        applyMeanVarNorm = false
lookupTableOrder = 1        
        vocabSize = "$confVocabSize$"
        nbrClass  = "$confClassSize$"
    ]
    SGD = [
        minibatchSize = 128:256:512
        learningRatesPerSample = 0.1
        momentumPerMB = 0
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0
        maxEpochs = 16
        numMBsToShowResult = 100
        gradUpdateType = "none"
        loadBestModel = true
        dropoutRate = 0.0
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$trainFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11                
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = categoryLabels
            ]
        ]
    ]
    cvReader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.valid.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$validFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
test = [
    action = "eval"
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$testFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
write = [
    action = "write"
    outputPath = "$OutputDir$/Write"
outputNodeNames = TrainNodeClassBasedCrossEntropy 
    format = [
sequencePrologue = "log P(W)="    
        type = "real"
    ]
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 1  
cacheBlockSize = 1              
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$testFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
currentDirectory=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data
RunDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu
DataDir=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data
ConfigDir=/home/alrezni/src/cntk_git/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN/../../../../../../Examples/Text/PennTreebank/Config
OutputDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu
DeviceId=0
timestamping=true
initOnCPUOnly=true
command=writeWordAndClassInfo:train:test
train=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]
test=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]

04/15/2016 13:15:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

04/15/2016 13:15:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
04/15/2016 13:15:32: RootDir = ".."
ConfigDir = "../Config"
DataDir   = "../Data"
OutputDir = "../Output"
ModelDir  = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models"
deviceId = "auto"
command = writeWordAndClassInfo:train:test:write
precision  = "float"
traceLevel = 1
modelPath  = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/rnn.dnn"
numCPUThreads = 1
confVocabSize = 10000
confClassSize = 50
trainFile = "ptb.train.txt"
validFile = "ptb.valid.txt"
testFile  = "ptb.test.txt"
writeWordAndClassInfo = [
    action = "writeWordAndClass"
    inputFile = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.train.txt"
    beginSequence = "</s>"
    endSequence   = "</s>"
    outputVocabFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
    outputWord2Cls  = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/word2cls.txt"
    outputCls2Index = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/cls2idx.txt"
    vocabSize = "10000"
    nbrClass = "50"
    cutoff = 0
    printValues = true
]
train = [
    action = "train"
    traceLevel = 1
epochSize = 0               
    SimpleNetworkBuilder = [
rnnType = "CLASSLSTM"   
recurrentLayer = 1      
        trainingCriterion = "classCrossEntropyWithSoftmax"
        evalCriterion     = "classCrossEntropyWithSoftmax"
        initValueScale = 6.0
        uniformInit = true
        layerSizes = "10000:150:200:10000"
defaultHiddenActivity = 0.1 
        addPrior = false
        addDropoutNodes = false
        applyMeanVarNorm = false
lookupTableOrder = 1        
        vocabSize = "10000"
        nbrClass  = "50"
    ]
    SGD = [
        minibatchSize = 128:256:512
        learningRatesPerSample = 0.1
        momentumPerMB = 0
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0
        maxEpochs = 16
        numMBsToShowResult = 100
        gradUpdateType = "none"
        loadBestModel = true
        dropoutRate = 0.0
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.train.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11                
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = categoryLabels
            ]
        ]
    ]
    cvReader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.valid.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.valid.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
test = [
    action = "eval"
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
write = [
    action = "write"
    outputPath = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Write"
outputNodeNames = TrainNodeClassBasedCrossEntropy 
    format = [
sequencePrologue = "log P(W)="    
        type = "real"
    ]
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 1  
cacheBlockSize = 1              
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
currentDirectory=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data
RunDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu
DataDir=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data
ConfigDir=/home/alrezni/src/cntk_git/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN/../../../../../../Examples/Text/PennTreebank/Config
OutputDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu
DeviceId=0
timestamping=true
initOnCPUOnly=true
command=writeWordAndClassInfo:train:test
train=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]
test=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]

04/15/2016 13:15:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

04/15/2016 13:15:32: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: rnn.cntk:]=true
configparameters: rnn.cntk:command=writeWordAndClassInfo:train:test
configparameters: rnn.cntk:confClassSize=50
configparameters: rnn.cntk:ConfigDir=/home/alrezni/src/cntk_git/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN/../../../../../../Examples/Text/PennTreebank/Config
configparameters: rnn.cntk:confVocabSize=10000
configparameters: rnn.cntk:currentDirectory=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data
configparameters: rnn.cntk:DataDir=/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data
configparameters: rnn.cntk:deviceId=0
configparameters: rnn.cntk:initOnCPUOnly=true
configparameters: rnn.cntk:ModelDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models
configparameters: rnn.cntk:modelPath=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/rnn.dnn
configparameters: rnn.cntk:numCPUThreads=1
configparameters: rnn.cntk:OutputDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu
configparameters: rnn.cntk:precision=float
configparameters: rnn.cntk:RootDir=..
configparameters: rnn.cntk:RunDir=/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu
configparameters: rnn.cntk:test=[
    action = "eval"
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
] [SGD=[maxEpochs=3]]

configparameters: rnn.cntk:testFile=ptb.test.txt
configparameters: rnn.cntk:timestamping=true
configparameters: rnn.cntk:traceLevel=1
configparameters: rnn.cntk:train=[
    action = "train"
    traceLevel = 1
epochSize = 0               
    SimpleNetworkBuilder = [
rnnType = "CLASSLSTM"   
recurrentLayer = 1      
        trainingCriterion = "classCrossEntropyWithSoftmax"
        evalCriterion     = "classCrossEntropyWithSoftmax"
        initValueScale = 6.0
        uniformInit = true
        layerSizes = "10000:150:200:10000"
defaultHiddenActivity = 0.1 
        addPrior = false
        addDropoutNodes = false
        applyMeanVarNorm = false
lookupTableOrder = 1        
        vocabSize = "10000"
        nbrClass  = "50"
    ]
    SGD = [
        minibatchSize = 128:256:512
        learningRatesPerSample = 0.1
        momentumPerMB = 0
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0
        maxEpochs = 16
        numMBsToShowResult = 100
        gradUpdateType = "none"
        loadBestModel = true
        dropoutRate = 0.0
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.train.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11                
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = categoryLabels
            ]
        ]
    ]
    cvReader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.valid.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.valid.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
] [SGD=[maxEpochs=3]] [epochSize=2048] [epochSize=2048]

configparameters: rnn.cntk:trainFile=ptb.train.txt
configparameters: rnn.cntk:validFile=ptb.valid.txt
configparameters: rnn.cntk:write=[
    action = "write"
    outputPath = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Write"
outputNodeNames = TrainNodeClassBasedCrossEntropy 
    format = [
sequencePrologue = "log P(W)="    
        type = "real"
    ]
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 1  
cacheBlockSize = 1              
        wordclass = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]

configparameters: rnn.cntk:writeWordAndClassInfo=[
    action = "writeWordAndClass"
    inputFile = "/home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.train.txt"
    beginSequence = "</s>"
    endSequence   = "</s>"
    outputVocabFile = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt"
    outputWord2Cls  = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/word2cls.txt"
    outputCls2Index = "/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/cls2idx.txt"
    vocabSize = "10000"
    nbrClass = "50"
    cutoff = 0
    printValues = true
]

04/15/2016 13:15:32: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
04/15/2016 13:15:32: Commands: writeWordAndClassInfo train test
04/15/2016 13:15:32: Precision = "float"
04/15/2016 13:15:32: Using 1 CPU threads.
04/15/2016 13:15:32: CNTKModelPath: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/rnn.dnn
04/15/2016 13:15:32: CNTKCommandTrainInfo: train : 3
04/15/2016 13:15:32: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3

04/15/2016 13:15:32: ##############################################################################
04/15/2016 13:15:32: #                                                                            #
04/15/2016 13:15:32: # Action "writeWordAndClass"                                                 #
04/15/2016 13:15:32: #                                                                            #
04/15/2016 13:15:32: ##############################################################################

Vocabulary file    --> /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/vocab.txt
Word-to-class map  --> /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/word2cls.txt
Class-to-index map --> /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/cls2idx.txt

Reading input file inputFile: /home/alrezni/src/cntk_git/Examples/Text/PennTreebank/Data/ptb.train.txt
Vocabulary size 10000.
Created vocabulary file with 10000 entries.
Created word-to-class map with 10000 entries.
Created class-to-index map with 50 entries.

04/15/2016 13:15:32: Action "writeWordAndClass" complete.


04/15/2016 13:15:32: ##############################################################################
04/15/2016 13:15:32: #                                                                            #
04/15/2016 13:15:32: # Action "train"                                                             #
04/15/2016 13:15:32: #                                                                            #
04/15/2016 13:15:32: ##############################################################################

04/15/2016 13:15:32: CNTKCommandTrainBegin: train
SimpleNetworkBuilder Using GPU 0
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt

04/15/2016 13:15:32: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	PosteriorProb = Softmax()
	TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax()
	outputs = TransposeTimes()

Loop[0] --> Loop_AutoName37 -> 31 nodes

	AutoName2	AutoName30	AutoName33
	AutoName1	AutoName21	AutoName24
	AutoName5	AutoName20	AutoName25
	AutoName26	AutoName6	AutoName27
	AutoName0	AutoName8	AutoName11
	AutoName4	AutoName7	AutoName12
	AutoName13	AutoName3	AutoName14
	AutoName15	AutoName17	AutoName18
	AutoName19	AutoName28	AutoName29
	AutoName34	AutoName35	AutoName36
	AutoName37

Validating network. 63 nodes to process in pass 1.


Validating network. 43 nodes to process in pass 2.


Validating network. 14 nodes to process in pass 3.


Validating network, final pass.

Validating --> W2 = LearnableParameter() :  -> [200 x 10000]
Validating --> WXO0 = LearnableParameter() :  -> [200 x 150]
Validating --> E0 = LearnableParameter() :  -> [150 x 10000]
Validating --> features = SparseInputValue() :  -> [10000 x *]
Validating --> LookupTable = LookupTable (E0, features) : [150 x 10000], [10000 x *] -> [150 x *]
Validating --> AutoName31 = Times (WXO0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bo0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName32 = Plus (AutoName31, bo0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHO0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCO0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXF0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName22 = Times (WXF0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bf0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName23 = Plus (AutoName22, bf0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHF0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCF0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXI0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName9 = Times (WXI0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bi0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName10 = Plus (AutoName9, bi0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHI0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCI0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXC0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName16 = Times (WXC0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> WHC0 = LearnableParameter() :  -> [200 x 200]
Validating --> bc0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName2 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName30 = Times (WHO0, AutoName2) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName33 = Plus (AutoName32, AutoName30) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName1 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName21 = Times (WHF0, AutoName1) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName24 = Plus (AutoName23, AutoName21) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName5 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName20 = DiagTimes (WCF0, AutoName5) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName25 = Plus (AutoName24, AutoName20) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName26 = Sigmoid (AutoName25) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName6 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName27 = ElementTimes (AutoName26, AutoName6) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName0 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName8 = Times (WHI0, AutoName0) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName11 = Plus (AutoName10, AutoName8) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName4 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName7 = DiagTimes (WCI0, AutoName4) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName12 = Plus (AutoName11, AutoName7) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName13 = Sigmoid (AutoName12) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName3 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName14 = Times (WHC0, AutoName3) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName15 = Plus (AutoName14, bc0) : [200 x 1 x *], [200 x 1] -> [200 x 1 x *]
Validating --> AutoName17 = Plus (AutoName16, AutoName15) : [200 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName18 = Tanh (AutoName17) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName19 = ElementTimes (AutoName13, AutoName18) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName28 = Plus (AutoName27, AutoName19) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName29 = DiagTimes (WCO0, AutoName28) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName34 = Plus (AutoName33, AutoName29) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName35 = Sigmoid (AutoName34) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName36 = Tanh (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName37 = ElementTimes (AutoName35, AutoName36) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> outputs = TransposeTimes (W2, AutoName37) : [200 x 10000], [200 x 1 x *] -> [10000 x 1 x *]
Validating --> PosteriorProb = Softmax (outputs) : [10000 x 1 x *] -> [10000 x 1 x *]
Validating --> labels = InputValue() :  -> [4 x *]
Validating --> WeightForClassPostProb = LearnableParameter() :  -> [50 x 200]
Validating --> ClassPostProb = Times (WeightForClassPostProb, AutoName37) : [50 x 200], [200 x 1 x *] -> [50 x 1 x *]
Validating --> TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax (labels, AutoName37, W2, ClassPostProb) : [4 x *], [200 x 1 x *], [200 x 10000], [50 x 1 x *] -> [1]


19 out of 63 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/15/2016 13:15:33: Created model with 63 nodes on GPU 0.

04/15/2016 13:15:33: Training criterion node(s):
04/15/2016 13:15:33: 	TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax


Allocating matrices for forward and/or backward propagation.
04/15/2016 13:15:33: No PreCompute nodes found, skipping PreCompute step.

04/15/2016 13:15:33: Starting Epoch 1: learning rate per sample = 0.100000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

04/15/2016 13:15:33: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
WARNING: The same matrix with dim [4, 108] has been transferred between different devices for 20 times.
04/15/2016 13:15:34: Finished Epoch[ 1 of 3]: [Training] CrossEntropyWithSoftmax = 6.8356953; TotalSamplesSeen = 2087; learningRatePerSample = 0.1; EpochTime=0.875219
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
Final Results: Minibatch[1-704]: * 73760    TrainNodeClassBasedCrossEntropy: ClassBasedCrossEntropyWithSoftmax/Sample = 6.5392296    perplexity = 691.75344    
04/15/2016 13:15:45: Finished Epoch[ 1 of 3]: [Validate] CrossEntropyWithSoftmax = 6.5392296
04/15/2016 13:15:45: SGD: Saving checkpoint model '/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/rnn.dnn.1'

04/15/2016 13:15:45: Starting Epoch 2: learning rate per sample = 0.100000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

04/15/2016 13:15:45: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
04/15/2016 13:15:46: Finished Epoch[ 2 of 3]: [Training] CrossEntropyWithSoftmax = 6.6260695; TotalSamplesSeen = 4186; learningRatePerSample = 0.1; EpochTime=0.602958
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
Final Results: Minibatch[1-353]: * 73760    TrainNodeClassBasedCrossEntropy: ClassBasedCrossEntropyWithSoftmax/Sample = 6.5062633    perplexity = 669.32068    
04/15/2016 13:15:54: Finished Epoch[ 2 of 3]: [Validate] CrossEntropyWithSoftmax = 6.5062633
04/15/2016 13:15:54: SGD: Saving checkpoint model '/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/rnn.dnn.2'

04/15/2016 13:15:54: Starting Epoch 3: learning rate per sample = 0.100000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

04/15/2016 13:15:54: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
04/15/2016 13:15:55: Finished Epoch[ 3 of 3]: [Training] CrossEntropyWithSoftmax = 8.0763569; TotalSamplesSeen = 6462; learningRatePerSample = 0.1; EpochTime=0.528292
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
Final Results: Minibatch[1-193]: * 73760    TrainNodeClassBasedCrossEntropy: ClassBasedCrossEntropyWithSoftmax/Sample = 9.0298885    perplexity = 8348.9282    
04/15/2016 13:16:02: Finished Epoch[ 3 of 3]: [Validate] CrossEntropyWithSoftmax = 9.0298885
04/15/2016 13:16:02: Loading previous model with best training-criterion value: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/rnn.dnn.2.
04/15/2016 13:16:02: learnRatePerSample reduced to 0.050000001
04/15/2016 13:16:02: SGD: Saving checkpoint model '/tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/Models/rnn.dnn'
04/15/2016 13:16:02: CNTKCommandTrainEnd: train

04/15/2016 13:16:02: Action "train" complete.


04/15/2016 13:16:02: ##############################################################################
04/15/2016 13:16:02: #                                                                            #
04/15/2016 13:16:02: # Action "eval"                                                              #
04/15/2016 13:16:02: #                                                                            #
04/15/2016 13:16:02: ##############################################################################

LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160415131349.795333/Examples/Text/PennTreebank_RNN@release_gpu/sentenceLabels.out.txt

Post-processing network...

3 roots:
	PosteriorProb = Softmax()
	TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax()
	outputs = TransposeTimes()

Loop[0] --> Loop_AutoName37 -> 31 nodes

	AutoName2	AutoName30	AutoName33
	AutoName1	AutoName21	AutoName24
	AutoName5	AutoName20	AutoName25
	AutoName26	AutoName6	AutoName27
	AutoName0	AutoName8	AutoName11
	AutoName4	AutoName7	AutoName12
	AutoName13	AutoName3	AutoName14
	AutoName15	AutoName17	AutoName18
	AutoName19	AutoName28	AutoName29
	AutoName34	AutoName35	AutoName36
	AutoName37

Validating network. 63 nodes to process in pass 1.


Validating network. 43 nodes to process in pass 2.


Validating network. 14 nodes to process in pass 3.


Validating network, final pass.

Validating --> W2 = LearnableParameter() :  -> [200 x 10000]
Validating --> WXO0 = LearnableParameter() :  -> [200 x 150]
Validating --> E0 = LearnableParameter() :  -> [150 x 10000]
Validating --> features = SparseInputValue() :  -> [10000 x *]
Validating --> LookupTable = LookupTable (E0, features) : [150 x 10000], [10000 x *] -> [150 x *]
Validating --> AutoName31 = Times (WXO0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bo0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName32 = Plus (AutoName31, bo0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHO0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCO0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXF0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName22 = Times (WXF0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bf0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName23 = Plus (AutoName22, bf0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHF0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCF0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXI0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName9 = Times (WXI0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bi0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName10 = Plus (AutoName9, bi0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHI0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCI0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXC0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName16 = Times (WXC0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> WHC0 = LearnableParameter() :  -> [200 x 200]
Validating --> bc0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName2 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName30 = Times (WHO0, AutoName2) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName33 = Plus (AutoName32, AutoName30) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName1 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName21 = Times (WHF0, AutoName1) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName24 = Plus (AutoName23, AutoName21) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName5 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName20 = DiagTimes (WCF0, AutoName5) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName25 = Plus (AutoName24, AutoName20) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName26 = Sigmoid (AutoName25) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName6 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName27 = ElementTimes (AutoName26, AutoName6) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName0 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName8 = Times (WHI0, AutoName0) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName11 = Plus (AutoName10, AutoName8) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName4 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName7 = DiagTimes (WCI0, AutoName4) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName12 = Plus (AutoName11, AutoName7) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName13 = Sigmoid (AutoName12) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName3 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName14 = Times (WHC0, AutoName3) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName15 = Plus (AutoName14, bc0) : [200 x 1 x *], [200 x 1] -> [200 x 1 x *]
Validating --> AutoName17 = Plus (AutoName16, AutoName15) : [200 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName18 = Tanh (AutoName17) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName19 = ElementTimes (AutoName13, AutoName18) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName28 = Plus (AutoName27, AutoName19) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName29 = DiagTimes (WCO0, AutoName28) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName34 = Plus (AutoName33, AutoName29) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName35 = Sigmoid (AutoName34) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName36 = Tanh (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName37 = ElementTimes (AutoName35, AutoName36) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> outputs = TransposeTimes (W2, AutoName37) : [200 x 10000], [200 x 1 x *] -> [10000 x 1 x *]
Validating --> PosteriorProb = Softmax (outputs) : [10000 x 1 x *] -> [10000 x 1 x *]
Validating --> labels = InputValue() :  -> [4 x *]
Validating --> WeightForClassPostProb = LearnableParameter() :  -> [50 x 200]
Validating --> ClassPostProb = Times (WeightForClassPostProb, AutoName37) : [50 x 200], [200 x 1 x *] -> [50 x 1 x *]
Validating --> TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax (labels, AutoName37, W2, ClassPostProb) : [4 x *], [200 x 1 x *], [200 x 10000], [50 x 1 x *] -> [1]


19 out of 63 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.
LMSequenceReader: Reading epoch data... 3760 sequences read.
WARNING: The same matrix with dim [4, 1872] has been transferred between different devices for 20 times.
LMSequenceReader: Reading epoch data... 0 sequences read.
Minibatch[1-60]: * 82402    TrainNodeClassBasedCrossEntropy: ClassBasedCrossEntropyWithSoftmax/Sample = 6.4614566    
Final Results: Minibatch[1-60]: * 82402    TrainNodeClassBasedCrossEntropy: ClassBasedCrossEntropyWithSoftmax/Sample = 6.4614566    perplexity = 639.9926    

04/15/2016 13:16:09: Action "eval" complete.

04/15/2016 13:16:09: __COMPLETED__
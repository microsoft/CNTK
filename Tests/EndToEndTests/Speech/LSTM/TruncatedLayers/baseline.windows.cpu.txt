-------------------------------------------------------------------
Build info: 

		Built time: Aug 24 2016 18:50:02
		Last modified date: Mon Aug 22 13:48:06 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: yes
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: c:\src\cub-1.4.1\
		CUDNN_PATH: C:\NVIDIA\cudnn-5.0\cuda
		Build Branch: fseide/cudnn5
		Build SHA1: 5f14fcaea07dc1d87a1841115818ca6065ae69f8
		Built by fseide on FSEIDE-GPU
		Build Path: C:\work\cntk\Source\CNTK\
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 4608; computeCapability = 5.2; type = "GeForce GTX TITAN X"; memory = 12288 MB
		Device[1]: cores = 4608; computeCapability = 5.2; type = "GeForce GTX TITAN X"; memory = 12288 MB
		Device[2]: cores = 4608; computeCapability = 5.2; type = "GeForce GTX TITAN X"; memory = 12288 MB
		Device[3]: cores = 4608; computeCapability = 5.2; type = "GeForce GTX TITAN X"; memory = 12288 MB
		Device[4]: cores = 4608; computeCapability = 5.2; type = "GeForce GTX TITAN X"; memory = 12288 MB
		Device[5]: cores = 4608; computeCapability = 5.2; type = "GeForce GTX TITAN X"; memory = 12288 MB
-------------------------------------------------------------------

Running on FSEIDE-GPU at 2016/08/24 19:47:06
Command line: 
..\..\..\..\x64\Release\CNTK.exe  currentDirectory=c:\work\CNTK\Tests\EndToEndTests\Speech\Data  configFile=c:\work\CNTK\Tests\EndToEndTests\Speech\LSTM\cntk.cntk  RunDir=c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance  DataDir=.  DeviceId=auto  makeMode=false  stderr=x  modelSelector=1


Configuration After Processing and Variable Resolution:

configparameters: cntk.cntk:// Note: These options are overridden from the command line in some test cases.=true
configparameters: cntk.cntk:command=speechTrain
configparameters: cntk.cntk:currentDirectory=c:\work\CNTK\Tests\EndToEndTests\Speech\Data
configparameters: cntk.cntk:DataDir=.
configparameters: cntk.cntk:deviceId=auto
configparameters: cntk.cntk:frameMode=false
configparameters: cntk.cntk:makeMode=false
configparameters: cntk.cntk:modelSelector=1
configparameters: cntk.cntk:parallelTrain=false
configparameters: cntk.cntk:precision=float
configparameters: cntk.cntk:RunDir=c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance
configparameters: cntk.cntk:speechTrain={
    action = "train"
    modelPath = "c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance/models/cntkSpeech.dnn"
    BrainScriptNetworkBuilder = {
        useLayerNorm = true
        // dimensions (needed for both model and readers)
        baseFeatDim = 33
        featDim = 11 * baseFeatDim
        labelDim = 132
        // hidden dimensions
        innerCellDim  = 1024
        hiddenDim     = 256
        numLSTMLayers = 3        // number of hidden LSTM model layers
        modelUsingCuDNN5 = Sequential
        (
            MeanVarNorm :
            (_ => OptimizedRNNStack(ParameterTensor {0:0, initOutputRank=-1, init='heNormal', initValueScale=1/10}, _, hiddenDim, numLayers=numLSTMLayers, bidirectional=true)) :
            DenseLayer {labelDim, init='heUniform', initValueScale=1/3}
        )
        modelUsingLayersLikeCuDNN5 = Sequential
        (
            MeanVarNorm :
            LayerStack {numLSTMLayers, _ => Sequential (
                (x => Splice (
                    RecurrentLSTMLayer {hiddenDim, init='heUniform', initValueScale=1/10} (x) :
                    RecurrentLSTMLayer {hiddenDim, goBackwards=true, init='heUniform', initValueScale=1/10} (x)
                ))
            )} :
            DenseLayer {labelDim, init='heUniform', initValueScale=1/3}
        )
        modelUsingLayers = Sequential
        (
            MeanVarNorm :
            LayerStack {numLSTMLayers, _ => Sequential (
                if useLayerNorm then LayerNormalizationLayer{} else Identity :
                RecurrentLSTMLayer {hiddenDim, cellShape=innerCellDim, init='heUniform', initValueScale=1/3}
            )} :
            DenseLayer {labelDim, init='heUniform', initValueScale=1/3}
        )
        modelRegressionTest (features) =
        {
            useSelfStabilization = true
            featNorm = MeanVarNorm(features)
            // we define the LSTM locally for now, since the one in CNTK.core.bs has a slightly changed configuration that breaks this test
            Stabilize (x, enabled=true) =
                if enabled
                then {
beta = Exp (BS.Parameters.BiasParam ((1))) 
                    result = beta .* x
                }.result
                else x
            LSTMP (outputDim, cellDim=outputDim, x, inputDim=x.dim, prevState, enableSelfStabilization=false) =
            {
                _privateInnards = {       // encapsulate the inner workings
                    dh = prevState.h // previous values
                    dc = prevState.c
                    // parameter macros--these carry their own weight matrices
                    B() = BS.Parameters.BiasParam (cellDim)
                    W(v) = BS.Parameters.WeightParam (cellDim, Inferred)  * Stabilize (v, enabled=enableSelfStabilization) // input-to-hidden
                    H(h) = BS.Parameters.WeightParam (cellDim, outputDim) * Stabilize (h, enabled=enableSelfStabilization) // hidden-to-hidden
                    C(c) = BS.Parameters.DiagWeightParam (cellDim)       .* Stabilize (c, enabled=enableSelfStabilization) // cell-to-hiddden (note: applied elementwise)
                    // note: the W(x) here are all different, they all come with their own set of weights; same for H(dh), C(dc), and B()
                    it = Sigmoid (W(x) + B() + H(dh) + C(dc))          // input gate(t)
                    bit = it .* Tanh (W(x) + (H(dh) + B()))            // applied to tanh of input network
                    ft = Sigmoid (W(x) + B() + H(dh) + C(dc))          // forget-me-not gate(t)
                    bft = ft .* dc                                     // applied to cell(t-1)
                    ct = bft + bit                                     // c(t) is sum of both
                    ot = Sigmoid (W(x) + B() + H(dh) + C(ct))          // output gate(t)
                    ht = ot .* Tanh (ct)                               // applied to tanh(cell(t))
                }
                c = _privateInnards.ct          // cell value
                h = if outputDim != cellDim     // output/hidden state
                    then {                      // project
                        Wmr = BS.Parameters.WeightParam (outputDim, cellDim);
                        htp = Wmr * Stabilize (_privateInnards.ht, enabled=enableSelfStabilization)
                    }.htp         // TODO: ^^ extend BS syntax to allow to say: then { Wmr = WeightParam(outputDim, cellDim) } in Wmr * Stabilize (...)
                    else _privateInnards.ht     // no projection
                dim = outputDim
            }
            RecurrentLSTMP (outputDim, cellDim=outputDim.dim, x, inputDim=x.dim, previousHook=BS.RNNs.PreviousHC, enableSelfStabilization=false) =
            {
                prevState = previousHook (lstmState)
                inputDim1 = inputDim ; cellDim1 = cellDim ; enableSelfStabilization1 = enableSelfStabilization
                lstmState = LSTMP (outputDim, cellDim=cellDim1, x, inputDim=inputDim1, prevState, enableSelfStabilization=enableSelfStabilization1)
            }.lstmState // we return the state record (h,c)
            // define the stack of hidden LSTM layers  --TODO: change to RecurrentLSTMPStack(), change stabilizer config
            S(x) = Stabilize (x, enabled=useSelfStabilization)
            LSTMoutput[k:1..numLSTMLayers] =
                if k == 1
                then /*BS.RNNs.*/ RecurrentLSTMP (hiddenDim, cellDim=innerCellDim, /*S*/ (featNorm),        inputDim=baseFeatDim, enableSelfStabilization=useSelfStabilization).h
                else /*BS.RNNs.*/ RecurrentLSTMP (hiddenDim, cellDim=innerCellDim, /*S*/ (LSTMoutput[k-1]), inputDim=hiddenDim,   enableSelfStabilization=useSelfStabilization).h
            // and add a softmax layer on top
            W = BS.Parameters.WeightParam (labelDim, Inferred)
            B = BS.Parameters.BiasParam   (labelDim)
            // (unnecessarily using explicit Times with inferInputRankToMap in order to have a test for inferInputRankToMap parameter)
            z = Times (W, S(LSTMoutput[numLSTMLayers]), inferInputRankToMap=0) + B; // top-level input to Softmax
        }.z
        // features
        features = Input((1 : featDim),  tag='feature') // TEST: Artificially reading data transposed
        realFeatures = FlattenDimensions (Transpose (features), 1, 2)             //       and swapping them back to (featDim:1), for testing Transpose()
feashift = RowSlice(featDim - baseFeatDim, baseFeatDim, realFeatures);  
        labels   = Input(labelDim, tag='label')
        // link model to inputs
models = [| modelRegressionTest; modelUsingLayers; modelUsingCuDNN5; modelUsingLayersLikeCuDNN5 |]  
model = models[1]     
        z = model (feashift)
        // link model to training
        ce  = /*Pass*/ SumElements (ReduceLogSum (z) - TransposeTimes (labels,          z),  tag='criterion')  // manually-defined per-sample objective
        err = /*Pass*/ SumElements (BS.Constants.One - TransposeTimes (labels, Hardmax (z)), tag='evaluation') // also track frame errors
        // decoding
        logPrior = LogPrior(labels)	 
        scaledLogLikelihood = Pass (z - logPrior, tag='output') // using Pass() since we can't assign a tag to x - y
        featureNodes = (features)
        labelNodes = (labels)
        criterionNodes = (ce)
        evaluationNodes = (err)
        outputNodes = (scaledLogLikelihood)
    }
    SGD = {
        epochSize = 20480 ; maxEpochs = 4 ; minibatchSize = 20
        learningRatesPerMB = 0.5 ; momentumAsTimeConstant = 2500
        numMBsToShowResult = 10
        keepCheckPointFiles = true       
    }
    reader = {
        readerType = "HTKMLFReader"
        randomize = "auto" ; readMethod = "blockRandomize"
        nbruttsineachrecurrentiter = 32
        miniBatchMode = "partial" ; verbosity = 0 ; useMersenneTwisterRand = true
        features = { dim =      363 ; type      = "real"     ; scpFile = "./glob_0000.scp" ; }
        labels   = { labelDim = 132 ; labelType = "category" ; mlfFile = "./glob_0000.mlf" ; labelMappingFile = "./state.list" }
    }
}

configparameters: cntk.cntk:stderr=x
configparameters: cntk.cntk:traceLevel=1
configparameters: cntk.cntk:truncated=true
Commands: speechTrain
Precision = "float"
CNTKModelPath: c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance/models/cntkSpeech.dnn
CNTKCommandTrainInfo: speechTrain : 4
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 4

##############################################################################
#                                                                            #
# Action "train"                                                             #
#                                                                            #
##############################################################################

CNTKCommandTrainBegin: speechTrain
LockDevice: Locked GPU 0 to test availability.
LockDevice: Unlocked GPU 0 after testing.
LockDevice: Locked GPU 1 to test availability.
LockDevice: Unlocked GPU 1 after testing.
LockDevice: Locked GPU 2 to test availability.
LockDevice: Unlocked GPU 2 after testing.
LockDevice: Locked GPU 4 to test availability.
LockDevice: Unlocked GPU 4 after testing.
LockDevice: Locked GPU 5 to test availability.
LockDevice: Unlocked GPU 5 after testing.
LockDevice: Locked GPU 3 to test availability.
LockDevice: Unlocked GPU 3 after testing.
LockDevice: Locked GPU 0 for exclusive use.
useParallelTrain option is not enabled. ParallelTrain config will be ignored.
Creating virgin network.
Node '<placeholder>' (LearnableParameter operation): Initializating Parameter[132 x 0] as heUniform later when dimensions are fully known.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 1.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 1.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 1.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[132] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[132] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 0.000000.
Node '<placeholder>' (LearnableParameter operation): Initializing Parameter[1] <- 1.000000.

Post-processing network...

6 roots:
	ce = SumElements()
	err = SumElements()
	logPrior._ = Mean()
	scaledLogLikelihood = Pass()
	z.x.x.invStdDev = InvStdDev()
	z.x.x.mean = Mean()

Validating network. 59 nodes to process in pass 1.

Validating --> modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].W = LearnableParameter() :  -> [132 x 0]
Validating --> features = InputValue() :  -> [1 x 363 x *]
Validating --> realFeatures.x = TransposeDimensions (features) : [1 x 363 x *] -> [363 x 1 x *]
Validating --> realFeatures = Reshape (realFeatures.x) : [363 x 1 x *] -> [363 x *]
Validating --> feashift = Slice (realFeatures) : [363 x *] -> [33 x *]
Validating --> z.x.x.mean = Mean (feashift) : [33 x *] -> [33]
Validating --> z.x.x.ElementTimesArgs[0] = Minus (feashift, z.x.x.mean) : [33 x *], [33] -> [33 x *]
Validating --> z.x.x.invStdDev = InvStdDev (feashift) : [33 x *] -> [33]
Validating --> z.x.x = ElementTimes (z.x.x.ElementTimesArgs[0], z.x.x.invStdDev) : [33 x *], [33] -> [33 x *]
Validating --> z.x.x.x.mean.r = ReduceElements (z.x.x) : [33 x *] -> [1 x *]
Validating --> z.x.x.x.x0 = Minus (z.x.x, z.x.x.x.mean.r) : [33 x *], [1 x *] -> [33 x *]
Validating --> z.x.x.x.std.z._ = ElementTimes (z.x.x.x.x0, z.x.x.x.x0) : [33 x *], [33 x *] -> [33 x *]
Validating --> z.x.x.x.std.z.r = ReduceElements (z.x.x.x.std.z._) : [33 x *] -> [1 x *]
Validating --> z.x.x.x.std = Sqrt (z.x.x.x.std.z.r) : [1 x *] -> [1 x *]
Validating --> z.x.x.x.xHat.y = Reciprocal (z.x.x.x.std) : [1 x *] -> [1 x *]
Validating --> z.x.x.x.xHat = ElementTimes (z.x.x.x.x0, z.x.x.x.xHat.y) : [33 x *], [1 x *] -> [33 x *]
Validating --> modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.gain = LearnableParameter() :  -> [1]
Validating --> z.x.x.x.val.PlusArgs[0] = ElementTimes (z.x.x.x.xHat, modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.gain) : [33 x *], [1] -> [33 x *]
Validating --> modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.bias = LearnableParameter() :  -> [1]
Validating --> z.x.x.x.val = Plus (z.x.x.x.val.PlusArgs[0], modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.bias) : [33 x *], [1] -> [33 x *]
Validating --> z.x.x.mean.r = ReduceElements (z.x.x.x.val) : [33 x *] -> [1 x *]
Validating --> z.x.x.x0 = Minus (z.x.x.x.val, z.x.x.mean.r) : [33 x *], [1 x *] -> [33 x *]
Validating --> z.x.x.std.z._ = ElementTimes (z.x.x.x0, z.x.x.x0) : [33 x *], [33 x *] -> [33 x *]
Validating --> z.x.x.std.z.r = ReduceElements (z.x.x.std.z._) : [33 x *] -> [1 x *]
Validating --> z.x.x.std = Sqrt (z.x.x.std.z.r) : [1 x *] -> [1 x *]
Validating --> z.x.x.xHat.y = Reciprocal (z.x.x.std) : [1 x *] -> [1 x *]
Validating --> z.x.x.xHat = ElementTimes (z.x.x.x0, z.x.x.xHat.y) : [33 x *], [1 x *] -> [33 x *]
Validating --> modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.gain = LearnableParameter() :  -> [1]
Validating --> z.x.x.val.PlusArgs[0] = ElementTimes (z.x.x.xHat, modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.gain) : [33 x *], [1] -> [33 x *]
Validating --> modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.bias = LearnableParameter() :  -> [1]
Validating --> z.x.x.val = Plus (z.x.x.val.PlusArgs[0], modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.bias) : [33 x *], [1] -> [33 x *]
Validating --> z.x.mean.r = ReduceElements (z.x.x.val) : [33 x *] -> [1 x *]
Validating --> z.x.x0 = Minus (z.x.x.val, z.x.mean.r) : [33 x *], [1 x *] -> [33 x *]
Validating --> z.x.std.z._ = ElementTimes (z.x.x0, z.x.x0) : [33 x *], [33 x *] -> [33 x *]
Validating --> z.x.std.z.r = ReduceElements (z.x.std.z._) : [33 x *] -> [1 x *]
Validating --> z.x.std = Sqrt (z.x.std.z.r) : [1 x *] -> [1 x *]
Validating --> z.x.xHat.y = Reciprocal (z.x.std) : [1 x *] -> [1 x *]
Validating --> z.x.xHat = ElementTimes (z.x.x0, z.x.xHat.y) : [33 x *], [1 x *] -> [33 x *]
Validating --> modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.gain = LearnableParameter() :  -> [1]
Validating --> z.x.val.PlusArgs[0] = ElementTimes (z.x.xHat, modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.gain) : [33 x *], [1] -> [33 x *]
Validating --> modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.bias = LearnableParameter() :  -> [1]
Validating --> z.x.val = Plus (z.x.val.PlusArgs[0], modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.bias) : [33 x *], [1] -> [33 x *]
Node 'modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [132 x 33].
Node 'modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].W' (LearnableParameter operation): Initializing Parameter[132 x 33] <- heUniform(seed=1, init dims=[132 x 33], range=0.426401*0.333333, onCPU=true).
Validating --> z.x.PlusArgs[0] = Times (modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].W, z.x.val) : [132 x 33], [33 x *] -> [132 x *]
Validating --> modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].b = LearnableParameter() :  -> [132]
Validating --> z = Plus (z.x.PlusArgs[0], modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].b) : [132 x *], [132] -> [132 x *]
Validating --> ce.matrix.MinusArgs[0].r = ReduceElements (z) : [132 x *] -> [1 x *]
Validating --> labels = InputValue() :  -> [132 x *]
Validating --> ce.matrix.MinusArgs[1] = TransposeTimes (labels, z) : [132 x *], [132 x *] -> [1 x *]
Validating --> ce.matrix = Minus (ce.matrix.MinusArgs[0].r, ce.matrix.MinusArgs[1]) : [1 x *], [1 x *] -> [1 x *]
Validating --> ce = SumElements (ce.matrix) : [1 x *] -> [1]
Validating --> BS.Constants.One = LearnableParameter() :  -> [1]
Validating --> err.matrix.MinusArgs[1].rightMatrix = Hardmax (z) : [132 x *] -> [132 x *]
Validating --> err.matrix.MinusArgs[1] = TransposeTimes (labels, err.matrix.MinusArgs[1].rightMatrix) : [132 x *], [132 x *] -> [1 x *]
Validating --> err.matrix = Minus (BS.Constants.One, err.matrix.MinusArgs[1]) : [1], [1 x *] -> [1 x *]
Validating --> err = SumElements (err.matrix) : [1 x *] -> [1]
Validating --> logPrior._ = Mean (labels) : [132 x *] -> [132]
Validating --> logPrior = Log (logPrior._) : [132] -> [132]
Validating --> scaledLogLikelihood._ = Minus (z, logPrior) : [132 x *], [132] -> [132 x *]
Validating --> scaledLogLikelihood = Pass (scaledLogLikelihood._) : [132 x *] -> [132 x *]

Validating network. 48 nodes to process in pass 2.


Validating network, final pass.



15 out of 59 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

reading script file ./glob_0000.scp ... 948 entries
total 132 state names in state list ./state.list
htkmlfreader: reading MLF file ./glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Created model with 59 nodes on GPU 0.

Training criterion node(s):
	ce = SumElements

Evaluation criterion node(s):
	err = SumElements


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 93 matrices, 50 are shared as 20, and 43 are not shared.

	{ ce.matrix.MinusArgs[0].r : [1 x *]
	  modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.bias : [1] (gradient)
	  z.x.PlusArgs[0] : [132 x *] (gradient) }
	{ z.x.std.z.r : [1 x *] (gradient)
	  z.x.xHat.y : [1 x *] }
	{ modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.gain : [1] (gradient)
	  z.x.std.z._ : [33 x *]
	  z.x.x.val : [33 x *] (gradient) }
	{ ce.matrix.MinusArgs[0].r : [1 x *] (gradient)
	  modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].b : [132] (gradient) }
	{ ce.matrix : [1 x *]
	  modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.gain : [1] (gradient)
	  z : [132 x *] (gradient)
	  z.x.val : [33 x *] (gradient) }
	{ z.x.PlusArgs[0] : [132 x *]
	  z.x.val.PlusArgs[0] : [33 x *] (gradient)
	  z.x.xHat.y : [1 x *] (gradient) }
	{ z.x.std : [1 x *] (gradient)
	  z.x.xHat : [33 x *] }
	{ z.x.val : [33 x *]
	  z.x.xHat : [33 x *] (gradient) }
	{ modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.bias : [1] (gradient)
	  z.x.mean.r : [1 x *] (gradient)
	  z.x.std : [1 x *]
	  z.x.std.z._ : [33 x *] (gradient) }
	{ modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].W : [132 x 33] (gradient)
	  z : [132 x *] }
	{ z.x.val.PlusArgs[0] : [33 x *]
	  z.x.x0 : [33 x *] (gradient) }
	{ z.x.x.std.z._ : [33 x *]
	  z.x.x.x.val : [33 x *] (gradient) }
	{ z.x.x.mean.r : [1 x *]
	  z.x.x.x.val.PlusArgs[0] : [33 x *] (gradient) }
	{ z.x.x.std.z.r : [1 x *] (gradient)
	  z.x.x.xHat.y : [1 x *] }
	{ z.x.x.std : [1 x *] (gradient)
	  z.x.x.xHat : [33 x *] }
	{ z.x.x.val.PlusArgs[0] : [33 x *]
	  z.x.x.x0 : [33 x *] (gradient) }
	{ modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.bias : [1] (gradient)
	  z.x.x.mean.r : [1 x *] (gradient)
	  z.x.x.std : [1 x *]
	  z.x.x.std.z._ : [33 x *] (gradient) }
	{ z.x.x.val : [33 x *]
	  z.x.x.xHat : [33 x *] (gradient) }
	{ modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.gain : [1] (gradient)
	  z.x.x.x.val : [33 x *] }
	{ z.x.mean.r : [1 x *]
	  z.x.x.val.PlusArgs[0] : [33 x *] (gradient)
	  z.x.x.xHat.y : [1 x *] (gradient) }


Training 4494 parameters in 8 out of 8 parameter tensors and 34 nodes with gradient:

	Node 'modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.bias' (LearnableParameter operation) : [1]
	Node 'modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions.gain' (LearnableParameter operation) : [1]
	Node 'modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.bias' (LearnableParameter operation) : [1]
	Node 'modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[1].arrayOfFunctions.gain' (LearnableParameter operation) : [1]
	Node 'modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.bias' (LearnableParameter operation) : [1]
	Node 'modelUsingLayers.arrayOfFunctions[1].arrayOfFunctions[2].arrayOfFunctions.gain' (LearnableParameter operation) : [1]
	Node 'modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].W' (LearnableParameter operation) : [132 x 33]
	Node 'modelUsingLayers.arrayOfFunctions[2].arrayOfFunctions[0].b' (LearnableParameter operation) : [132]


Precomputing --> 3 PreCompute nodes found.

	z.x.x.mean = Mean()
	z.x.x.invStdDev = InvStdDev()
	logPrior._ = Mean()
minibatchiterator: epoch 0: frames [0..252734] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Precomputing --> Completed.


Starting Epoch 1: learning rate per sample = 0.000781  effective momentum = 0.774129  momentum as time constant = 2499.8 samples
minibatchiterator: epoch 0: frames [0..20480] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10, 0.98%]: ce = 4.66767303 * 6400; err = 0.91546875 * 6400; time = 0.2887s; samplesPerSecond = 22170.8
 Epoch[ 1 of 4]-Minibatch[  11-  20, 1.95%]: ce = 3.70825836 * 6400; err = 0.81328125 * 6400; time = 0.2875s; samplesPerSecond = 22262.7
 Epoch[ 1 of 4]-Minibatch[  21-  30, 2.93%]: ce = 3.33248487 * 5494; err = 0.76010193 * 5494; time = 0.2812s; samplesPerSecond = 19535.5
 Epoch[ 1 of 4]-Minibatch[  31-  40, 3.91%]: ce = 2.99555574 * 2062; err = 0.71096023 * 2062; time = 0.2791s; samplesPerSecond = 7387.2
Finished Epoch[ 1 of 4]: [Training] ce = 3.82782733 * 20498; err = 0.81988487 * 20498; totalSamplesSeen = 20498; learningRatePerSample = 0.00078125001; epochTime=1.26677s
SGD: Saving checkpoint model 'c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance/models/cntkSpeech.dnn.1'

Starting Epoch 2: learning rate per sample = 0.000781  effective momentum = 0.774129  momentum as time constant = 2499.8 samples
minibatchiterator: epoch 1: frames [20480..40960] (first utterance at frame 20498), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10, 0.98%]: ce = 2.79078125 * 6400; err = 0.67343750 * 6400; time = 0.3380s; samplesPerSecond = 18936.5
 Epoch[ 2 of 4]-Minibatch[  11-  20, 1.95%]: ce = 2.69503479 * 6400; err = 0.67078125 * 6400; time = 0.2841s; samplesPerSecond = 22529.7
 Epoch[ 2 of 4]-Minibatch[  21-  30, 2.93%]: ce = 2.57620659 * 5626; err = 0.66601493 * 5626; time = 0.2900s; samplesPerSecond = 19396.7
 Epoch[ 2 of 4]-Minibatch[  31-  40, 3.91%]: ce = 2.67395288 * 1816; err = 0.67400881 * 1816; time = 0.2783s; samplesPerSecond = 6526.1
 Epoch[ 2 of 4]-Minibatch[  41-  50, 4.88%]: ce = 2.65301668 * 238; err = 0.66386555 * 238; time = 0.2768s; samplesPerSecond = 859.9
Finished Epoch[ 2 of 4]: [Training] ce = 2.68886549 * 20514; err = 0.67056644 * 20514; totalSamplesSeen = 41012; learningRatePerSample = 0.00078125001; epochTime=1.54214s
SGD: Saving checkpoint model 'c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance/models/cntkSpeech.dnn.2'

Starting Epoch 3: learning rate per sample = 0.000781  effective momentum = 0.774129  momentum as time constant = 2499.8 samples
minibatchiterator: epoch 2: frames [40960..61440] (first utterance at frame 41012), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10, 0.98%]: ce = 2.53135300 * 6400; err = 0.63375000 * 6400; time = 0.3158s; samplesPerSecond = 20266.3
 Epoch[ 3 of 4]-Minibatch[  11-  20, 1.95%]: ce = 2.43102463 * 6400; err = 0.62484375 * 6400; time = 0.2839s; samplesPerSecond = 22540.1
 Epoch[ 3 of 4]-Minibatch[  21-  30, 2.93%]: ce = 2.39190500 * 5748; err = 0.64022269 * 5748; time = 0.2828s; samplesPerSecond = 20324.8
 Epoch[ 3 of 4]-Minibatch[  31-  40, 3.91%]: ce = 2.38617170 * 1828; err = 0.62910284 * 1828; time = 0.2778s; samplesPerSecond = 6580.5
Finished Epoch[ 3 of 4]: [Training] ce = 2.44292149 * 20598; err = 0.63268278 * 20598; totalSamplesSeen = 61610; learningRatePerSample = 0.00078125001; epochTime=1.31938s
SGD: Saving checkpoint model 'c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance/models/cntkSpeech.dnn.3'

Starting Epoch 4: learning rate per sample = 0.000781  effective momentum = 0.774129  momentum as time constant = 2499.8 samples
minibatchiterator: epoch 3: frames [61440..81920] (first utterance at frame 61610), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10, 0.98%]: ce = 2.44734314 * 6400; err = 0.64000000 * 6400; time = 0.5571s; samplesPerSecond = 11488.6
 Epoch[ 4 of 4]-Minibatch[  11-  20, 1.95%]: ce = 2.36628693 * 6400; err = 0.64250000 * 6400; time = 0.3727s; samplesPerSecond = 17173.8
 Epoch[ 4 of 4]-Minibatch[  21-  30, 2.93%]: ce = 2.54873548 * 5882; err = 0.66558994 * 5882; time = 0.3400s; samplesPerSecond = 17301.2
 Epoch[ 4 of 4]-Minibatch[  31-  40, 3.91%]: ce = 2.53740664 * 1682; err = 0.69262782 * 1682; time = 0.3355s; samplesPerSecond = 5013.5
Finished Epoch[ 4 of 4]: [Training] ce = 2.45861419 * 20376; err = 0.65272870 * 20376; totalSamplesSeen = 81986; learningRatePerSample = 0.00078125001; epochTime=1.68303s
SGD: Saving checkpoint model 'c:\work\CNTK\Tests\EndToEndTests\Speech\RunDir\LSTM\FullUtterance/models/cntkSpeech.dnn'
CNTKCommandTrainEnd: speechTrain

Action "train" complete.

__COMPLETED__

CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3530 @ 2.80GHz
    Hardware threads: 4
    Total Memory: 12580404 kB
-------------------------------------------------------------------
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 2 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu DeviceId=0 timestamping=true numCPUThreads=2 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:39

C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
requestnodes [MPIWrapper]: using 2 out of 2 MPI nodes on a single host (2 requested); we (0) are in (participating)
CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:39

C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
requestnodes [MPIWrapper]: using 2 out of 2 MPI nodes on a single host (2 requested); we (1) are in (participating)
MPI Rank 0: 12/15/2016 08:29:39: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr_speechTrain.logrank0
MPI Rank 0: CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:39
MPI Rank 0: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
MPI Rank 0: 12/15/2016 08:29:39: Using 2 CPU threads.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:39: ##############################################################################
MPI Rank 0: 12/15/2016 08:29:39: #                                                                            #
MPI Rank 0: 12/15/2016 08:29:39: # speechTrain command (train action)                                         #
MPI Rank 0: 12/15/2016 08:29:39: #                                                                            #
MPI Rank 0: 12/15/2016 08:29:39: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:39: 
MPI Rank 0: Creating virgin network.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 6 roots:
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 	err = ClassificationError()
MPI Rank 0: 	featNorm.invStdDev = InvStdDev()
MPI Rank 0: 	featNorm.mean = Mean()
MPI Rank 0: 	logPrior._ = Mean()
MPI Rank 0: 	scaledLogLikelihood = Minus()
MPI Rank 0: 
MPI Rank 0: Validating network. 37 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> labels = InputValue() :  -> [132 x *]
MPI Rank 0: Validating --> outLayer.W = LearnableParameter() :  -> [132 x 512]
MPI Rank 0: Validating --> link = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> finalHiddenToPlus.scalarScalingFactor = Dropout (link) : [1 x 1] -> [1 x 1]
MPI Rank 0: Validating --> layers[3].Eh._._.W = LearnableParameter() :  -> [512 x 512]
MPI Rank 0: Validating --> layers[2].Eh._._.W = LearnableParameter() :  -> [512 x 512]
MPI Rank 0: Validating --> layers[1].Eh._._.W = LearnableParameter() :  -> [512 x 363]
MPI Rank 0: Validating --> features = InputValue() :  -> [363 x *]
MPI Rank 0: Validating --> featNorm.mean = Mean (features) : [363 x *] -> [363]
MPI Rank 0: Validating --> featNorm.ElementTimesArgs[0] = Minus (features, featNorm.mean) : [363 x *], [363] -> [363 x *]
MPI Rank 0: Validating --> featNorm.invStdDev = InvStdDev (features) : [363 x *] -> [363]
MPI Rank 0: Validating --> featNorm = ElementTimes (featNorm.ElementTimesArgs[0], featNorm.invStdDev) : [363 x *], [363] -> [363 x *]
MPI Rank 0: Validating --> layers[1].Eh._._.z.PlusArgs[0] = Times (layers[1].Eh._._.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
MPI Rank 0: Validating --> layers[1].Eh._._.B = LearnableParameter() :  -> [512 x 1]
MPI Rank 0: Validating --> layers[1].Eh._._.z = Plus (layers[1].Eh._._.z.PlusArgs[0], layers[1].Eh._._.B) : [512 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[1].Eh._ = Sigmoid (layers[1].Eh._._.z) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[1].Eh = Dropout (layers[1].Eh._) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[2].Eh._._.z.PlusArgs[0] = Times (layers[2].Eh._._.W, layers[1].Eh) : [512 x 512], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[2].Eh._._.B = LearnableParameter() :  -> [512 x 1]
MPI Rank 0: Validating --> layers[2].Eh._._.z = Plus (layers[2].Eh._._.z.PlusArgs[0], layers[2].Eh._._.B) : [512 x 1 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[2].Eh._ = Sigmoid (layers[2].Eh._._.z) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[2].Eh = Dropout (layers[2].Eh._) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[3].Eh._._.z.PlusArgs[0] = Times (layers[3].Eh._._.W, layers[2].Eh) : [512 x 512], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[3].Eh._._.B = LearnableParameter() :  -> [512 x 1]
MPI Rank 0: Validating --> layers[3].Eh._._.z = Plus (layers[3].Eh._._.z.PlusArgs[0], layers[3].Eh._._.B) : [512 x 1 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[3].Eh._ = Sigmoid (layers[3].Eh._._.z) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> layers[3].Eh = Dropout (layers[3].Eh._) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> finalHiddenToPlus = ElementTimes (finalHiddenToPlus.scalarScalingFactor, layers[3].Eh) : [1 x 1], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> outLayer.in = Plus (finalHiddenToPlus, layers[2].Eh) : [512 x 1 x *], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 0: Validating --> outLayer.z.PlusArgs[0] = Times (outLayer.W, outLayer.in) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
MPI Rank 0: Validating --> outLayer.B = LearnableParameter() :  -> [132 x 1]
MPI Rank 0: Validating --> outZ = Plus (outLayer.z.PlusArgs[0], outLayer.B) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (labels, outZ) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 0: Validating --> err = ClassificationError (labels, outZ) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 0: Validating --> logPrior._ = Mean (labels) : [132 x *] -> [132]
MPI Rank 0: Validating --> logPrior = Log (logPrior._) : [132] -> [132]
MPI Rank 0: Validating --> scaledLogLikelihood = Minus (outZ, logPrior) : [132 x 1 x *], [132] -> [132 x 1 x *]
MPI Rank 0: 
MPI Rank 0: Validating network. 26 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: reading script file glob_0000.scp ... 948 entries
MPI Rank 0: total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
MPI Rank 0: htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
MPI Rank 0: ...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
MPI Rank 0: label set 0: 129 classes
MPI Rank 0: minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
MPI Rank 0: 12/15/2016 08:29:40: 
MPI Rank 0: Model has 37 nodes. Using GPU 0.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:40: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 0: 12/15/2016 08:29:40: Evaluation criterion: err = ClassificationError
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing: Out of 62 matrices, 33 are shared as 15, and 29 are not shared.
MPI Rank 0: 
MPI Rank 0: 	{ layers[1].Eh._._.W : [512 x 363] (gradient)
MPI Rank 0: 	  layers[1].Eh._._.z : [512 x 1 x *] }
MPI Rank 0: 	{ layers[2].Eh._._.W : [512 x 512] (gradient)
MPI Rank 0: 	  layers[2].Eh._._.z : [512 x 1 x *] }
MPI Rank 0: 	{ layers[3].Eh : [512 x 1 x *] (gradient)
MPI Rank 0: 	  outLayer.in : [512 x 1 x *] }
MPI Rank 0: 	{ layers[1].Eh._ : [512 x 1 x *] (gradient)
MPI Rank 0: 	  layers[1].Eh._._.B : [512 x 1] (gradient)
MPI Rank 0: 	  layers[2].Eh._._.z.PlusArgs[0] : [512 x 1 x *] }
MPI Rank 0: 	{ outLayer.W : [132 x 512] (gradient)
MPI Rank 0: 	  outZ : [132 x 1 x *] }
MPI Rank 0: 	{ outLayer.in : [512 x 1 x *] (gradient)
MPI Rank 0: 	  outZ : [132 x 1 x *] (gradient) }
MPI Rank 0: 	{ layers[2].Eh : [512 x 1 x *] (gradient)
MPI Rank 0: 	  outLayer.z.PlusArgs[0] : [132 x 1 x *] (gradient) }
MPI Rank 0: 	{ finalHiddenToPlus : [512 x 1 x *]
MPI Rank 0: 	  layers[3].Eh._ : [512 x 1 x *] (gradient)
MPI Rank 0: 	  layers[3].Eh._._.B : [512 x 1] (gradient) }
MPI Rank 0: 	{ finalHiddenToPlus : [512 x 1 x *] (gradient)
MPI Rank 0: 	  outLayer.z.PlusArgs[0] : [132 x 1 x *] }
MPI Rank 0: 	{ layers[2].Eh._ : [512 x 1 x *] (gradient)
MPI Rank 0: 	  layers[2].Eh._._.B : [512 x 1] (gradient)
MPI Rank 0: 	  layers[3].Eh._._.z.PlusArgs[0] : [512 x 1 x *] }
MPI Rank 0: 	{ layers[1].Eh._ : [512 x 1 x *]
MPI Rank 0: 	  layers[1].Eh._._.z.PlusArgs[0] : [512 x *] (gradient) }
MPI Rank 0: 	{ layers[3].Eh._._.W : [512 x 512] (gradient)
MPI Rank 0: 	  layers[3].Eh._._.z : [512 x 1 x *] }
MPI Rank 0: 	{ layers[3].Eh._ : [512 x 1 x *]
MPI Rank 0: 	  layers[3].Eh._._.z.PlusArgs[0] : [512 x 1 x *] (gradient) }
MPI Rank 0: 	{ layers[1].Eh : [512 x 1 x *] (gradient)
MPI Rank 0: 	  layers[2].Eh._._.z : [512 x 1 x *] (gradient) }
MPI Rank 0: 	{ layers[2].Eh._ : [512 x 1 x *]
MPI Rank 0: 	  layers[2].Eh._._.z.PlusArgs[0] : [512 x 1 x *] (gradient) }
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:40: Training 779396 parameters in 8 out of 8 parameter tensors and 25 nodes with gradient:
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'layers[1].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'layers[1].Eh._._.W' (LearnableParameter operation) : [512 x 363]
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'layers[2].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'layers[2].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'layers[3].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'layers[3].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'outLayer.B' (LearnableParameter operation) : [132 x 1]
MPI Rank 0: 12/15/2016 08:29:40: 	Node 'outLayer.W' (LearnableParameter operation) : [132 x 512]
MPI Rank 0: 
MPI Rank 0: Initializing dataParallelSGD with FP32 aggregation.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:40: Precomputing --> 3 PreCompute nodes found.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:40: 	featNorm.mean = Mean()
MPI Rank 0: 12/15/2016 08:29:40: 	featNorm.invStdDev = InvStdDev()
MPI Rank 0: 12/15/2016 08:29:40: 	logPrior._ = Mean()
MPI Rank 0: minibatchiterator: epoch 0: frames [0..252734] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
MPI Rank 0: requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:41: Precomputing --> Completed.
MPI Rank 0: 
MPI Rank 0: Setting dropout rate to 0.1.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:42: Starting Epoch 1: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 0: minibatchiterator: epoch 0: frames [0..20480] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:42: Starting minibatch loop.
MPI Rank 0: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[   1-  10, 12.50%]: ce = 4.63874359 * 2560; err = 0.90859375 * 2560; time = 0.1297s; samplesPerSecond = 19739.2
MPI Rank 0: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  11-  20, 25.00%]: ce = 7.18073807 * 2560; err = 0.92265625 * 2560; time = 0.0589s; samplesPerSecond = 43498.9
MPI Rank 0: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  21-  30, 37.50%]: ce = 6.11719131 * 2560; err = 0.92382813 * 2560; time = 0.0575s; samplesPerSecond = 44527.9
MPI Rank 0: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  31-  40, 50.00%]: ce = 4.87287598 * 2560; err = 0.92578125 * 2560; time = 0.0581s; samplesPerSecond = 44077.9
MPI Rank 0: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  41-  50, 62.50%]: ce = 4.17484741 * 2560; err = 0.90195313 * 2560; time = 0.0582s; samplesPerSecond = 43968.9
MPI Rank 0: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  51-  60, 75.00%]: ce = 4.01487427 * 2560; err = 0.88710937 * 2560; time = 0.0594s; samplesPerSecond = 43098.4
MPI Rank 0: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  61-  70, 87.50%]: ce = 3.95324707 * 2560; err = 0.86328125 * 2560; time = 0.0587s; samplesPerSecond = 43606.4
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 1 of 5]-Minibatch[  71-  80, 100.00%]: ce = 3.92308655 * 2560; err = 0.87500000 * 2560; time = 0.0599s; samplesPerSecond = 42713.7
MPI Rank 0: 12/15/2016 08:29:43: Finished Epoch[ 1 of 5]: [Training] ce = 4.85945053 * 20480; err = 0.90102539 * 20480; totalSamplesSeen = 20480; learningRatePerSample = 0.001953125; epochTime=0.544747s
MPI Rank 0: 12/15/2016 08:29:43: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn.1'
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:43: Starting Epoch 2: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 0: minibatchiterator: epoch 1: frames [20480..40960] (first utterance at frame 20480), data subset 0 of 2, with 1 datapasses
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:43: Starting minibatch loop, DataParallelSGD training (myRank = 0, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[   1-  10, 12.50%]: ce = 3.90146071 * 2560; err = 0.86757812 * 2560; time = 0.1508s; samplesPerSecond = 16974.0
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  11-  20, 25.00%]: ce = 3.83399547 * 2560; err = 0.87617188 * 2560; time = 0.1019s; samplesPerSecond = 25122.9
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  21-  30, 37.50%]: ce = 3.76395930 * 2560; err = 0.85000000 * 2560; time = 0.1019s; samplesPerSecond = 25127.8
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  31-  40, 50.00%]: ce = 3.64447644 * 2560; err = 0.82226563 * 2560; time = 0.1006s; samplesPerSecond = 25445.5
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  41-  50, 62.50%]: ce = 3.54770106 * 2560; err = 0.83828125 * 2560; time = 0.1022s; samplesPerSecond = 25050.1
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  51-  60, 75.00%]: ce = 3.39861332 * 2560; err = 0.80195313 * 2560; time = 0.1011s; samplesPerSecond = 25323.7
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  61-  70, 87.50%]: ce = 3.33650705 * 2560; err = 0.80898437 * 2560; time = 0.1011s; samplesPerSecond = 25326.0
MPI Rank 0: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  71-  80, 100.00%]: ce = 3.31736512 * 2560; err = 0.80703125 * 2560; time = 0.1020s; samplesPerSecond = 25087.7
MPI Rank 0: 12/15/2016 08:29:43: Finished Epoch[ 2 of 5]: [Training] ce = 3.59300981 * 20480; err = 0.83403320 * 20480; totalSamplesSeen = 40960; learningRatePerSample = 0.001953125; epochTime=0.870393s
MPI Rank 0: 12/15/2016 08:29:44: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn.2'
MPI Rank 0: Setting dropout rate to 0.15.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:44: Starting Epoch 3: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 0: minibatchiterator: epoch 2: frames [40960..61440] (first utterance at frame 40960), data subset 0 of 2, with 1 datapasses
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:44: Starting minibatch loop, DataParallelSGD training (myRank = 0, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[   1-  10, 12.50%]: ce = 3.23060966 * 2560; err = 0.78593750 * 2560; time = 0.1451s; samplesPerSecond = 17641.1
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  11-  20, 25.00%]: ce = 3.16172538 * 2560; err = 0.77460938 * 2560; time = 0.1022s; samplesPerSecond = 25041.1
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  21-  30, 37.50%]: ce = 3.11703613 * 2560; err = 0.76367188 * 2560; time = 0.1002s; samplesPerSecond = 25546.1
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  31-  40, 50.00%]: ce = 3.15996076 * 2560; err = 0.77460938 * 2560; time = 0.1032s; samplesPerSecond = 24798.8
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  41-  50, 62.50%]: ce = 3.02947482 * 2560; err = 0.74179688 * 2560; time = 0.1036s; samplesPerSecond = 24711.1
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  51-  60, 75.00%]: ce = 3.01643370 * 2560; err = 0.73164063 * 2560; time = 0.1031s; samplesPerSecond = 24828.8
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.91772761 * 2560; err = 0.72109375 * 2560; time = 0.1039s; samplesPerSecond = 24631.0
MPI Rank 0: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.90788425 * 2560; err = 0.72617188 * 2560; time = 0.1010s; samplesPerSecond = 25335.0
MPI Rank 0: 12/15/2016 08:29:44: Finished Epoch[ 3 of 5]: [Training] ce = 3.06760654 * 20480; err = 0.75244141 * 20480; totalSamplesSeen = 61440; learningRatePerSample = 0.001953125; epochTime=0.871326s
MPI Rank 0: 12/15/2016 08:29:44: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn.3'
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:45: Starting Epoch 4: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 0: minibatchiterator: epoch 3: frames [61440..81920] (first utterance at frame 61440), data subset 0 of 2, with 1 datapasses
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:45: Starting minibatch loop, DataParallelSGD training (myRank = 0, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.82334281 * 2560; err = 0.71171875 * 2560; time = 0.1463s; samplesPerSecond = 17495.2
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.74443290 * 2560; err = 0.69179687 * 2560; time = 0.1029s; samplesPerSecond = 24872.5
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.72986787 * 2560; err = 0.69140625 * 2560; time = 0.1012s; samplesPerSecond = 25284.9
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.74554855 * 2560; err = 0.68984375 * 2560; time = 0.1002s; samplesPerSecond = 25536.9
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.58012299 * 2560; err = 0.66796875 * 2560; time = 0.1004s; samplesPerSecond = 25496.0
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.62333928 * 2560; err = 0.67226562 * 2560; time = 0.1013s; samplesPerSecond = 25271.7
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.61982588 * 2560; err = 0.67851562 * 2560; time = 0.1014s; samplesPerSecond = 25248.8
MPI Rank 0: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.58207620 * 2560; err = 0.65625000 * 2560; time = 0.1010s; samplesPerSecond = 25351.8
MPI Rank 0: 12/15/2016 08:29:45: Finished Epoch[ 4 of 5]: [Training] ce = 2.68106956 * 20480; err = 0.68247070 * 20480; totalSamplesSeen = 81920; learningRatePerSample = 0.001953125; epochTime=0.864176s
MPI Rank 0: 12/15/2016 08:29:45: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn.4'
MPI Rank 0: Setting dropout rate to 0.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:45: Starting Epoch 5: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 0: minibatchiterator: epoch 4: frames [81920..102400] (first utterance at frame 81920), data subset 0 of 2, with 1 datapasses
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:45: Starting minibatch loop, DataParallelSGD training (myRank = 0, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.43513054 * 2560; err = 0.61757812 * 2560; time = 0.0961s; samplesPerSecond = 26633.9
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.42029950 * 2560; err = 0.63320312 * 2560; time = 0.0951s; samplesPerSecond = 26909.1
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.30856062 * 2560; err = 0.61132813 * 2560; time = 0.0948s; samplesPerSecond = 27005.1
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.33952171 * 2560; err = 0.61093750 * 2560; time = 0.0941s; samplesPerSecond = 27205.7
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.28281580 * 2560; err = 0.59179688 * 2560; time = 0.0942s; samplesPerSecond = 27186.3
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.18557287 * 2560; err = 0.57812500 * 2560; time = 0.0937s; samplesPerSecond = 27318.3
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.23650560 * 2560; err = 0.58710938 * 2560; time = 0.0942s; samplesPerSecond = 27170.2
MPI Rank 0: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.17937329 * 2560; err = 0.58125000 * 2560; time = 0.0914s; samplesPerSecond = 28020.7
MPI Rank 0: 12/15/2016 08:29:46: Finished Epoch[ 5 of 5]: [Training] ce = 2.29847249 * 20480; err = 0.60141602 * 20480; totalSamplesSeen = 102400; learningRatePerSample = 0.001953125; epochTime=0.762473s
MPI Rank 0: 12/15/2016 08:29:46: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn'
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:46: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:46: __COMPLETED__
MPI Rank 1: 12/15/2016 08:29:40: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr_speechTrain.logrank1
MPI Rank 1: CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:39
MPI Rank 1: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
MPI Rank 1: 12/15/2016 08:29:40: Using 2 CPU threads.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:40: ##############################################################################
MPI Rank 1: 12/15/2016 08:29:40: #                                                                            #
MPI Rank 1: 12/15/2016 08:29:40: # speechTrain command (train action)                                         #
MPI Rank 1: 12/15/2016 08:29:40: #                                                                            #
MPI Rank 1: 12/15/2016 08:29:40: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:40: 
MPI Rank 1: Creating virgin network.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 6 roots:
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 	err = ClassificationError()
MPI Rank 1: 	featNorm.invStdDev = InvStdDev()
MPI Rank 1: 	featNorm.mean = Mean()
MPI Rank 1: 	logPrior._ = Mean()
MPI Rank 1: 	scaledLogLikelihood = Minus()
MPI Rank 1: 
MPI Rank 1: Validating network. 37 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> labels = InputValue() :  -> [132 x *]
MPI Rank 1: Validating --> outLayer.W = LearnableParameter() :  -> [132 x 512]
MPI Rank 1: Validating --> link = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> finalHiddenToPlus.scalarScalingFactor = Dropout (link) : [1 x 1] -> [1 x 1]
MPI Rank 1: Validating --> layers[3].Eh._._.W = LearnableParameter() :  -> [512 x 512]
MPI Rank 1: Validating --> layers[2].Eh._._.W = LearnableParameter() :  -> [512 x 512]
MPI Rank 1: Validating --> layers[1].Eh._._.W = LearnableParameter() :  -> [512 x 363]
MPI Rank 1: Validating --> features = InputValue() :  -> [363 x *]
MPI Rank 1: Validating --> featNorm.mean = Mean (features) : [363 x *] -> [363]
MPI Rank 1: Validating --> featNorm.ElementTimesArgs[0] = Minus (features, featNorm.mean) : [363 x *], [363] -> [363 x *]
MPI Rank 1: Validating --> featNorm.invStdDev = InvStdDev (features) : [363 x *] -> [363]
MPI Rank 1: Validating --> featNorm = ElementTimes (featNorm.ElementTimesArgs[0], featNorm.invStdDev) : [363 x *], [363] -> [363 x *]
MPI Rank 1: Validating --> layers[1].Eh._._.z.PlusArgs[0] = Times (layers[1].Eh._._.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
MPI Rank 1: Validating --> layers[1].Eh._._.B = LearnableParameter() :  -> [512 x 1]
MPI Rank 1: Validating --> layers[1].Eh._._.z = Plus (layers[1].Eh._._.z.PlusArgs[0], layers[1].Eh._._.B) : [512 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[1].Eh._ = Sigmoid (layers[1].Eh._._.z) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[1].Eh = Dropout (layers[1].Eh._) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[2].Eh._._.z.PlusArgs[0] = Times (layers[2].Eh._._.W, layers[1].Eh) : [512 x 512], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[2].Eh._._.B = LearnableParameter() :  -> [512 x 1]
MPI Rank 1: Validating --> layers[2].Eh._._.z = Plus (layers[2].Eh._._.z.PlusArgs[0], layers[2].Eh._._.B) : [512 x 1 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[2].Eh._ = Sigmoid (layers[2].Eh._._.z) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[2].Eh = Dropout (layers[2].Eh._) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[3].Eh._._.z.PlusArgs[0] = Times (layers[3].Eh._._.W, layers[2].Eh) : [512 x 512], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[3].Eh._._.B = LearnableParameter() :  -> [512 x 1]
MPI Rank 1: Validating --> layers[3].Eh._._.z = Plus (layers[3].Eh._._.z.PlusArgs[0], layers[3].Eh._._.B) : [512 x 1 x *], [512 x 1] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[3].Eh._ = Sigmoid (layers[3].Eh._._.z) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> layers[3].Eh = Dropout (layers[3].Eh._) : [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> finalHiddenToPlus = ElementTimes (finalHiddenToPlus.scalarScalingFactor, layers[3].Eh) : [1 x 1], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> outLayer.in = Plus (finalHiddenToPlus, layers[2].Eh) : [512 x 1 x *], [512 x 1 x *] -> [512 x 1 x *]
MPI Rank 1: Validating --> outLayer.z.PlusArgs[0] = Times (outLayer.W, outLayer.in) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
MPI Rank 1: Validating --> outLayer.B = LearnableParameter() :  -> [132 x 1]
MPI Rank 1: Validating --> outZ = Plus (outLayer.z.PlusArgs[0], outLayer.B) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (labels, outZ) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 1: Validating --> err = ClassificationError (labels, outZ) : [132 x *], [132 x 1 x *] -> [1]
MPI Rank 1: Validating --> logPrior._ = Mean (labels) : [132 x *] -> [132]
MPI Rank 1: Validating --> logPrior = Log (logPrior._) : [132] -> [132]
MPI Rank 1: Validating --> scaledLogLikelihood = Minus (outZ, logPrior) : [132 x 1 x *], [132] -> [132 x 1 x *]
MPI Rank 1: 
MPI Rank 1: Validating network. 26 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: reading script file glob_0000.scp ... 948 entries
MPI Rank 1: total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
MPI Rank 1: htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
MPI Rank 1: ...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
MPI Rank 1: label set 0: 129 classes
MPI Rank 1: minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
MPI Rank 1: 12/15/2016 08:29:40: 
MPI Rank 1: Model has 37 nodes. Using GPU 0.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:40: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 1: 12/15/2016 08:29:40: Evaluation criterion: err = ClassificationError
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing: Out of 62 matrices, 33 are shared as 15, and 29 are not shared.
MPI Rank 1: 
MPI Rank 1: 	{ layers[1].Eh._ : [512 x 1 x *]
MPI Rank 1: 	  layers[1].Eh._._.z.PlusArgs[0] : [512 x *] (gradient) }
MPI Rank 1: 	{ layers[1].Eh : [512 x 1 x *] (gradient)
MPI Rank 1: 	  layers[2].Eh._._.z : [512 x 1 x *] (gradient) }
MPI Rank 1: 	{ layers[2].Eh._ : [512 x 1 x *] (gradient)
MPI Rank 1: 	  layers[2].Eh._._.B : [512 x 1] (gradient)
MPI Rank 1: 	  layers[3].Eh._._.z.PlusArgs[0] : [512 x 1 x *] }
MPI Rank 1: 	{ layers[3].Eh._._.W : [512 x 512] (gradient)
MPI Rank 1: 	  layers[3].Eh._._.z : [512 x 1 x *] }
MPI Rank 1: 	{ layers[3].Eh._ : [512 x 1 x *]
MPI Rank 1: 	  layers[3].Eh._._.z.PlusArgs[0] : [512 x 1 x *] (gradient) }
MPI Rank 1: 	{ layers[2].Eh : [512 x 1 x *] (gradient)
MPI Rank 1: 	  outLayer.z.PlusArgs[0] : [132 x 1 x *] (gradient) }
MPI Rank 1: 	{ layers[1].Eh._._.W : [512 x 363] (gradient)
MPI Rank 1: 	  layers[1].Eh._._.z : [512 x 1 x *] }
MPI Rank 1: 	{ layers[2].Eh._._.W : [512 x 512] (gradient)
MPI Rank 1: 	  layers[2].Eh._._.z : [512 x 1 x *] }
MPI Rank 1: 	{ outLayer.in : [512 x 1 x *] (gradient)
MPI Rank 1: 	  outZ : [132 x 1 x *] (gradient) }
MPI Rank 1: 	{ finalHiddenToPlus : [512 x 1 x *]
MPI Rank 1: 	  layers[3].Eh._ : [512 x 1 x *] (gradient)
MPI Rank 1: 	  layers[3].Eh._._.B : [512 x 1] (gradient) }
MPI Rank 1: 	{ layers[1].Eh._ : [512 x 1 x *] (gradient)
MPI Rank 1: 	  layers[1].Eh._._.B : [512 x 1] (gradient)
MPI Rank 1: 	  layers[2].Eh._._.z.PlusArgs[0] : [512 x 1 x *] }
MPI Rank 1: 	{ layers[3].Eh : [512 x 1 x *] (gradient)
MPI Rank 1: 	  outLayer.in : [512 x 1 x *] }
MPI Rank 1: 	{ finalHiddenToPlus : [512 x 1 x *] (gradient)
MPI Rank 1: 	  outLayer.z.PlusArgs[0] : [132 x 1 x *] }
MPI Rank 1: 	{ layers[2].Eh._ : [512 x 1 x *]
MPI Rank 1: 	  layers[2].Eh._._.z.PlusArgs[0] : [512 x 1 x *] (gradient) }
MPI Rank 1: 	{ outLayer.W : [132 x 512] (gradient)
MPI Rank 1: 	  outZ : [132 x 1 x *] }
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:40: Training 779396 parameters in 8 out of 8 parameter tensors and 25 nodes with gradient:
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'layers[1].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'layers[1].Eh._._.W' (LearnableParameter operation) : [512 x 363]
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'layers[2].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'layers[2].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'layers[3].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'layers[3].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'outLayer.B' (LearnableParameter operation) : [132 x 1]
MPI Rank 1: 12/15/2016 08:29:40: 	Node 'outLayer.W' (LearnableParameter operation) : [132 x 512]
MPI Rank 1: 
MPI Rank 1: Initializing dataParallelSGD with FP32 aggregation.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:40: Precomputing --> 3 PreCompute nodes found.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:40: 	featNorm.mean = Mean()
MPI Rank 1: 12/15/2016 08:29:40: 	featNorm.invStdDev = InvStdDev()
MPI Rank 1: 12/15/2016 08:29:40: 	logPrior._ = Mean()
MPI Rank 1: minibatchiterator: epoch 0: frames [0..252734] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
MPI Rank 1: requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:42: Precomputing --> Completed.
MPI Rank 1: 
MPI Rank 1: Setting dropout rate to 0.1.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:42: Starting Epoch 1: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 1: minibatchiterator: epoch 0: frames [0..20480] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:42: Starting minibatch loop.
MPI Rank 1: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[   1-  10, 12.50%]: ce = 4.63874359 * 2560; err = 0.90859375 * 2560; time = 0.1359s; samplesPerSecond = 18843.3
MPI Rank 1: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  11-  20, 25.00%]: ce = 7.18073807 * 2560; err = 0.92265625 * 2560; time = 0.0642s; samplesPerSecond = 39859.9
MPI Rank 1: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  21-  30, 37.50%]: ce = 6.11719131 * 2560; err = 0.92382813 * 2560; time = 0.0636s; samplesPerSecond = 40236.4
MPI Rank 1: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  31-  40, 50.00%]: ce = 4.87287598 * 2560; err = 0.92578125 * 2560; time = 0.0632s; samplesPerSecond = 40515.9
MPI Rank 1: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  41-  50, 62.50%]: ce = 4.17484741 * 2560; err = 0.90195313 * 2560; time = 0.0620s; samplesPerSecond = 41281.0
MPI Rank 1: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  51-  60, 75.00%]: ce = 4.01487427 * 2560; err = 0.88710937 * 2560; time = 0.0617s; samplesPerSecond = 41458.8
MPI Rank 1: 12/15/2016 08:29:42:  Epoch[ 1 of 5]-Minibatch[  61-  70, 87.50%]: ce = 3.95324707 * 2560; err = 0.86328125 * 2560; time = 0.0602s; samplesPerSecond = 42512.2
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 1 of 5]-Minibatch[  71-  80, 100.00%]: ce = 3.92308655 * 2560; err = 0.87500000 * 2560; time = 0.0488s; samplesPerSecond = 52475.1
MPI Rank 1: 12/15/2016 08:29:43: Finished Epoch[ 1 of 5]: [Training] ce = 4.85945053 * 20480; err = 0.90102539 * 20480; totalSamplesSeen = 20480; learningRatePerSample = 0.001953125; epochTime=0.56475s
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:43: Starting Epoch 2: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 1: minibatchiterator: epoch 1: frames [20480..40960] (first utterance at frame 20480), data subset 1 of 2, with 1 datapasses
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:43: Starting minibatch loop, DataParallelSGD training (myRank = 1, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[   1-  10, 12.50%]: ce = 3.90146071 * 2560; err = 0.86757812 * 2560; time = 0.1516s; samplesPerSecond = 16888.0
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  11-  20, 25.00%]: ce = 3.83399547 * 2560; err = 0.87617188 * 2560; time = 0.1019s; samplesPerSecond = 25118.5
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  21-  30, 37.50%]: ce = 3.76395930 * 2560; err = 0.85000000 * 2560; time = 0.1019s; samplesPerSecond = 25124.4
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  31-  40, 50.00%]: ce = 3.64447644 * 2560; err = 0.82226563 * 2560; time = 0.1006s; samplesPerSecond = 25436.7
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  41-  50, 62.50%]: ce = 3.54770106 * 2560; err = 0.83828125 * 2560; time = 0.1011s; samplesPerSecond = 25332.7
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  51-  60, 75.00%]: ce = 3.39861332 * 2560; err = 0.80195313 * 2560; time = 0.1022s; samplesPerSecond = 25043.8
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  61-  70, 87.50%]: ce = 3.33650705 * 2560; err = 0.80898437 * 2560; time = 0.1011s; samplesPerSecond = 25318.7
MPI Rank 1: 12/15/2016 08:29:43:  Epoch[ 2 of 5]-Minibatch[  71-  80, 100.00%]: ce = 3.31736512 * 2560; err = 0.80703125 * 2560; time = 0.1009s; samplesPerSecond = 25368.6
MPI Rank 1: 12/15/2016 08:29:43: Finished Epoch[ 2 of 5]: [Training] ce = 3.59300981 * 20480; err = 0.83403320 * 20480; totalSamplesSeen = 40960; learningRatePerSample = 0.001953125; epochTime=0.869801s
MPI Rank 1: Setting dropout rate to 0.15.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:44: Starting Epoch 3: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 1: minibatchiterator: epoch 2: frames [40960..61440] (first utterance at frame 40960), data subset 1 of 2, with 1 datapasses
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:44: Starting minibatch loop, DataParallelSGD training (myRank = 1, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[   1-  10, 12.50%]: ce = 3.23060966 * 2560; err = 0.78593750 * 2560; time = 0.1459s; samplesPerSecond = 17549.4
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  11-  20, 25.00%]: ce = 3.16172538 * 2560; err = 0.77460938 * 2560; time = 0.1011s; samplesPerSecond = 25321.0
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  21-  30, 37.50%]: ce = 3.11703613 * 2560; err = 0.76367188 * 2560; time = 0.1013s; samplesPerSecond = 25260.0
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  31-  40, 50.00%]: ce = 3.15996076 * 2560; err = 0.77460938 * 2560; time = 0.1032s; samplesPerSecond = 24794.4
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  41-  50, 62.50%]: ce = 3.02947482 * 2560; err = 0.74179688 * 2560; time = 0.1036s; samplesPerSecond = 24704.9
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  51-  60, 75.00%]: ce = 3.01643370 * 2560; err = 0.73164063 * 2560; time = 0.1032s; samplesPerSecond = 24809.6
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.91772761 * 2560; err = 0.72109375 * 2560; time = 0.1039s; samplesPerSecond = 24638.1
MPI Rank 1: 12/15/2016 08:29:44:  Epoch[ 3 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.90788425 * 2560; err = 0.72617188 * 2560; time = 0.0999s; samplesPerSecond = 25621.0
MPI Rank 1: 12/15/2016 08:29:44: Finished Epoch[ 3 of 5]: [Training] ce = 3.06760654 * 20480; err = 0.75244141 * 20480; totalSamplesSeen = 61440; learningRatePerSample = 0.001953125; epochTime=0.870738s
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:45: Starting Epoch 4: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 1: minibatchiterator: epoch 3: frames [61440..81920] (first utterance at frame 61440), data subset 1 of 2, with 1 datapasses
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:45: Starting minibatch loop, DataParallelSGD training (myRank = 1, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.82334281 * 2560; err = 0.71171875 * 2560; time = 0.1472s; samplesPerSecond = 17386.0
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.74443290 * 2560; err = 0.69179687 * 2560; time = 0.1030s; samplesPerSecond = 24866.4
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.72986787 * 2560; err = 0.69140625 * 2560; time = 0.1013s; samplesPerSecond = 25281.0
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.74554855 * 2560; err = 0.68984375 * 2560; time = 0.1003s; samplesPerSecond = 25516.1
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.58012299 * 2560; err = 0.66796875 * 2560; time = 0.1004s; samplesPerSecond = 25490.9
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.62333928 * 2560; err = 0.67226562 * 2560; time = 0.1013s; samplesPerSecond = 25266.0
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.61982588 * 2560; err = 0.67851562 * 2560; time = 0.1014s; samplesPerSecond = 25257.8
MPI Rank 1: 12/15/2016 08:29:45:  Epoch[ 4 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.58207620 * 2560; err = 0.65625000 * 2560; time = 0.1010s; samplesPerSecond = 25345.8
MPI Rank 1: 12/15/2016 08:29:45: Finished Epoch[ 4 of 5]: [Training] ce = 2.68106956 * 20480; err = 0.68247070 * 20480; totalSamplesSeen = 81920; learningRatePerSample = 0.001953125; epochTime=0.863593s
MPI Rank 1: Setting dropout rate to 0.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:45: Starting Epoch 5: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 1: minibatchiterator: epoch 4: frames [81920..102400] (first utterance at frame 81920), data subset 1 of 2, with 1 datapasses
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:45: Starting minibatch loop, DataParallelSGD training (myRank = 1, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.43513054 * 2560; err = 0.61757812 * 2560; time = 0.0964s; samplesPerSecond = 26553.3
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.42029950 * 2560; err = 0.63320312 * 2560; time = 0.0940s; samplesPerSecond = 27229.4
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.30856062 * 2560; err = 0.61132813 * 2560; time = 0.0948s; samplesPerSecond = 27010.2
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.33952171 * 2560; err = 0.61093750 * 2560; time = 0.0941s; samplesPerSecond = 27215.2
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.28281580 * 2560; err = 0.59179688 * 2560; time = 0.0941s; samplesPerSecond = 27195.0
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.18557287 * 2560; err = 0.57812500 * 2560; time = 0.0948s; samplesPerSecond = 26997.4
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.23650560 * 2560; err = 0.58710938 * 2560; time = 0.0931s; samplesPerSecond = 27493.5
MPI Rank 1: 12/15/2016 08:29:46:  Epoch[ 5 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.17937329 * 2560; err = 0.58125000 * 2560; time = 0.0913s; samplesPerSecond = 28049.0
MPI Rank 1: 12/15/2016 08:29:46: Finished Epoch[ 5 of 5]: [Training] ce = 2.29847249 * 20480; err = 0.60141602 * 20480; totalSamplesSeen = 102400; learningRatePerSample = 0.001953125; epochTime=0.76176s
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:46: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:46: __COMPLETED__
=== Deleting last 2 epochs and restart
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 2 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu DeviceId=0 timestamping=true numCPUThreads=2 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:49

C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
requestnodes [MPIWrapper]: using 2 out of 2 MPI nodes on a single host (2 requested); we (0) are in (participating)
CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:49

C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
requestnodes [MPIWrapper]: using 2 out of 2 MPI nodes on a single host (2 requested); we (1) are in (participating)
MPI Rank 0: 12/15/2016 08:29:49: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr_speechTrain.logrank0
MPI Rank 0: CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:49
MPI Rank 0: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
MPI Rank 0: 12/15/2016 08:29:49: Using 2 CPU threads.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:49: ##############################################################################
MPI Rank 0: 12/15/2016 08:29:49: #                                                                            #
MPI Rank 0: 12/15/2016 08:29:49: # speechTrain command (train action)                                         #
MPI Rank 0: 12/15/2016 08:29:49: #                                                                            #
MPI Rank 0: 12/15/2016 08:29:49: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:49: 
MPI Rank 0: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn.3'.
MPI Rank 0: reading script file glob_0000.scp ... 948 entries
MPI Rank 0: total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
MPI Rank 0: htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
MPI Rank 0: ...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
MPI Rank 0: label set 0: 129 classes
MPI Rank 0: minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
MPI Rank 0: 12/15/2016 08:29:50: 
MPI Rank 0: Model has 37 nodes. Using GPU 0.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:50: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 0: 12/15/2016 08:29:50: Evaluation criterion: err = ClassificationError
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:50: Training 779396 parameters in 8 out of 8 parameter tensors and 25 nodes with gradient:
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'layers[1].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'layers[1].Eh._._.W' (LearnableParameter operation) : [512 x 363]
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'layers[2].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'layers[2].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'layers[3].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'layers[3].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'outLayer.B' (LearnableParameter operation) : [132 x 1]
MPI Rank 0: 12/15/2016 08:29:50: 	Node 'outLayer.W' (LearnableParameter operation) : [132 x 512]
MPI Rank 0: 
MPI Rank 0: Initializing dataParallelSGD with FP32 aggregation.
MPI Rank 0: 12/15/2016 08:29:50: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 0: Setting dropout rate to 0.15.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:50: Starting Epoch 4: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 0: minibatchiterator: epoch 3: frames [61440..81920] (first utterance at frame 61440), data subset 0 of 2, with 1 datapasses
MPI Rank 0: requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:50: Starting minibatch loop, DataParallelSGD training (myRank = 0, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 0: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.82334281 * 2560; err = 0.71171875 * 2560; time = 0.5008s; samplesPerSecond = 5111.6
MPI Rank 0: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.74443290 * 2560; err = 0.69179687 * 2560; time = 0.1086s; samplesPerSecond = 23579.3
MPI Rank 0: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.72986787 * 2560; err = 0.69140625 * 2560; time = 0.1033s; samplesPerSecond = 24789.1
MPI Rank 0: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.74554855 * 2560; err = 0.68984375 * 2560; time = 0.1069s; samplesPerSecond = 23950.8
MPI Rank 0: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.58012299 * 2560; err = 0.66796875 * 2560; time = 0.1033s; samplesPerSecond = 24777.2
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 4 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.62333928 * 2560; err = 0.67226562 * 2560; time = 0.1093s; samplesPerSecond = 23425.8
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 4 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.61982588 * 2560; err = 0.67851562 * 2560; time = 0.1054s; samplesPerSecond = 24289.8
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 4 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.58207620 * 2560; err = 0.65625000 * 2560; time = 0.1082s; samplesPerSecond = 23656.6
MPI Rank 0: 12/15/2016 08:29:52: Finished Epoch[ 4 of 5]: [Training] ce = 2.68106956 * 20480; err = 0.68247070 * 20480; totalSamplesSeen = 81920; learningRatePerSample = 0.001953125; epochTime=1.3867s
MPI Rank 0: 12/15/2016 08:29:52: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn.4'
MPI Rank 0: Setting dropout rate to 0.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:52: Starting Epoch 5: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 0: minibatchiterator: epoch 4: frames [81920..102400] (first utterance at frame 81920), data subset 0 of 2, with 1 datapasses
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:52: Starting minibatch loop, DataParallelSGD training (myRank = 0, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.43513054 * 2560; err = 0.61757812 * 2560; time = 0.0936s; samplesPerSecond = 27355.7
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.42029950 * 2560; err = 0.63320312 * 2560; time = 0.0932s; samplesPerSecond = 27477.8
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.30856062 * 2560; err = 0.61132813 * 2560; time = 0.0931s; samplesPerSecond = 27501.5
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.33952171 * 2560; err = 0.61093750 * 2560; time = 0.0979s; samplesPerSecond = 26137.7
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.28281580 * 2560; err = 0.59179688 * 2560; time = 0.0935s; samplesPerSecond = 27389.9
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.18557287 * 2560; err = 0.57812500 * 2560; time = 0.0923s; samplesPerSecond = 27720.6
MPI Rank 0: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.23650560 * 2560; err = 0.58710938 * 2560; time = 0.0931s; samplesPerSecond = 27505.0
MPI Rank 0: 12/15/2016 08:29:53:  Epoch[ 5 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.17937329 * 2560; err = 0.58125000 * 2560; time = 0.0921s; samplesPerSecond = 27803.7
MPI Rank 0: 12/15/2016 08:29:53: Finished Epoch[ 5 of 5]: [Training] ce = 2.29847249 * 20480; err = 0.60141602 * 20480; totalSamplesSeen = 102400; learningRatePerSample = 0.001953125; epochTime=0.757339s
MPI Rank 0: 12/15/2016 08:29:53: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn'
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:53: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 12/15/2016 08:29:53: __COMPLETED__
MPI Rank 1: 12/15/2016 08:29:50: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr_speechTrain.logrank1
MPI Rank 1: CNTK 2.0.beta6.0+ (HEAD 5f1fab, Dec 15 2016 06:29:34) on cntk-muc03 at 2016/12/15 08:29:49
MPI Rank 1: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout/cntk.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\Dropout  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/stderr
MPI Rank 1: 12/15/2016 08:29:50: Using 2 CPU threads.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:50: ##############################################################################
MPI Rank 1: 12/15/2016 08:29:50: #                                                                            #
MPI Rank 1: 12/15/2016 08:29:50: # speechTrain command (train action)                                         #
MPI Rank 1: 12/15/2016 08:29:50: #                                                                            #
MPI Rank 1: 12/15/2016 08:29:50: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:50: 
MPI Rank 1: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20161215082748.614918\Speech\DNN_Dropout@release_gpu/models/cntkSpeech.dnn.3'.
MPI Rank 1: reading script file glob_0000.scp ... 948 entries
MPI Rank 1: total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
MPI Rank 1: htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
MPI Rank 1: ...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
MPI Rank 1: label set 0: 129 classes
MPI Rank 1: minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
MPI Rank 1: 12/15/2016 08:29:50: 
MPI Rank 1: Model has 37 nodes. Using GPU 0.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:50: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 1: 12/15/2016 08:29:50: Evaluation criterion: err = ClassificationError
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:50: Training 779396 parameters in 8 out of 8 parameter tensors and 25 nodes with gradient:
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'layers[1].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'layers[1].Eh._._.W' (LearnableParameter operation) : [512 x 363]
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'layers[2].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'layers[2].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'layers[3].Eh._._.B' (LearnableParameter operation) : [512 x 1]
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'layers[3].Eh._._.W' (LearnableParameter operation) : [512 x 512]
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'outLayer.B' (LearnableParameter operation) : [132 x 1]
MPI Rank 1: 12/15/2016 08:29:50: 	Node 'outLayer.W' (LearnableParameter operation) : [132 x 512]
MPI Rank 1: 
MPI Rank 1: Initializing dataParallelSGD with FP32 aggregation.
MPI Rank 1: 12/15/2016 08:29:50: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 1: Setting dropout rate to 0.15.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:50: Starting Epoch 4: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 1: minibatchiterator: epoch 3: frames [61440..81920] (first utterance at frame 61440), data subset 1 of 2, with 1 datapasses
MPI Rank 1: requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:50: Starting minibatch loop, DataParallelSGD training (myRank = 1, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 1: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.82334281 * 2560; err = 0.71171875 * 2560; time = 0.5313s; samplesPerSecond = 4818.7
MPI Rank 1: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.74443290 * 2560; err = 0.69179687 * 2560; time = 0.1086s; samplesPerSecond = 23582.3
MPI Rank 1: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.72986787 * 2560; err = 0.69140625 * 2560; time = 0.1033s; samplesPerSecond = 24774.5
MPI Rank 1: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.74554855 * 2560; err = 0.68984375 * 2560; time = 0.1070s; samplesPerSecond = 23936.2
MPI Rank 1: 12/15/2016 08:29:51:  Epoch[ 4 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.58012299 * 2560; err = 0.66796875 * 2560; time = 0.1032s; samplesPerSecond = 24800.7
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 4 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.62333928 * 2560; err = 0.67226562 * 2560; time = 0.1082s; samplesPerSecond = 23666.7
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 4 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.61982588 * 2560; err = 0.67851562 * 2560; time = 0.1054s; samplesPerSecond = 24293.5
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 4 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.58207620 * 2560; err = 0.65625000 * 2560; time = 0.1082s; samplesPerSecond = 23660.8
MPI Rank 1: 12/15/2016 08:29:52: Finished Epoch[ 4 of 5]: [Training] ce = 2.68106956 * 20480; err = 0.68247070 * 20480; totalSamplesSeen = 81920; learningRatePerSample = 0.001953125; epochTime=1.38614s
MPI Rank 1: Setting dropout rate to 0.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:52: Starting Epoch 5: learning rate per sample = 0.001953  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
MPI Rank 1: minibatchiterator: epoch 4: frames [81920..102400] (first utterance at frame 81920), data subset 1 of 2, with 1 datapasses
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:52: Starting minibatch loop, DataParallelSGD training (myRank = 1, numNodes = 2, numGradientBits = 32), distributed reading is ENABLED.
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[   1-  10, 12.50%]: ce = 2.43513054 * 2560; err = 0.61757812 * 2560; time = 0.0945s; samplesPerSecond = 27082.8
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  11-  20, 25.00%]: ce = 2.42029950 * 2560; err = 0.63320312 * 2560; time = 0.0931s; samplesPerSecond = 27493.8
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  21-  30, 37.50%]: ce = 2.30856062 * 2560; err = 0.61132813 * 2560; time = 0.0920s; samplesPerSecond = 27818.2
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  31-  40, 50.00%]: ce = 2.33952171 * 2560; err = 0.61093750 * 2560; time = 0.0979s; samplesPerSecond = 26157.9
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  41-  50, 62.50%]: ce = 2.28281580 * 2560; err = 0.59179688 * 2560; time = 0.0936s; samplesPerSecond = 27359.2
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  51-  60, 75.00%]: ce = 2.18557287 * 2560; err = 0.57812500 * 2560; time = 0.0922s; samplesPerSecond = 27754.0
MPI Rank 1: 12/15/2016 08:29:52:  Epoch[ 5 of 5]-Minibatch[  61-  70, 87.50%]: ce = 2.23650560 * 2560; err = 0.58710938 * 2560; time = 0.0931s; samplesPerSecond = 27497.0
MPI Rank 1: 12/15/2016 08:29:53:  Epoch[ 5 of 5]-Minibatch[  71-  80, 100.00%]: ce = 2.17937329 * 2560; err = 0.58125000 * 2560; time = 0.0919s; samplesPerSecond = 27852.1
MPI Rank 1: 12/15/2016 08:29:53: Finished Epoch[ 5 of 5]: [Training] ce = 2.29847249 * 20480; err = 0.60141602 * 20480; totalSamplesSeen = 102400; learningRatePerSample = 0.001953125; epochTime=0.757862s
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:53: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 12/15/2016 08:29:53: __COMPLETED__

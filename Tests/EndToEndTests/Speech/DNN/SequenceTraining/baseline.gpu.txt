=== Running /home/mluser/src/cplx_master/build/debug/bin/cntk configFile=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/cntk_sequence.config RunDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu DataDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData ConfigDir=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining DeviceId=0 currentDirectory=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData
running on localhost at 2015/11/19 17:25:45
command line: 
/home/mluser/src/cplx_master/build/debug/bin/cntk configFile=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/cntk_sequence.config RunDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu DataDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData ConfigDir=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining DeviceId=0 currentDirectory=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision=float
deviceId=$DeviceId$
command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros=$ConfigDir$/macros.txt
GlobalMean=GlobalStats/mean.363
GlobalInvStd=GlobalStats/var.363
GlobalPrior=GlobalStats/prior.132
traceLevel=1
Truncated=false
SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]
DPT_Pre1=[
    action=train
    modelPath=$RunDir$/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/dnn_1layer.txt
    ]
]
AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=$RunDir$/models/Pre1/cntkSpeech
    NewModel=$RunDir$/models/Pre2/cntkSpeech.0
    editPath=$ConfigDir$/add_layer.mel
]
DPT_Pre2=[
    action=train
    modelPath=$RunDir$/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/dnn_1layer.txt
    ]
]
AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=$RunDir$/models/Pre2/cntkSpeech
    NewModel=$RunDir$/models/cntkSpeech.0
    editPath=$ConfigDir$/add_layer.mel
]
speechTrain=[
    action=train
    modelPath=$RunDir$/models/cntkSpeech
    deviceId=$DeviceId$
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]
reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=$DataDir$/glob_0000.scp
  ]
  labels=[
      mlfFile=$DataDir$/glob_0000.mlf
      labelMappingFile=$DataDir$/state.list
      labelDim=132
      labelType=Category
  ]
]
replaceCriterionNode=[
    action=edit
    CurrModel=$RunDir$/models/cntkSpeech
    NewModel=$RunDir$/models/cntkSpeech.sequence.0
    editPath=$ConfigDir$/replace_ce_with_sequence_criterion.mel
]
sequenceTrain=[
    action=train
    modelPath=$RunDir$/models/cntkSpeech.sequence
    deviceId=$DeviceId$
    traceLevel=1
    NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/nonexistentfile.txt
    ]
  SGD=[
      epochSize=81920
      minibatchSize=10
      learningRatesPerSample=0.000002
      momentumPerSample=0.999589
      dropoutRate=0.0
      maxEpochs=3
      hsmoothingWeight=0.95
      frameDropThresh=1e-10
      numMBsToShowResult=10
      gradientClippingWithTruncation=true
      clippingThresholdPerSample=1.0
  ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      frameMode=false
      nbruttsineachrecurrentiter=2
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=$DataDir$/glob_0000.scp
      ]
      labels=[
          mlfFile=$DataDir$/glob_0000.mlf
          labelMappingFile=$DataDir$/state.list
          labelDim=132
          labelType=Category
      ]
      hmms=[
          phoneFile=$DataDir$/model.overalltying
          transpFile=$DataDir$/model.transprob
      ]
      lattices=[
          denlatTocFile=$DataDir$/*.lats.toc
      ]
    ]
]
RunDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu
DataDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData
ConfigDir=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining
DeviceId=0
currentDirectory=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision=float
deviceId=0
command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/macros.txt
GlobalMean=GlobalStats/mean.363
GlobalInvStd=GlobalStats/var.363
GlobalPrior=GlobalStats/prior.132
traceLevel=1
Truncated=false
SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]
DPT_Pre1=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/dnn_1layer.txt
    ]
]
AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NewModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/add_layer.mel
]
DPT_Pre2=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/dnn_1layer.txt
    ]
]
AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NewModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/add_layer.mel
]
speechTrain=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    deviceId=0
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]
reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp
  ]
  labels=[
      mlfFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf
      labelMappingFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
      labelDim=132
      labelType=Category
  ]
]
replaceCriterionNode=[
    action=edit
    CurrModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    NewModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel
]
sequenceTrain=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence
    deviceId=0
    traceLevel=1
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/nonexistentfile.txt
    ]
  SGD=[
      epochSize=81920
      minibatchSize=10
      learningRatesPerSample=0.000002
      momentumPerSample=0.999589
      dropoutRate=0.0
      maxEpochs=3
      hsmoothingWeight=0.95
      frameDropThresh=1e-10
      numMBsToShowResult=10
      gradientClippingWithTruncation=true
      clippingThresholdPerSample=1.0
  ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      frameMode=false
      nbruttsineachrecurrentiter=2
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp
      ]
      labels=[
          mlfFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf
          labelMappingFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
          labelDim=132
          labelType=Category
      ]
      hmms=[
          phoneFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.overalltying
          transpFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.transprob
      ]
      lattices=[
          denlatTocFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/*.lats.toc
      ]
    ]
]
RunDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu
DataDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData
ConfigDir=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining
DeviceId=0
currentDirectory=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.config:AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NewModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/add_layer.mel
]

configparameters: cntk_sequence.config:AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NewModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/add_layer.mel
]

configparameters: cntk_sequence.config:command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.config:ConfigDir=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining
configparameters: cntk_sequence.config:currentDirectory=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData
configparameters: cntk_sequence.config:DataDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData
configparameters: cntk_sequence.config:deviceId=0
configparameters: cntk_sequence.config:DPT_Pre1=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/dnn_1layer.txt
    ]
]

configparameters: cntk_sequence.config:DPT_Pre2=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/dnn_1layer.txt
    ]
]

configparameters: cntk_sequence.config:GlobalInvStd=GlobalStats/var.363
configparameters: cntk_sequence.config:GlobalMean=GlobalStats/mean.363
configparameters: cntk_sequence.config:GlobalPrior=GlobalStats/prior.132
configparameters: cntk_sequence.config:ndlMacros=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/macros.txt
configparameters: cntk_sequence.config:precision=float
configparameters: cntk_sequence.config:reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp
  ]
  labels=[
      mlfFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf
      labelMappingFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
      labelDim=132
      labelType=Category
  ]
]

configparameters: cntk_sequence.config:replaceCriterionNode=[
    action=edit
    CurrModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    NewModel=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel
]

configparameters: cntk_sequence.config:RunDir=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu
configparameters: cntk_sequence.config:sequenceTrain=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence
    deviceId=0
    traceLevel=1
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/nonexistentfile.txt
    ]
  SGD=[
      epochSize=81920
      minibatchSize=10
      learningRatesPerSample=0.000002
      momentumPerSample=0.999589
      dropoutRate=0.0
      maxEpochs=3
      hsmoothingWeight=0.95
      frameDropThresh=1e-10
      numMBsToShowResult=10
      gradientClippingWithTruncation=true
      clippingThresholdPerSample=1.0
  ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      frameMode=false
      nbruttsineachrecurrentiter=2
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp
      ]
      labels=[
          mlfFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf
          labelMappingFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
          labelDim=132
          labelType=Category
      ]
      hmms=[
          phoneFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.overalltying
          transpFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.transprob
      ]
      lattices=[
          denlatTocFile=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/*.lats.toc
      ]
    ]
]

configparameters: cntk_sequence.config:SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]

configparameters: cntk_sequence.config:speechTrain=[
    action=train
    modelPath=/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    deviceId=0
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/DNN/SequenceTraining/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]

configparameters: cntk_sequence.config:traceLevel=1
configparameters: cntk_sequence.config:Truncated=false
<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: DPT_Pre1 AddLayer2 DPT_Pre2 AddLayer3 speechTrain replaceCriterionNode sequenceTrain 
precision = float
CNTKModelPath: /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
CNTKCommandTrainInfo: DPT_Pre1 : 2
CNTKModelPath: /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
CNTKCommandTrainInfo: DPT_Pre2 : 2
CNTKModelPath: /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech
CNTKCommandTrainInfo: speechTrain : 4
CNTKModelPath: /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence
CNTKCommandTrainInfo: sequenceTrain : 3
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11
CNTKCommandTrainBegin: DPT_Pre1
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 1], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 1], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 1] = InputValue
HL1.W[512, 363] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 1] = InputValue

Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.

SetUniformRandomValue (GPU): creating curand object with seed 1
GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  3.74183807; EvalErr[0]PerSample = 0.80195313; TotalTime = 2.87763s; TotalTimePerSample = 1.12407ms; SamplesPerSecond = 889
 Epoch[ 1 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.91124802; EvalErr[0]PerSample = 0.70898438; TotalTime = 0.07673s; TotalTimePerSample = 0.02997ms; SamplesPerSecond = 33362
 Epoch[ 1 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.58016052; EvalErr[0]PerSample = 0.66640625; TotalTime = 0.07656s; TotalTimePerSample = 0.02990ms; SamplesPerSecond = 33439
 Epoch[ 1 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.27427139; EvalErr[0]PerSample = 0.58750000; TotalTime = 0.07647s; TotalTimePerSample = 0.02987ms; SamplesPerSecond = 33476
 Epoch[ 1 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.05503540; EvalErr[0]PerSample = 0.56093750; TotalTime = 0.07661s; TotalTimePerSample = 0.02992ms; SamplesPerSecond = 33417
 Epoch[ 1 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.91055145; EvalErr[0]PerSample = 0.52812500; TotalTime = 0.07668s; TotalTimePerSample = 0.02995ms; SamplesPerSecond = 33387
 Epoch[ 1 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.81562653; EvalErr[0]PerSample = 0.51171875; TotalTime = 0.07653s; TotalTimePerSample = 0.02989ms; SamplesPerSecond = 33452
 Epoch[ 1 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.68803253; EvalErr[0]PerSample = 0.48476562; TotalTime = 0.07671s; TotalTimePerSample = 0.02996ms; SamplesPerSecond = 33372
 Epoch[ 1 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.57382050; EvalErr[0]PerSample = 0.45429687; TotalTime = 0.07651s; TotalTimePerSample = 0.02989ms; SamplesPerSecond = 33458
 Epoch[ 1 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.62090149; EvalErr[0]PerSample = 0.47304687; TotalTime = 0.07649s; TotalTimePerSample = 0.02988ms; SamplesPerSecond = 33469
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.59272461; EvalErr[0]PerSample = 0.47500000; TotalTime = 0.07664s; TotalTimePerSample = 0.02994ms; SamplesPerSecond = 33401
 Epoch[ 1 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.51520386; EvalErr[0]PerSample = 0.44531250; TotalTime = 0.07659s; TotalTimePerSample = 0.02992ms; SamplesPerSecond = 33422
 Epoch[ 1 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.49181976; EvalErr[0]PerSample = 0.45039062; TotalTime = 0.07474s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34252
 Epoch[ 1 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.53703613; EvalErr[0]PerSample = 0.44804688; TotalTime = 0.07474s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34250
 Epoch[ 1 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.43095398; EvalErr[0]PerSample = 0.41640625; TotalTime = 0.07477s; TotalTimePerSample = 0.02921ms; SamplesPerSecond = 34236
 Epoch[ 1 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41503601; EvalErr[0]PerSample = 0.40078125; TotalTime = 0.07462s; TotalTimePerSample = 0.02915ms; SamplesPerSecond = 34307
 Epoch[ 1 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.38913574; EvalErr[0]PerSample = 0.41132812; TotalTime = 0.07465s; TotalTimePerSample = 0.02916ms; SamplesPerSecond = 34295
 Epoch[ 1 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41207886; EvalErr[0]PerSample = 0.42226562; TotalTime = 0.07466s; TotalTimePerSample = 0.02916ms; SamplesPerSecond = 34288
 Epoch[ 1 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.39968262; EvalErr[0]PerSample = 0.40664062; TotalTime = 0.07471s; TotalTimePerSample = 0.02918ms; SamplesPerSecond = 34264
 Epoch[ 1 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.42729187; EvalErr[0]PerSample = 0.42617187; TotalTime = 0.07472s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34261
 Epoch[ 1 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41336365; EvalErr[0]PerSample = 0.42343750; TotalTime = 0.07475s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34247
 Epoch[ 1 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33186951; EvalErr[0]PerSample = 0.39960937; TotalTime = 0.07463s; TotalTimePerSample = 0.02915ms; SamplesPerSecond = 34304
 Epoch[ 1 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28581238; EvalErr[0]PerSample = 0.38710937; TotalTime = 0.07466s; TotalTimePerSample = 0.02916ms; SamplesPerSecond = 34290
 Epoch[ 1 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.34127502; EvalErr[0]PerSample = 0.40976563; TotalTime = 0.07475s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34249
 Epoch[ 1 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.32666016; EvalErr[0]PerSample = 0.39726563; TotalTime = 0.07474s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34253
 Epoch[ 1 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21437378; EvalErr[0]PerSample = 0.37265625; TotalTime = 0.07471s; TotalTimePerSample = 0.02918ms; SamplesPerSecond = 34266
 Epoch[ 1 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23749695; EvalErr[0]PerSample = 0.37343750; TotalTime = 0.07472s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34260
 Epoch[ 1 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.29956665; EvalErr[0]PerSample = 0.39023438; TotalTime = 0.07479s; TotalTimePerSample = 0.02922ms; SamplesPerSecond = 34228
 Epoch[ 1 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21198120; EvalErr[0]PerSample = 0.37382813; TotalTime = 0.07474s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34253
 Epoch[ 1 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20528259; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.07469s; TotalTimePerSample = 0.02918ms; SamplesPerSecond = 34273
 Epoch[ 1 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23613586; EvalErr[0]PerSample = 0.37343750; TotalTime = 0.07473s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34256
 Epoch[ 1 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.25615234; EvalErr[0]PerSample = 0.38164063; TotalTime = 0.06937s; TotalTimePerSample = 0.02710ms; SamplesPerSecond = 36905
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.6294507; EvalErrPerSample = 0.46030274; AvgLearningRatePerSample = 0.003125000047; EpochTime=6.064291
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23230944; EvalErr[0]PerSample = 0.38320312; TotalTime = 0.07594s; TotalTimePerSample = 0.02966ms; SamplesPerSecond = 33711
 Epoch[ 2 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20511351; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.07475s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34249
 Epoch[ 2 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28783760; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.07472s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34262
 Epoch[ 2 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22809334; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.07473s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34257
 Epoch[ 2 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18090286; EvalErr[0]PerSample = 0.35468750; TotalTime = 0.07468s; TotalTimePerSample = 0.02917ms; SamplesPerSecond = 34279
 Epoch[ 2 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28175354; EvalErr[0]PerSample = 0.37695312; TotalTime = 0.07469s; TotalTimePerSample = 0.02917ms; SamplesPerSecond = 34276
 Epoch[ 2 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22251205; EvalErr[0]PerSample = 0.37382813; TotalTime = 0.07480s; TotalTimePerSample = 0.02922ms; SamplesPerSecond = 34222
 Epoch[ 2 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17863007; EvalErr[0]PerSample = 0.36328125; TotalTime = 0.07475s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34247
 Epoch[ 2 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23061218; EvalErr[0]PerSample = 0.35742188; TotalTime = 0.07465s; TotalTimePerSample = 0.02916ms; SamplesPerSecond = 34291
 Epoch[ 2 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18048782; EvalErr[0]PerSample = 0.37578125; TotalTime = 0.07491s; TotalTimePerSample = 0.02926ms; SamplesPerSecond = 34173
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19648056; EvalErr[0]PerSample = 0.35976562; TotalTime = 0.07473s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34258
 Epoch[ 2 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18896942; EvalErr[0]PerSample = 0.35429688; TotalTime = 0.07474s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34249
 Epoch[ 2 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16628113; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.07471s; TotalTimePerSample = 0.02918ms; SamplesPerSecond = 34264
 Epoch[ 2 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12856445; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.07477s; TotalTimePerSample = 0.02921ms; SamplesPerSecond = 34238
 Epoch[ 2 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10083466; EvalErr[0]PerSample = 0.32617188; TotalTime = 0.07477s; TotalTimePerSample = 0.02921ms; SamplesPerSecond = 34236
 Epoch[ 2 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09875336; EvalErr[0]PerSample = 0.33906250; TotalTime = 0.07486s; TotalTimePerSample = 0.02924ms; SamplesPerSecond = 34197
 Epoch[ 2 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18634949; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.07471s; TotalTimePerSample = 0.02918ms; SamplesPerSecond = 34266
 Epoch[ 2 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.15709991; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.07476s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34242
 Epoch[ 2 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10971069; EvalErr[0]PerSample = 0.34960938; TotalTime = 0.07468s; TotalTimePerSample = 0.02917ms; SamplesPerSecond = 34280
 Epoch[ 2 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11317139; EvalErr[0]PerSample = 0.35000000; TotalTime = 0.07477s; TotalTimePerSample = 0.02921ms; SamplesPerSecond = 34239
 Epoch[ 2 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08727722; EvalErr[0]PerSample = 0.32578125; TotalTime = 0.07471s; TotalTimePerSample = 0.02918ms; SamplesPerSecond = 34267
 Epoch[ 2 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12295990; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.07473s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34256
 Epoch[ 2 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12966003; EvalErr[0]PerSample = 0.35078125; TotalTime = 0.07465s; TotalTimePerSample = 0.02916ms; SamplesPerSecond = 34292
 Epoch[ 2 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27489319; EvalErr[0]PerSample = 0.39257812; TotalTime = 0.07472s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34260
 Epoch[ 2 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17423401; EvalErr[0]PerSample = 0.35156250; TotalTime = 0.07473s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34254
 Epoch[ 2 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13240051; EvalErr[0]PerSample = 0.35625000; TotalTime = 0.07475s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34246
 Epoch[ 2 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13792114; EvalErr[0]PerSample = 0.34335938; TotalTime = 0.07478s; TotalTimePerSample = 0.02921ms; SamplesPerSecond = 34233
 Epoch[ 2 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13433228; EvalErr[0]PerSample = 0.33710937; TotalTime = 0.07473s; TotalTimePerSample = 0.02919ms; SamplesPerSecond = 34255
 Epoch[ 2 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.05835876; EvalErr[0]PerSample = 0.33710937; TotalTime = 0.07479s; TotalTimePerSample = 0.02922ms; SamplesPerSecond = 34228
 Epoch[ 2 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09596558; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.07488s; TotalTimePerSample = 0.02925ms; SamplesPerSecond = 34185
 Epoch[ 2 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08180847; EvalErr[0]PerSample = 0.33242187; TotalTime = 0.07475s; TotalTimePerSample = 0.02920ms; SamplesPerSecond = 34248
 Epoch[ 2 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06572876; EvalErr[0]PerSample = 0.33632812; TotalTime = 0.06930s; TotalTimePerSample = 0.02707ms; SamplesPerSecond = 36938
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1615628; EvalErrPerSample = 0.35460207; AvgLearningRatePerSample = 0.003125000047; EpochTime=2.397192
CNTKCommandTrainEnd: DPT_Pre1


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[1, 1] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[132, 0])
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: DPT_Pre2
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0.


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  4.30124588; EvalErr[0]PerSample = 0.80703125; TotalTime = 0.10277s; TotalTimePerSample = 0.04014ms; SamplesPerSecond = 24909
 Epoch[ 1 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.75448074; EvalErr[0]PerSample = 0.69960937; TotalTime = 0.09954s; TotalTimePerSample = 0.03888ms; SamplesPerSecond = 25719
 Epoch[ 1 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.20926208; EvalErr[0]PerSample = 0.58515625; TotalTime = 0.09955s; TotalTimePerSample = 0.03889ms; SamplesPerSecond = 25715
 Epoch[ 1 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.88578110; EvalErr[0]PerSample = 0.50117188; TotalTime = 0.09957s; TotalTimePerSample = 0.03889ms; SamplesPerSecond = 25711
 Epoch[ 1 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.71906204; EvalErr[0]PerSample = 0.47773437; TotalTime = 0.09952s; TotalTimePerSample = 0.03887ms; SamplesPerSecond = 25723
 Epoch[ 1 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.60130463; EvalErr[0]PerSample = 0.44648437; TotalTime = 0.09945s; TotalTimePerSample = 0.03885ms; SamplesPerSecond = 25741
 Epoch[ 1 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.56077118; EvalErr[0]PerSample = 0.45000000; TotalTime = 0.09935s; TotalTimePerSample = 0.03881ms; SamplesPerSecond = 25767
 Epoch[ 1 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.47116547; EvalErr[0]PerSample = 0.42460938; TotalTime = 0.10115s; TotalTimePerSample = 0.03951ms; SamplesPerSecond = 25308
 Epoch[ 1 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.38874512; EvalErr[0]PerSample = 0.40781250; TotalTime = 0.10145s; TotalTimePerSample = 0.03963ms; SamplesPerSecond = 25233
 Epoch[ 1 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41911163; EvalErr[0]PerSample = 0.42539063; TotalTime = 0.10123s; TotalTimePerSample = 0.03954ms; SamplesPerSecond = 25289
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.38730774; EvalErr[0]PerSample = 0.42148438; TotalTime = 0.10150s; TotalTimePerSample = 0.03965ms; SamplesPerSecond = 25220
 Epoch[ 1 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.36617889; EvalErr[0]PerSample = 0.41015625; TotalTime = 0.10141s; TotalTimePerSample = 0.03961ms; SamplesPerSecond = 25243
 Epoch[ 1 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33381653; EvalErr[0]PerSample = 0.40781250; TotalTime = 0.10125s; TotalTimePerSample = 0.03955ms; SamplesPerSecond = 25284
 Epoch[ 1 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.39802246; EvalErr[0]PerSample = 0.40546875; TotalTime = 0.10113s; TotalTimePerSample = 0.03950ms; SamplesPerSecond = 25314
 Epoch[ 1 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33336182; EvalErr[0]PerSample = 0.40195313; TotalTime = 0.10126s; TotalTimePerSample = 0.03955ms; SamplesPerSecond = 25282
 Epoch[ 1 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33834229; EvalErr[0]PerSample = 0.40195313; TotalTime = 0.10108s; TotalTimePerSample = 0.03948ms; SamplesPerSecond = 25327
 Epoch[ 1 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.26663208; EvalErr[0]PerSample = 0.37578125; TotalTime = 0.10114s; TotalTimePerSample = 0.03951ms; SamplesPerSecond = 25311
 Epoch[ 1 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28086243; EvalErr[0]PerSample = 0.39296875; TotalTime = 0.10123s; TotalTimePerSample = 0.03954ms; SamplesPerSecond = 25289
 Epoch[ 1 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.29481506; EvalErr[0]PerSample = 0.39531250; TotalTime = 0.10156s; TotalTimePerSample = 0.03967ms; SamplesPerSecond = 25207
 Epoch[ 1 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27625122; EvalErr[0]PerSample = 0.39375000; TotalTime = 0.10130s; TotalTimePerSample = 0.03957ms; SamplesPerSecond = 25271
 Epoch[ 1 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.26905518; EvalErr[0]PerSample = 0.38984375; TotalTime = 0.10149s; TotalTimePerSample = 0.03964ms; SamplesPerSecond = 25224
 Epoch[ 1 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21494751; EvalErr[0]PerSample = 0.36250000; TotalTime = 0.10152s; TotalTimePerSample = 0.03966ms; SamplesPerSecond = 25217
 Epoch[ 1 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20699158; EvalErr[0]PerSample = 0.36914062; TotalTime = 0.10137s; TotalTimePerSample = 0.03960ms; SamplesPerSecond = 25253
 Epoch[ 1 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.25002136; EvalErr[0]PerSample = 0.37851563; TotalTime = 0.10134s; TotalTimePerSample = 0.03959ms; SamplesPerSecond = 25260
 Epoch[ 1 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22617493; EvalErr[0]PerSample = 0.37656250; TotalTime = 0.10117s; TotalTimePerSample = 0.03952ms; SamplesPerSecond = 25303
 Epoch[ 1 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14840393; EvalErr[0]PerSample = 0.35468750; TotalTime = 0.10115s; TotalTimePerSample = 0.03951ms; SamplesPerSecond = 25308
 Epoch[ 1 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16649780; EvalErr[0]PerSample = 0.35468750; TotalTime = 0.10118s; TotalTimePerSample = 0.03952ms; SamplesPerSecond = 25302
 Epoch[ 1 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22885742; EvalErr[0]PerSample = 0.36992188; TotalTime = 0.10138s; TotalTimePerSample = 0.03960ms; SamplesPerSecond = 25250
 Epoch[ 1 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16533203; EvalErr[0]PerSample = 0.36484375; TotalTime = 0.10144s; TotalTimePerSample = 0.03963ms; SamplesPerSecond = 25235
 Epoch[ 1 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17502136; EvalErr[0]PerSample = 0.35664062; TotalTime = 0.10139s; TotalTimePerSample = 0.03960ms; SamplesPerSecond = 25250
 Epoch[ 1 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16159058; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.10142s; TotalTimePerSample = 0.03962ms; SamplesPerSecond = 25242
 Epoch[ 1 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17113953; EvalErr[0]PerSample = 0.35429688; TotalTime = 0.09582s; TotalTimePerSample = 0.03743ms; SamplesPerSecond = 26715
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.4990798; EvalErrPerSample = 0.42547607; AvgLearningRatePerSample = 0.003125000047; EpochTime=4.080698
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14215412; EvalErr[0]PerSample = 0.34882812; TotalTime = 0.10278s; TotalTimePerSample = 0.04015ms; SamplesPerSecond = 24907
 Epoch[ 2 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17049236; EvalErr[0]PerSample = 0.36328125; TotalTime = 0.10145s; TotalTimePerSample = 0.03963ms; SamplesPerSecond = 25233
 Epoch[ 2 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.24373856; EvalErr[0]PerSample = 0.37460938; TotalTime = 0.10161s; TotalTimePerSample = 0.03969ms; SamplesPerSecond = 25193
 Epoch[ 2 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18655586; EvalErr[0]PerSample = 0.36445312; TotalTime = 0.10116s; TotalTimePerSample = 0.03951ms; SamplesPerSecond = 25307
 Epoch[ 2 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13848000; EvalErr[0]PerSample = 0.35039063; TotalTime = 0.10149s; TotalTimePerSample = 0.03965ms; SamplesPerSecond = 25223
 Epoch[ 2 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21884232; EvalErr[0]PerSample = 0.36757812; TotalTime = 0.10116s; TotalTimePerSample = 0.03952ms; SamplesPerSecond = 25305
 Epoch[ 2 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14372940; EvalErr[0]PerSample = 0.35000000; TotalTime = 0.10115s; TotalTimePerSample = 0.03951ms; SamplesPerSecond = 25308
 Epoch[ 2 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12769089; EvalErr[0]PerSample = 0.34960938; TotalTime = 0.10110s; TotalTimePerSample = 0.03949ms; SamplesPerSecond = 25321
 Epoch[ 2 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14114227; EvalErr[0]PerSample = 0.33554688; TotalTime = 0.10107s; TotalTimePerSample = 0.03948ms; SamplesPerSecond = 25329
 Epoch[ 2 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12445145; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.10139s; TotalTimePerSample = 0.03961ms; SamplesPerSecond = 25249
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14137955; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.10166s; TotalTimePerSample = 0.03971ms; SamplesPerSecond = 25181
 Epoch[ 2 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12705154; EvalErr[0]PerSample = 0.33867188; TotalTime = 0.10147s; TotalTimePerSample = 0.03964ms; SamplesPerSecond = 25229
 Epoch[ 2 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10779419; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.10135s; TotalTimePerSample = 0.03959ms; SamplesPerSecond = 25258
 Epoch[ 2 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.07003021; EvalErr[0]PerSample = 0.32500000; TotalTime = 0.10116s; TotalTimePerSample = 0.03952ms; SamplesPerSecond = 25306
 Epoch[ 2 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.05308990; EvalErr[0]PerSample = 0.31406250; TotalTime = 0.10121s; TotalTimePerSample = 0.03953ms; SamplesPerSecond = 25294
 Epoch[ 2 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06392975; EvalErr[0]PerSample = 0.33085938; TotalTime = 0.10125s; TotalTimePerSample = 0.03955ms; SamplesPerSecond = 25284
 Epoch[ 2 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14430847; EvalErr[0]PerSample = 0.35507813; TotalTime = 0.10118s; TotalTimePerSample = 0.03952ms; SamplesPerSecond = 25302
 Epoch[ 2 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14809570; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.10116s; TotalTimePerSample = 0.03952ms; SamplesPerSecond = 25306
 Epoch[ 2 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08184509; EvalErr[0]PerSample = 0.33515625; TotalTime = 0.10136s; TotalTimePerSample = 0.03959ms; SamplesPerSecond = 25257
 Epoch[ 2 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.07637024; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.10142s; TotalTimePerSample = 0.03962ms; SamplesPerSecond = 25240
 Epoch[ 2 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06249695; EvalErr[0]PerSample = 0.32500000; TotalTime = 0.10148s; TotalTimePerSample = 0.03964ms; SamplesPerSecond = 25227
 Epoch[ 2 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09361877; EvalErr[0]PerSample = 0.33320312; TotalTime = 0.10143s; TotalTimePerSample = 0.03962ms; SamplesPerSecond = 25239
 Epoch[ 2 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12118530; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.10136s; TotalTimePerSample = 0.03959ms; SamplesPerSecond = 25256
 Epoch[ 2 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13457642; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.10126s; TotalTimePerSample = 0.03955ms; SamplesPerSecond = 25282
 Epoch[ 2 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09024963; EvalErr[0]PerSample = 0.33984375; TotalTime = 0.10125s; TotalTimePerSample = 0.03955ms; SamplesPerSecond = 25284
 Epoch[ 2 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.07457275; EvalErr[0]PerSample = 0.33164063; TotalTime = 0.10118s; TotalTimePerSample = 0.03952ms; SamplesPerSecond = 25301
 Epoch[ 2 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.05975952; EvalErr[0]PerSample = 0.32070312; TotalTime = 0.10119s; TotalTimePerSample = 0.03953ms; SamplesPerSecond = 25298
 Epoch[ 2 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09778137; EvalErr[0]PerSample = 0.33242187; TotalTime = 0.10123s; TotalTimePerSample = 0.03954ms; SamplesPerSecond = 25289
 Epoch[ 2 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.01963196; EvalErr[0]PerSample = 0.32539062; TotalTime = 0.10139s; TotalTimePerSample = 0.03960ms; SamplesPerSecond = 25250
 Epoch[ 2 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.07533875; EvalErr[0]PerSample = 0.33515625; TotalTime = 0.10161s; TotalTimePerSample = 0.03969ms; SamplesPerSecond = 25195
 Epoch[ 2 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06417236; EvalErr[0]PerSample = 0.33007812; TotalTime = 0.10142s; TotalTimePerSample = 0.03962ms; SamplesPerSecond = 25242
 Epoch[ 2 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.04990234; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.09606s; TotalTimePerSample = 0.03752ms; SamplesPerSecond = 26651
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1123269; EvalErrPerSample = 0.34179688; AvgLearningRatePerSample = 0.003125000047; EpochTime=3.247955
CNTKCommandTrainEnd: DPT_Pre2


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[1, 1] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[132, 0])
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0.


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 13 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  3.97086334; EvalErr[0]PerSample = 0.81445312; TotalTime = 0.12962s; TotalTimePerSample = 0.05063ms; SamplesPerSecond = 19750
 Epoch[ 1 of 4]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.63975830; EvalErr[0]PerSample = 0.63320312; TotalTime = 0.12626s; TotalTimePerSample = 0.04932ms; SamplesPerSecond = 20275
 Epoch[ 1 of 4]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.02565231; EvalErr[0]PerSample = 0.54257813; TotalTime = 0.12615s; TotalTimePerSample = 0.04928ms; SamplesPerSecond = 20294
 Epoch[ 1 of 4]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.74204865; EvalErr[0]PerSample = 0.47500000; TotalTime = 0.12607s; TotalTimePerSample = 0.04925ms; SamplesPerSecond = 20305
 Epoch[ 1 of 4]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.58343964; EvalErr[0]PerSample = 0.45156250; TotalTime = 0.12608s; TotalTimePerSample = 0.04925ms; SamplesPerSecond = 20305
 Epoch[ 1 of 4]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.47893143; EvalErr[0]PerSample = 0.42343750; TotalTime = 0.12602s; TotalTimePerSample = 0.04923ms; SamplesPerSecond = 20313
 Epoch[ 1 of 4]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.43405457; EvalErr[0]PerSample = 0.40898438; TotalTime = 0.12615s; TotalTimePerSample = 0.04928ms; SamplesPerSecond = 20293
 Epoch[ 1 of 4]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.35973663; EvalErr[0]PerSample = 0.39648438; TotalTime = 0.12595s; TotalTimePerSample = 0.04920ms; SamplesPerSecond = 20326
 Epoch[ 1 of 4]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28108978; EvalErr[0]PerSample = 0.37968750; TotalTime = 0.12608s; TotalTimePerSample = 0.04925ms; SamplesPerSecond = 20303
 Epoch[ 1 of 4]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.29773560; EvalErr[0]PerSample = 0.39765625; TotalTime = 0.12631s; TotalTimePerSample = 0.04934ms; SamplesPerSecond = 20267
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 4]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28441925; EvalErr[0]PerSample = 0.39062500; TotalTime = 0.12629s; TotalTimePerSample = 0.04933ms; SamplesPerSecond = 20271
 Epoch[ 1 of 4]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27777252; EvalErr[0]PerSample = 0.38164063; TotalTime = 0.12640s; TotalTimePerSample = 0.04937ms; SamplesPerSecond = 20253
 Epoch[ 1 of 4]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23615112; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.12600s; TotalTimePerSample = 0.04922ms; SamplesPerSecond = 20316
 Epoch[ 1 of 4]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.31171112; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.12609s; TotalTimePerSample = 0.04925ms; SamplesPerSecond = 20303
 Epoch[ 1 of 4]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.25573883; EvalErr[0]PerSample = 0.37773438; TotalTime = 0.12615s; TotalTimePerSample = 0.04928ms; SamplesPerSecond = 20293
 Epoch[ 1 of 4]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27382965; EvalErr[0]PerSample = 0.38398437; TotalTime = 0.12602s; TotalTimePerSample = 0.04923ms; SamplesPerSecond = 20313
 Epoch[ 1 of 4]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20634155; EvalErr[0]PerSample = 0.36406250; TotalTime = 0.12604s; TotalTimePerSample = 0.04923ms; SamplesPerSecond = 20311
 Epoch[ 1 of 4]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20973816; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.12598s; TotalTimePerSample = 0.04921ms; SamplesPerSecond = 20320
 Epoch[ 1 of 4]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20688782; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.12609s; TotalTimePerSample = 0.04925ms; SamplesPerSecond = 20303
 Epoch[ 1 of 4]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20260315; EvalErr[0]PerSample = 0.37226562; TotalTime = 0.12618s; TotalTimePerSample = 0.04929ms; SamplesPerSecond = 20289
 Epoch[ 1 of 4]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20553894; EvalErr[0]PerSample = 0.37187500; TotalTime = 0.12606s; TotalTimePerSample = 0.04924ms; SamplesPerSecond = 20307
 Epoch[ 1 of 4]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14160156; EvalErr[0]PerSample = 0.34726563; TotalTime = 0.12612s; TotalTimePerSample = 0.04927ms; SamplesPerSecond = 20297
 Epoch[ 1 of 4]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.15316467; EvalErr[0]PerSample = 0.35273437; TotalTime = 0.12606s; TotalTimePerSample = 0.04924ms; SamplesPerSecond = 20307
 Epoch[ 1 of 4]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19352417; EvalErr[0]PerSample = 0.35468750; TotalTime = 0.12609s; TotalTimePerSample = 0.04925ms; SamplesPerSecond = 20303
 Epoch[ 1 of 4]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17192078; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.12616s; TotalTimePerSample = 0.04928ms; SamplesPerSecond = 20291
 Epoch[ 1 of 4]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08281555; EvalErr[0]PerSample = 0.33867188; TotalTime = 0.12597s; TotalTimePerSample = 0.04921ms; SamplesPerSecond = 20322
 Epoch[ 1 of 4]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11028442; EvalErr[0]PerSample = 0.34453125; TotalTime = 0.12610s; TotalTimePerSample = 0.04926ms; SamplesPerSecond = 20301
 Epoch[ 1 of 4]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17454224; EvalErr[0]PerSample = 0.35312500; TotalTime = 0.12593s; TotalTimePerSample = 0.04919ms; SamplesPerSecond = 20329
 Epoch[ 1 of 4]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11068115; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.12620s; TotalTimePerSample = 0.04930ms; SamplesPerSecond = 20285
 Epoch[ 1 of 4]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12955627; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.12609s; TotalTimePerSample = 0.04926ms; SamplesPerSecond = 20302
 Epoch[ 1 of 4]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12482300; EvalErr[0]PerSample = 0.34570312; TotalTime = 0.12601s; TotalTimePerSample = 0.04922ms; SamplesPerSecond = 20315
 Epoch[ 1 of 4]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12771912; EvalErr[0]PerSample = 0.34453125; TotalTime = 0.12070s; TotalTimePerSample = 0.04715ms; SamplesPerSecond = 21209
Finished Epoch[ 1 of 4]: [Training Set] TrainLossPerSample = 1.4063962; EvalErrPerSample = 0.40274659; AvgLearningRatePerSample = 0.003125000047; EpochTime=4.879822
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.51739788; EvalErr[0]PerSample = 0.41425781; TotalTime = 0.18945s; TotalTimePerSample = 0.03700ms; SamplesPerSecond = 27025
 Epoch[ 2 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.25793457; EvalErr[0]PerSample = 0.37539062; TotalTime = 0.17951s; TotalTimePerSample = 0.03506ms; SamplesPerSecond = 28522
 Epoch[ 2 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.18638287; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.17928s; TotalTimePerSample = 0.03502ms; SamplesPerSecond = 28558
 Epoch[ 2 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.12794571; EvalErr[0]PerSample = 0.34218750; TotalTime = 0.17986s; TotalTimePerSample = 0.03513ms; SamplesPerSecond = 28466
 Epoch[ 2 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.14070625; EvalErr[0]PerSample = 0.34570312; TotalTime = 0.17961s; TotalTimePerSample = 0.03508ms; SamplesPerSecond = 28505
 Epoch[ 2 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.14582825; EvalErr[0]PerSample = 0.34765625; TotalTime = 0.18019s; TotalTimePerSample = 0.03519ms; SamplesPerSecond = 28414
 Epoch[ 2 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11193542; EvalErr[0]PerSample = 0.34414062; TotalTime = 0.18065s; TotalTimePerSample = 0.03528ms; SamplesPerSecond = 28341
 Epoch[ 2 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08574600; EvalErr[0]PerSample = 0.33789062; TotalTime = 0.17946s; TotalTimePerSample = 0.03505ms; SamplesPerSecond = 28530
 Epoch[ 2 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.21058884; EvalErr[0]PerSample = 0.37363281; TotalTime = 0.17892s; TotalTimePerSample = 0.03494ms; SamplesPerSecond = 28616
 Epoch[ 2 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.09668579; EvalErr[0]PerSample = 0.34335938; TotalTime = 0.17988s; TotalTimePerSample = 0.03513ms; SamplesPerSecond = 28464
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05844955; EvalErr[0]PerSample = 0.32675781; TotalTime = 0.17948s; TotalTimePerSample = 0.03505ms; SamplesPerSecond = 28527
 Epoch[ 2 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.10728455; EvalErr[0]PerSample = 0.34726563; TotalTime = 0.18050s; TotalTimePerSample = 0.03525ms; SamplesPerSecond = 28366
 Epoch[ 2 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08716888; EvalErr[0]PerSample = 0.33593750; TotalTime = 0.17907s; TotalTimePerSample = 0.03497ms; SamplesPerSecond = 28592
 Epoch[ 2 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06779022; EvalErr[0]PerSample = 0.31855469; TotalTime = 0.17932s; TotalTimePerSample = 0.03502ms; SamplesPerSecond = 28552
 Epoch[ 2 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04079590; EvalErr[0]PerSample = 0.32910156; TotalTime = 0.17972s; TotalTimePerSample = 0.03510ms; SamplesPerSecond = 28488
 Epoch[ 2 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06249695; EvalErr[0]PerSample = 0.32968750; TotalTime = 0.16769s; TotalTimePerSample = 0.03275ms; SamplesPerSecond = 30533
Finished Epoch[ 2 of 4]: [Training Set] TrainLossPerSample = 1.1440711; EvalErrPerSample = 0.34866944; AvgLearningRatePerSample = 0.003125000047; EpochTime=2.888686
Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11238871; EvalErr[0]PerSample = 0.34804687; TotalTime = 0.18104s; TotalTimePerSample = 0.03536ms; SamplesPerSecond = 28280
 Epoch[ 3 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.09456167; EvalErr[0]PerSample = 0.34121094; TotalTime = 0.18004s; TotalTimePerSample = 0.03516ms; SamplesPerSecond = 28437
 Epoch[ 3 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.10800095; EvalErr[0]PerSample = 0.34667969; TotalTime = 0.17912s; TotalTimePerSample = 0.03498ms; SamplesPerSecond = 28584
 Epoch[ 3 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.16617966; EvalErr[0]PerSample = 0.35566406; TotalTime = 0.17940s; TotalTimePerSample = 0.03504ms; SamplesPerSecond = 28540
 Epoch[ 3 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.14173546; EvalErr[0]PerSample = 0.34550781; TotalTime = 0.17842s; TotalTimePerSample = 0.03485ms; SamplesPerSecond = 28695
 Epoch[ 3 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.07876015; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.17973s; TotalTimePerSample = 0.03510ms; SamplesPerSecond = 28487
 Epoch[ 3 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08043213; EvalErr[0]PerSample = 0.33437500; TotalTime = 0.18378s; TotalTimePerSample = 0.03589ms; SamplesPerSecond = 27859
 Epoch[ 3 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.07423630; EvalErr[0]PerSample = 0.33007812; TotalTime = 0.18184s; TotalTimePerSample = 0.03552ms; SamplesPerSecond = 28156
 Epoch[ 3 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02659454; EvalErr[0]PerSample = 0.31113281; TotalTime = 0.18202s; TotalTimePerSample = 0.03555ms; SamplesPerSecond = 28128
 Epoch[ 3 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04602737; EvalErr[0]PerSample = 0.31855469; TotalTime = 0.18325s; TotalTimePerSample = 0.03579ms; SamplesPerSecond = 27939
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 3 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05524902; EvalErr[0]PerSample = 0.33613281; TotalTime = 0.17935s; TotalTimePerSample = 0.03503ms; SamplesPerSecond = 28548
 Epoch[ 3 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.07627411; EvalErr[0]PerSample = 0.33613281; TotalTime = 0.17981s; TotalTimePerSample = 0.03512ms; SamplesPerSecond = 28474
 Epoch[ 3 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05101776; EvalErr[0]PerSample = 0.31660156; TotalTime = 0.18006s; TotalTimePerSample = 0.03517ms; SamplesPerSecond = 28434
 Epoch[ 3 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.03016815; EvalErr[0]PerSample = 0.32480469; TotalTime = 0.17997s; TotalTimePerSample = 0.03515ms; SamplesPerSecond = 28448
 Epoch[ 3 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04644623; EvalErr[0]PerSample = 0.32929687; TotalTime = 0.18024s; TotalTimePerSample = 0.03520ms; SamplesPerSecond = 28405
 Epoch[ 3 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02751465; EvalErr[0]PerSample = 0.32265625; TotalTime = 0.16988s; TotalTimePerSample = 0.03318ms; SamplesPerSecond = 30138
Finished Epoch[ 3 of 4]: [Training Set] TrainLossPerSample = 1.0759742; EvalErrPerSample = 0.33315429; AvgLearningRatePerSample = 0.003125000047; EpochTime=2.893692
Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.03003817; EvalErr[0]PerSample = 0.31289062; TotalTime = 0.18114s; TotalTimePerSample = 0.03538ms; SamplesPerSecond = 28265
 Epoch[ 4 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 4926; TrainLossPerSample =  1.04547925; EvalErr[0]PerSample = 0.32947625; TotalTime = 0.39363s; TotalTimePerSample = 0.07991ms; SamplesPerSecond = 12514
 Epoch[ 4 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.01249599; EvalErr[0]PerSample = 0.32246094; TotalTime = 0.18057s; TotalTimePerSample = 0.03527ms; SamplesPerSecond = 28354
 Epoch[ 4 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99796467; EvalErr[0]PerSample = 0.31425781; TotalTime = 0.17997s; TotalTimePerSample = 0.03515ms; SamplesPerSecond = 28449
 Epoch[ 4 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99781761; EvalErr[0]PerSample = 0.31464844; TotalTime = 0.17977s; TotalTimePerSample = 0.03511ms; SamplesPerSecond = 28481
 Epoch[ 4 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00107079; EvalErr[0]PerSample = 0.31855469; TotalTime = 0.18009s; TotalTimePerSample = 0.03517ms; SamplesPerSecond = 28429
 Epoch[ 4 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02518806; EvalErr[0]PerSample = 0.31972656; TotalTime = 0.17991s; TotalTimePerSample = 0.03514ms; SamplesPerSecond = 28458
 Epoch[ 4 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00891876; EvalErr[0]PerSample = 0.31660156; TotalTime = 0.17972s; TotalTimePerSample = 0.03510ms; SamplesPerSecond = 28488
 Epoch[ 4 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99774780; EvalErr[0]PerSample = 0.30585937; TotalTime = 0.18051s; TotalTimePerSample = 0.03526ms; SamplesPerSecond = 28364
 Epoch[ 4 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00037842; EvalErr[0]PerSample = 0.30722656; TotalTime = 0.17930s; TotalTimePerSample = 0.03502ms; SamplesPerSecond = 28555
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 4 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02586746; EvalErr[0]PerSample = 0.31816406; TotalTime = 0.17988s; TotalTimePerSample = 0.03513ms; SamplesPerSecond = 28463
 Epoch[ 4 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06024628; EvalErr[0]PerSample = 0.33574219; TotalTime = 0.17898s; TotalTimePerSample = 0.03496ms; SamplesPerSecond = 28607
 Epoch[ 4 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.98301010; EvalErr[0]PerSample = 0.30214844; TotalTime = 0.17934s; TotalTimePerSample = 0.03503ms; SamplesPerSecond = 28548
 Epoch[ 4 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.96488800; EvalErr[0]PerSample = 0.30156250; TotalTime = 0.18003s; TotalTimePerSample = 0.03516ms; SamplesPerSecond = 28439
 Epoch[ 4 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99069977; EvalErr[0]PerSample = 0.31640625; TotalTime = 0.17865s; TotalTimePerSample = 0.03489ms; SamplesPerSecond = 28659
 Epoch[ 4 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.97961731; EvalErr[0]PerSample = 0.29921875; TotalTime = 0.17196s; TotalTimePerSample = 0.03359ms; SamplesPerSecond = 29775
Finished Epoch[ 4 of 4]: [Training Set] TrainLossPerSample = 1.0073979; EvalErrPerSample = 0.31477052; AvgLearningRatePerSample = 0.003125000047; EpochTime=3.10907
CNTKCommandTrainEnd: speechTrain


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 13 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[0, 0] = SequenceWithSoftmax(labels[132, 0], OL.z[132, 0], ScaledLogLikelihood[132, 0])
ScaledLogLikelihood[132, 0] = Minus(OL.z[132, 0], logPrior[132, 1])
logPrior[132, 1] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[512, 0])
HL3.y[512, 0] = Sigmoid(HL3.z[512, 0])
HL3.z[512, 0] = Plus(HL3.t[512, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[512, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[1, 1] = SequenceWithSoftmax(labels[132, 0], OL.z[132, 0], ScaledLogLikelihood[132, 0])
ScaledLogLikelihood[132, 0] = Minus(OL.z[132, 0], logPrior[132, 1])
logPrior[132, 1] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[512, 0])
HL3.y[512, 0] = Sigmoid(HL3.z[512, 0])
HL3.z[512, 0] = Plus(HL3.t[512, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[512, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[1, 1] = SequenceWithSoftmax(labels[132, 0], OL.z[132, 0], ScaledLogLikelihood[132, 0])
ScaledLogLikelihood[132, 0] = Minus(OL.z[132, 0], logPrior[132, 1])
logPrior[132, 1] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[512, 0])
HL3.y[512, 0] = Sigmoid(HL3.z[512, 0])
HL3.z[512, 0] = Plus(HL3.t[512, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[512, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading '/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.overalltying', '/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list', '/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('/tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/TestData/CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151119172541.45857/Speech/DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0.


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[0, 0] = SequenceWithSoftmax(labels[132, 0], OL.z[0, 0], ScaledLogLikelihood[0, 0])
ScaledLogLikelihood[0, 0] = Minus(OL.z[0, 0], logPrior[0, 0])
logPrior[0, 0] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 15 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-10
Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms
getcachedidmap: reading 'CY2SCH070040324_1092265200.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080041801_1024690842.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050121418_1697698502.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070061608_1068884372.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090052313_1116838701.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH100092119_1769162281.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110070803_1061569312.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040030304_1783125393.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050090541_1705321237.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040121540_2063741403.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH100092303_1769985203.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050121330_1705081299.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050070531_1705536174.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060042707_1939966343.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040120414_2063653575.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050111321_1705093424.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040031340_1350166299.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070080320_1067791763.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060060407_1233548343.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090090208_1064021467.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110070817_767763687.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040070546_2063363669.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040080519_2063294856.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070060121_1068947138.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080071106_1143001388.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH010061231_1369712653.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080061318_1080287029.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060050603_416427387.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040080404_2063355153.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070081708_1067766888.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070101111_1071091997.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070061523_1068809263.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060050506_415705324.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070081604_866394296.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH100112501_1779035656.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050111011_1704954393.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040121314_2063679450.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040081222_2063295388.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050091619_1705363705.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060032122_1764296812.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050051513_1110722876.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050080342_1705448690.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH010061544_691164888.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110071501_1059265218.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060091323_1311139777.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080110609_1362153092.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050120821_1705096268.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060062620_1679599206.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH010080244_1211145048.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060082605_403727641.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070032203_1093924684.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110121120_1050992515.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH030121606_1629053528.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040121221_2063738606.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070062507_2087712345.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH020071229_644244140.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080040406_982069998.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050110617_547221844.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040011112_1027256127.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080062603_966497326.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060062605_1536194576.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070111908_1069919341.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090090404_1123576357.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040081012_2063351544.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050090827_1393311596.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050120722_1705080393.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090100302_1080239467.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090121023_160495828.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050051308_1443335642.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090081411_995585687.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040090837_1205589951.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080071106_1143018888.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070110213_1069879356.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040120646_805508469.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH030061032_1628532606.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050081506_1705673846.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040051114_2063220700.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060091920_1713759019.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH100090709_1765693687.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060121820_1476601534.numden.lats.symlist'

Starting minibatch loop.
copyhmms: 0.000000 ms
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.016951
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.090763
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.030578
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.078396
backpointers: 27.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.051528
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.075720
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.028965
backpointers: 39.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.025573
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.157795
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.955094
backpointers: 26.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.013404
backpointers: 27.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.062532
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.050060
backpointers: 24.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.085075
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.035518
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.088676
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.068275
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.081556
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.011559
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.045462
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.058040
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.078263
 Epoch[ 1 of 3]-Minibatch[   1-  10 of 8192]: SamplesSeen = 5566; TrainLossPerSample =  0.08468075; EvalErr[0]PerSample = 0.33093784; TotalTime = 9.03499s; TotalTimePerSample = 1.62325ms; SamplesPerSecond = 616
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.000341
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 110 launches for forward, 110 launches for backward
dengamma value 1.049229
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.033738
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.017619
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.036176
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.063524
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.122385
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.051659
backpointers: 44.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.040061
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.007486
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.001422
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.079357
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.034550
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.006944
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.054820
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.000686
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.009433
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.057346
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 161 launches for forward, 161 launches for backward
dengamma value 1.029017
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.100386
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.041619
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 106 launches for forward, 106 launches for backward
dengamma value 1.085854
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.049163
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.221116
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.062740
backpointers: 28.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.062362
 Epoch[ 1 of 3]-Minibatch[  11-  20 of 8192]: SamplesSeen = 7398; TrainLossPerSample =  0.07930380; EvalErr[0]PerSample = 0.31711273; TotalTime = 1.43005s; TotalTimePerSample = 0.19330ms; SamplesPerSecond = 5173
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.121956
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.003287
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.041682
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 0.988532
backpointers: 25.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.057405
backpointers: 44.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.078456
backpointers: 23.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.078765
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.073846
backpointers: 20.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.100777
backpointers: 44.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.102242
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 103 launches for forward, 103 launches for backward
dengamma value 1.028463
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.079809
backpointers: 44.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.055759
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.942416
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.103102
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.038075
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.120044
backpointers: 32.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.016724
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.028796
backpointers: 25.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.056334
backpointers: 22.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.055807
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.053802
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.057694
backpointers: 26.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.108763
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.101425
 Epoch[ 1 of 3]-Minibatch[  21-  30 of 8192]: SamplesSeen = 6300; TrainLossPerSample =  0.07753848; EvalErr[0]PerSample = 0.34031746; TotalTime = 1.20351s; TotalTimePerSample = 0.19103ms; SamplesPerSecond = 5234
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.107473
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.205362
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.070284
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.063702
backpointers: 38.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.046532
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 0.992090
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.003756
backpointers: 25.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.055655
backpointers: 24.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.113884
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.053838
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.044706
backpointers: 33.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.074876
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.086499
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.045390
backpointers: 38.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.115207
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.057181
backpointers: 38.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.008082
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.056726
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.019678
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 0.988532
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 0.931805
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.055384
 Epoch[ 1 of 3]-Minibatch[  31-  40 of 8192]: SamplesSeen = 5636; TrainLossPerSample =  0.07894523; EvalErr[0]PerSample = 0.32665011; TotalTime = 1.06644s; TotalTimePerSample = 0.18922ms; SamplesPerSecond = 5284
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 113 launches for forward, 113 launches for backward
dengamma value 1.046053
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.032466
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 88 launches for forward, 88 launches for backward
dengamma value 1.046131
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.071244
backpointers: 26.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.064012
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.082939
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.074270
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.037943
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 0.953982
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.039899
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.022986
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.064703
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.056453
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 0.966001
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.056884
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 0.983751
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.096840
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.020773
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.057210
backpointers: 43.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 0.977982
 Epoch[ 1 of 3]-Minibatch[  41-  50 of 8192]: SamplesSeen = 6970; TrainLossPerSample =  0.08023927; EvalErr[0]PerSample = 0.34921090; TotalTime = 1.30876s; TotalTimePerSample = 0.18777ms; SamplesPerSecond = 5325
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.064818
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.043452
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.077275
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.042999
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.068105
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.072082
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.053097
backpointers: 23.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 102 launches for forward, 102 launches for backward
dengamma value 1.076511
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.057454
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.057286
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.017794
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.064837
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.034165
backpointers: 28.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.067063
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.014472
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.092647
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.054529
backpointers: 40.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.002981
backpointers: 23.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 175 launches for forward, 175 launches for backward
dengamma value 1.126155
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.009531
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.010978
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.068980
 Epoch[ 1 of 3]-Minibatch[  51-  60 of 8192]: SamplesSeen = 6996; TrainLossPerSample =  0.07982592; EvalErr[0]PerSample = 0.32004002; TotalTime = 1.38432s; TotalTimePerSample = 0.19787ms; SamplesPerSecond = 5053
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.080229
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.027715
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.056990
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.050864
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.004432
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.014527
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.045878
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 0.981322
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.136079
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.001208
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.051288
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.014801
backpointers: 45.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.096011
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.092254
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.093714
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 0.976186
backpointers: 39.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.000405
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.044027
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 138 launches for forward, 138 launches for backward
dengamma value 1.035734
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.129123
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.039698
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.065553
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.095157
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.029285
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.020676
 Epoch[ 1 of 3]-Minibatch[  61-  70 of 8192]: SamplesSeen = 6180; TrainLossPerSample =  0.08487786; EvalErr[0]PerSample = 0.33414239; TotalTime = 1.19674s; TotalTimePerSample = 0.19365ms; SamplesPerSecond = 5164
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.045464
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.067789
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.142396
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.009637
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.075449
backpointers: 39.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.039409
backpointers: 40.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 0.995406
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.065594
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.034966
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.049933
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.035808
backpointers: 22.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.039026
backpointers: 36.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.100130
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 0.975159
backpointers: 25.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.109057
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.051146
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.044200
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.108704
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.062041
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.126588
 Epoch[ 1 of 3]-Minibatch[  71-  80 of 8192]: SamplesSeen = 4860; TrainLossPerSample =  0.08340803; EvalErr[0]PerSample = 0.33600823; TotalTime = 0.94276s; TotalTimePerSample = 0.19398ms; SamplesPerSecond = 5155
backpointers: 41.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.040750
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 0.972598
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 126 launches for forward, 126 launches for backward
dengamma value 1.026896
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.065175
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.034522
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.091507
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.012595
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.077661
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.020183
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.998050
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.048212
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.955666
backpointers: 27.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.027402
backpointers: 27.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.077979
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.052710
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.102155
backpointers: 38.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.069645
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.048981
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.042925
backpointers: 39.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.015464
backpointers: 25.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 124 launches for forward, 124 launches for backward
dengamma value 1.111772
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.068376
 Epoch[ 1 of 3]-Minibatch[  81-  90 of 8192]: SamplesSeen = 6046; TrainLossPerSample =  0.07605808; EvalErr[0]PerSample = 0.32699305; TotalTime = 1.18071s; TotalTimePerSample = 0.19529ms; SamplesPerSecond = 5120
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.040386
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.007380
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.017758
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 109 launches for forward, 109 launches for backward
dengamma value 0.964054
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.029415
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 109 launches for forward, 109 launches for backward
dengamma value 1.102677
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.043826
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 0.998165
backpointers: 25.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.036032
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.067882
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 0.996039
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.030009
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.057537
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.086534
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.004075
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.061849
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.015197
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.008634
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.002380
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.010348
backpointers: 42.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.999857
backpointers: 40.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.069942
backpointers: 39.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.028589
backpointers: 41.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.041727
 Epoch[ 1 of 3]-Minibatch[  91- 100 of 8192]: SamplesSeen = 6942; TrainLossPerSample =  0.08570117; EvalErr[0]PerSample = 0.34644195; TotalTime = 1.27712s; TotalTimePerSample = 0.18397ms; SamplesPerSecond = 5435
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.018619
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 130 launches for forward, 130 launches for backward
dengamma value 1.046682
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.003926
backpointers: 40.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 0.938060
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.073180
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.092333
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.136608
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.023432
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.033466
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.108754
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.063739
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.007792
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.075606
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.032966
backpointers: 23.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.107917
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.016783
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.044823
backpointers: 46.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.067759
backpointers: 42.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.024057
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.004260
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 0.982422
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.064614
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.070432
 Epoch[ 1 of 3]-Minibatch[ 101- 110 of 8192]: SamplesSeen = 5784; TrainLossPerSample =  0.08741906; EvalErr[0]PerSample = 0.33022130; TotalTime = 1.06170s; TotalTimePerSample = 0.18356ms; SamplesPerSecond = 5447
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.080328
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 109 launches for forward, 109 launches for backward
dengamma value 1.086655
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.013888
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.012087
backpointers: 22.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.125584
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.005360
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 0.969809
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.963508
backpointers: 43.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 0.944890
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.039242
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.039696
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 0.993007
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.068278
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 1.038109
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.073677
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 97 launches for forward, 97 launches for backward
dengamma value 0.994489
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.024373
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.046639
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.066258
backpointers: 32.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 0.992722
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.099014
 Epoch[ 1 of 3]-Minibatch[ 111- 120 of 8192]: SamplesSeen = 6258; TrainLossPerSample =  0.08745384; EvalErr[0]PerSample = 0.34324065; TotalTime = 1.15263s; TotalTimePerSample = 0.18419ms; SamplesPerSecond = 5429
backpointers: 22.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.101366
backpointers: 42.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 10 launches for forward, 10 launches for backward
dengamma value 1.094228
backpointers: 41.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.080228
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 110 launches for forward, 110 launches for backward
dengamma value 1.082982
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.024713
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.118944
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.075450
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.018848
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.044026
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 9 launches for forward, 9 launches for backward
dengamma value 1.024038
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 141 launches for forward, 141 launches for backward
dengamma value 1.027898
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.065873
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.048961
backpointers: 39.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.082500
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 0.969462
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.006500
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.077539
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.016960
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.104984
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.005075
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.036383
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 0.972230
 Epoch[ 1 of 3]-Minibatch[ 121- 130 of 8192]: SamplesSeen = 6116; TrainLossPerSample =  0.08172140; EvalErr[0]PerSample = 0.32946370; TotalTime = 1.17418s; TotalTimePerSample = 0.19199ms; SamplesPerSecond = 5208
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.044913
backpointers: 24.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 121 launches for forward, 121 launches for backward
dengamma value 1.055020
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.002380
backpointers: 24.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 121 launches for forward, 121 launches for backward
dengamma value 1.055052
Finished Epoch[ 1 of 3]: [Training Set] TrainLossPerSample = 0.082210436; EvalErrPerSample = 0.33285296; AvgLearningRatePerSample = 1.999999995e-06; EpochTime=47.99884
Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 82146), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.038088
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.002548
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 0.987010
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.071360
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.070828
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.009418
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 98 launches for forward, 98 launches for backward
dengamma value 1.064635
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.013010
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.030043
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.030622
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.044549
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.010495
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.051460
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.090772
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 0.973568
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.998958
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.023571
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.025588
backpointers: 26.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.069013
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.050955
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.065861
backpointers: 39.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.098596
 Epoch[ 2 of 3]-Minibatch[   1-  10 of 8192]: SamplesSeen = 5826; TrainLossPerSample =  0.08266546; EvalErr[0]PerSample = 0.34637830; TotalTime = 1.13975s; TotalTimePerSample = 0.19563ms; SamplesPerSecond = 5111
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.064950
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.105561
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.088102
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.057782
backpointers: 24.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.052364
backpointers: 44.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.019249
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.086992
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.084409
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.099333
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.052746
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.043194
backpointers: 44.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.962219
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.080682
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.084176
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.038184
backpointers: 27.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 103 launches for forward, 103 launches for backward
dengamma value 1.044506
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 132 launches for forward, 132 launches for backward
dengamma value 1.042780
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 101 launches for forward, 101 launches for backward
dengamma value 1.028939
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.045591
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.019333
 Epoch[ 2 of 3]-Minibatch[  11-  20 of 8192]: SamplesSeen = 6380; TrainLossPerSample =  0.07862261; EvalErr[0]PerSample = 0.31771160; TotalTime = 1.28482s; TotalTimePerSample = 0.20138ms; SamplesPerSecond = 4965
backpointers: 20.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 1.097878
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 97 launches for forward, 97 launches for backward
dengamma value 1.030249
backpointers: 22.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 130 launches for forward, 130 launches for backward
dengamma value 1.117329
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 0.947982
backpointers: 45.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.039146
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.078896
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 124 launches for forward, 124 launches for backward
dengamma value 1.112872
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.039877
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.075539
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 12 launches for forward, 12 launches for backward
dengamma value 1.035017
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 0.996563
backpointers: 32.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 98 launches for forward, 98 launches for backward
dengamma value 1.089636
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.032593
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.060184
backpointers: 37.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 89 launches for forward, 89 launches for backward
dengamma value 1.022234
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.025989
backpointers: 27.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.060254
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.087375
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.042716
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.045934
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.090363
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.062508
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 0.998471
 Epoch[ 2 of 3]-Minibatch[  21-  30 of 8192]: SamplesSeen = 6574; TrainLossPerSample =  0.07571978; EvalErr[0]PerSample = 0.32278674; TotalTime = 1.34909s; TotalTimePerSample = 0.20522ms; SamplesPerSecond = 4872
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 99 launches for forward, 99 launches for backward
dengamma value 1.086696
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.048196
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.054776
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.056882
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 0.933055
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 0.965800
backpointers: 57.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.138787
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.083072
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.030130
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.030360
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.056761
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 0.972244
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.055198
backpointers: 41.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.018788
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 0.972026
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 128 launches for forward, 128 launches for backward
dengamma value 1.083465
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.074164
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.103243
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 0.994083
backpointers: 37.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 0.986000
backpointers: 24.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.081591
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.127209
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.010590
 Epoch[ 2 of 3]-Minibatch[  31-  40 of 8192]: SamplesSeen = 6324; TrainLossPerSample =  0.08651957; EvalErr[0]PerSample = 0.34203036; TotalTime = 1.17431s; TotalTimePerSample = 0.18569ms; SamplesPerSecond = 5385
backpointers: 40.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 0.961097
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 0.942765
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.942351
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.054443
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.085019
backpointers: 26.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.104924
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.047166
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.155987
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.035481
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 0.987519
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.017748
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.022225
backpointers: 41.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.962975
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.105984
backpointers: 33.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.043334
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.000560
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.944519
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 0.998488
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.026647
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.055243
 Epoch[ 2 of 3]-Minibatch[  41-  50 of 8192]: SamplesSeen = 4800; TrainLossPerSample =  0.08918203; EvalErr[0]PerSample = 0.36812500; TotalTime = 0.89378s; TotalTimePerSample = 0.18620ms; SamplesPerSecond = 5370
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.001890
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.050395
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.050062
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.061193
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.048663
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.126828
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.100739
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.027145
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.072075
backpointers: 22.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.017510
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.076090
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.017353
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 0.993484
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.005100
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.069697
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.028585
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.027644
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 120 launches for forward, 120 launches for backward
dengamma value 1.021442
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.020226
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.042390
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.027079
backpointers: 22.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.061462
 Epoch[ 2 of 3]-Minibatch[  51-  60 of 8192]: SamplesSeen = 6176; TrainLossPerSample =  0.08016600; EvalErr[0]PerSample = 0.35443653; TotalTime = 1.12631s; TotalTimePerSample = 0.18237ms; SamplesPerSecond = 5483
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.096101
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.062995
backpointers: 44.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.078059
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 0.989344
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.080757
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.055379
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.096505
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 0.957997
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.066838
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 142 launches for forward, 142 launches for backward
dengamma value 1.035295
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.065538
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.023607
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.060317
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.002619
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.075739
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.046245
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.101097
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.151892
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.041482
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 0.967724
backpointers: 38.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.085342
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.054153
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.041504
 Epoch[ 2 of 3]-Minibatch[  61-  70 of 8192]: SamplesSeen = 5534; TrainLossPerSample =  0.08858833; EvalErr[0]PerSample = 0.31387785; TotalTime = 1.10455s; TotalTimePerSample = 0.19959ms; SamplesPerSecond = 5010
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.067130
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.007158
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 0.986670
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.029442
backpointers: 27.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.114901
backpointers: 36.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 0.969158
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.074201
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.066679
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 134 launches for forward, 134 launches for backward
dengamma value 1.056504
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.052486
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.104449
backpointers: 22.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 168 launches for forward, 168 launches for backward
dengamma value 1.115204
backpointers: 28.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.058134
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.084407
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.046963
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.046734
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.054714
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.997923
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.068547
backpointers: 28.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.044441
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.006314
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.075537
 Epoch[ 2 of 3]-Minibatch[  71-  80 of 8192]: SamplesSeen = 5936; TrainLossPerSample =  0.08162021; EvalErr[0]PerSample = 0.31822776; TotalTime = 1.27134s; TotalTimePerSample = 0.21417ms; SamplesPerSecond = 4669
backpointers: 36.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 0.916216
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.074995
backpointers: 29.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.078435
backpointers: 25.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.057328
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.078485
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.014861
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.078280
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.042853
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.037249
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.071795
backpointers: 38.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 0.911832
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.077340
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 12 launches for forward, 12 launches for backward
dengamma value 1.124653
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.091120
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.008635
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 102 launches for forward, 102 launches for backward
dengamma value 1.089288
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.092013
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.083488
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.100742
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.108284
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.063566
 Epoch[ 2 of 3]-Minibatch[  81-  90 of 8192]: SamplesSeen = 5248; TrainLossPerSample =  0.08116978; EvalErr[0]PerSample = 0.33250762; TotalTime = 1.00827s; TotalTimePerSample = 0.19212ms; SamplesPerSecond = 5204
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.047409
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.037768
backpointers: 41.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.056159
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.024341
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.056013
backpointers: 25.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 119 launches for forward, 119 launches for backward
dengamma value 1.070255
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.048169
backpointers: 39.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 0.997240
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 105 launches for forward, 105 launches for backward
dengamma value 1.063288
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.048935
backpointers: 24.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 158 launches for forward, 158 launches for backward
dengamma value 1.120314
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.079426
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 0.977880
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.088436
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.104661
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.038362
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.099799
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.101263
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.057282
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.034506
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.008389
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.062621
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 118 launches for forward, 118 launches for backward
dengamma value 1.054342
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.062158
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.028869
backpointers: 21.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.072664
 Epoch[ 2 of 3]-Minibatch[  91- 100 of 8192]: SamplesSeen = 6888; TrainLossPerSample =  0.08035083; EvalErr[0]PerSample = 0.31939605; TotalTime = 1.36037s; TotalTimePerSample = 0.19750ms; SamplesPerSecond = 5063
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 129 launches for forward, 129 launches for backward
dengamma value 1.051605
backpointers: 38.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.077176
backpointers: 37.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.040242
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 111 launches for forward, 111 launches for backward
dengamma value 1.052068
backpointers: 39.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.030234
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.035224
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.019526
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.093799
backpointers: 24.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 119 launches for forward, 119 launches for backward
dengamma value 1.070660
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.031425
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.032620
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.101631
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.078673
backpointers: 41.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.010443
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 108 launches for forward, 108 launches for backward
dengamma value 1.098603
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.014724
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.011566
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.126488
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.057342
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.039591
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.021315
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.023771
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 0.984565
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.040249
 Epoch[ 2 of 3]-Minibatch[ 101- 110 of 8192]: SamplesSeen = 6572; TrainLossPerSample =  0.08466605; EvalErr[0]PerSample = 0.34342666; TotalTime = 1.23591s; TotalTimePerSample = 0.18806ms; SamplesPerSecond = 5317
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 0.998905
backpointers: 26.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.085306
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.021620
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 170 launches for forward, 170 launches for backward
dengamma value 1.092687
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.054521
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.014457
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.014129
backpointers: 26.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.077565
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.044229
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.004763
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.057981
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.067377
backpointers: 22.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.072525
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.052167
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.031651
backpointers: 38.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.044801
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 108 launches for forward, 108 launches for backward
dengamma value 1.029338
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.108843
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.001231
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.062691
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.052591
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.131819
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.072210
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.030625
 Epoch[ 2 of 3]-Minibatch[ 111- 120 of 8192]: SamplesSeen = 6622; TrainLossPerSample =  0.08517949; EvalErr[0]PerSample = 0.31289641; TotalTime = 1.35009s; TotalTimePerSample = 0.20388ms; SamplesPerSecond = 4904
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.032055
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 161 launches for forward, 161 launches for backward
dengamma value 1.096506
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.021111
backpointers: 45.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 0.987919
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.039444
backpointers: 25.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.029018
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 0.952959
backpointers: 21.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 0.995627
backpointers: 36.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 0.949031
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.053269
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.073244
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 106 launches for forward, 106 launches for backward
dengamma value 1.135889
backpointers: 29.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.109631
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.096984
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.029104
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.048600
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 1.014163
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.039531
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.034862
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 0.976176
backpointers: 41.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.050905
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.092453
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.038136
 Epoch[ 2 of 3]-Minibatch[ 121- 130 of 8192]: SamplesSeen = 5824; TrainLossPerSample =  0.08575490; EvalErr[0]PerSample = 0.32417582; TotalTime = 1.06628s; TotalTimePerSample = 0.18308ms; SamplesPerSecond = 5461
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 0.981325
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.041492
backpointers: 21.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 150 launches for forward, 150 launches for backward
dengamma value 1.111667
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 0.966546
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.016518
backpointers: 21.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 126 launches for forward, 126 launches for backward
dengamma value 1.091933
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.068772
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.161398
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.039845
Finished Epoch[ 2 of 3]: [Training Set] TrainLossPerSample = 0.082834214; EvalErrPerSample = 0.33025581; AvgLearningRatePerSample = 1.999999995e-06; EpochTime=15.979751
Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898 
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163922), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.085630
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.032653
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.072816
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.078261
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.092765
backpointers: 38.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.007782
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.032480
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.003907
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.020698
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.011118
backpointers: 26.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 98 launches for forward, 98 launches for backward
dengamma value 1.048709
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.039067
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.028426
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.057734
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.057475
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.128095
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.013283
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.114727
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.028415
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.071979
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.005469
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.054217
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.103122
 Epoch[ 3 of 3]-Minibatch[   1-  10 of 8192]: SamplesSeen = 5074; TrainLossPerSample =  0.08242372; EvalErr[0]PerSample = 0.33011431; TotalTime = 1.05326s; TotalTimePerSample = 0.20758ms; SamplesPerSecond = 4817
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.012585
backpointers: 24.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 114 launches for forward, 114 launches for backward
dengamma value 1.099718
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.057921
backpointers: 26.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.070138
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 0.994067
backpointers: 40.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 0.995537
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 0.996810
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.999127
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.112330
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.067682
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.038603
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.025632
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.044260
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.046131
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.090714
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.093416
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 111 launches for forward, 111 launches for backward
dengamma value 1.034906
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.044763
backpointers: 24.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 169 launches for forward, 169 launches for backward
dengamma value 1.087702
backpointers: 41.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 0.970572
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.046708
backpointers: 38.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.023588
 Epoch[ 3 of 3]-Minibatch[  11-  20 of 8192]: SamplesSeen = 7136; TrainLossPerSample =  0.07981886; EvalErr[0]PerSample = 0.32315022; TotalTime = 1.33737s; TotalTimePerSample = 0.18741ms; SamplesPerSecond = 5335
backpointers: 23.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.093834
backpointers: 33.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.015773
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 0.996985
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.012523
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.077096
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.027190
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.119522
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 0.996340
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.021191
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.065129
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 1.088257
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.057078
backpointers: 23.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.100466
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.021776
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.070165
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.022536
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.051410
backpointers: 32.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 88 launches for forward, 88 launches for backward
dengamma value 1.018535
backpointers: 41.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 1.111403
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.114549
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.034455
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.007732
backpointers: 42.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.024432
 Epoch[ 3 of 3]-Minibatch[  21-  30 of 8192]: SamplesSeen = 5504; TrainLossPerSample =  0.09111519; EvalErr[0]PerSample = 0.34520349; TotalTime = 1.11367s; TotalTimePerSample = 0.20234ms; SamplesPerSecond = 4942
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.050855
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.066213
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.055944
backpointers: 22.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 128 launches for forward, 128 launches for backward
dengamma value 1.078349
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.063846
backpointers: 39.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.001603
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.064123
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.076174
backpointers: 32.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.023789
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.123867
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 99 launches for forward, 99 launches for backward
dengamma value 1.043842
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 137 launches for forward, 137 launches for backward
dengamma value 1.062468
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.052183
backpointers: 22.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.097915
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 0.979307
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.049687
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.006878
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.024591
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.941949
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.133572
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.066064
 Epoch[ 3 of 3]-Minibatch[  31-  40 of 8192]: SamplesSeen = 6028; TrainLossPerSample =  0.08181858; EvalErr[0]PerSample = 0.33560053; TotalTime = 1.20587s; TotalTimePerSample = 0.20004ms; SamplesPerSecond = 4998
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.066584
backpointers: 20.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 125 launches for forward, 125 launches for backward
dengamma value 1.100234
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.069436
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.066658
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 0.988324
backpointers: 40.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 0.980409
backpointers: 43.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.052009
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 102 launches for forward, 102 launches for backward
dengamma value 1.002361
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.044768
backpointers: 28.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.112311
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.071653
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.030606
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.069438
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.058430
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.054340
backpointers: 38.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.002708
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.019374
backpointers: 38.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.149197
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.016561
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.046652
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.031632
 Epoch[ 3 of 3]-Minibatch[  41-  50 of 8192]: SamplesSeen = 6028; TrainLossPerSample =  0.08364010; EvalErr[0]PerSample = 0.33560053; TotalTime = 1.12364s; TotalTimePerSample = 0.18640ms; SamplesPerSecond = 5364
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.028070
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.045467
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.017207
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 0.993784
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.053291
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.102319
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.089345
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.061480
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 0.981645
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.175551
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 0.994262
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 0.991496
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.063669
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.044150
backpointers: 39.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.042424
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.087742
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 0.998621
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 0.983086
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.061653
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.021335
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.065320
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.047158
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.009269
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.023060
 Epoch[ 3 of 3]-Minibatch[  51-  60 of 8192]: SamplesSeen = 6782; TrainLossPerSample =  0.08396008; EvalErr[0]PerSample = 0.33190799; TotalTime = 1.26703s; TotalTimePerSample = 0.18682ms; SamplesPerSecond = 5352
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.063373
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 113 launches for forward, 113 launches for backward
dengamma value 1.057358
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.109836
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.019116
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.095504
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.046314
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.105244
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.103004
backpointers: 26.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 136 launches for forward, 136 launches for backward
dengamma value 1.081641
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.028557
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.078995
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 100 launches for forward, 100 launches for backward
dengamma value 1.099618
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.001807
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.129626
backpointers: 25.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 138 launches for forward, 138 launches for backward
dengamma value 1.129011
backpointers: 33.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.004430
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.055570
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.120236
backpointers: 29.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.067421
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 0.997786
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.023166
 Epoch[ 3 of 3]-Minibatch[  61-  70 of 8192]: SamplesSeen = 5458; TrainLossPerSample =  0.07731828; EvalErr[0]PerSample = 0.30230854; TotalTime = 1.14436s; TotalTimePerSample = 0.20967ms; SamplesPerSecond = 4769
backpointers: 26.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.066885
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.029330
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.084840
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.060537
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.228046
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.095044
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 0.978831
backpointers: 27.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.073193
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.084648
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.046759
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.047481
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.077822
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 88 launches for forward, 88 launches for backward
dengamma value 1.123007
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.162762
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.047473
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 126 launches for forward, 126 launches for backward
dengamma value 1.068705
backpointers: 32.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.023569
backpointers: 41.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 110 launches for forward, 110 launches for backward
dengamma value 1.023528
backpointers: 39.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.008829
backpointers: 24.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.108584
backpointers: 24.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 100 launches for forward, 100 launches for backward
dengamma value 1.081726
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.112577
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.090438
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.095271
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.017312
 Epoch[ 3 of 3]-Minibatch[  71-  80 of 8192]: SamplesSeen = 6610; TrainLossPerSample =  0.07864290; EvalErr[0]PerSample = 0.29924357; TotalTime = 1.28598s; TotalTimePerSample = 0.19455ms; SamplesPerSecond = 5140
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.075826
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.067948
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.110302
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 120 launches for forward, 120 launches for backward
dengamma value 1.081711
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.067880
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 0.978330
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.029284
backpointers: 25.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.073561
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.084059
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.066804
backpointers: 53.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.036851
backpointers: 43.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.953758
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.118915
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.105659
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.013800
backpointers: 26.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.045754
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 0.912958
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.062305
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.108428
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.055655
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.026025
backpointers: 29.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.084758
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.040212
 Epoch[ 3 of 3]-Minibatch[  81-  90 of 8192]: SamplesSeen = 5854; TrainLossPerSample =  0.08365361; EvalErr[0]PerSample = 0.32302699; TotalTime = 1.07815s; TotalTimePerSample = 0.18417ms; SamplesPerSecond = 5429
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.062826
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.129052
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.093953
backpointers: 46.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.883005
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.052997
backpointers: 27.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.060440
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 0.975450
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.086084
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.026298
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.029125
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.029762
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.016837
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.105651
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.054374
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.113862
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.079821
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.052164
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.105429
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.042695
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.973192
backpointers: 38.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.029195
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.063837
backpointers: 24.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.029671
 Epoch[ 3 of 3]-Minibatch[  91- 100 of 8192]: SamplesSeen = 4674; TrainLossPerSample =  0.08812171; EvalErr[0]PerSample = 0.33483098; TotalTime = 0.92455s; TotalTimePerSample = 0.19781ms; SamplesPerSecond = 5055
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 0.997764
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.035159
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.058940
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.075590
backpointers: 44.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.030439
backpointers: 26.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.078578
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.044168
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.009967
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.125321
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.044931
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.036468
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.062922
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 127 launches for forward, 127 launches for backward
dengamma value 1.070043
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.009147
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 0.922847
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 0.981278
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 0.998950
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.053180
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.037255
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 0.980541
backpointers: 26.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.083944
 Epoch[ 3 of 3]-Minibatch[ 101- 110 of 8192]: SamplesSeen = 6248; TrainLossPerSample =  0.08040510; EvalErr[0]PerSample = 0.33034571; TotalTime = 1.19270s; TotalTimePerSample = 0.19089ms; SamplesPerSecond = 5238
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.975813
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 129 launches for forward, 129 launches for backward
dengamma value 1.051268
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.058537
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.076220
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.143107
backpointers: 24.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.025791
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.054328
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.050157
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.095524
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.000419
backpointers: 40.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.005942
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.012887
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.061799
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.055634
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 0.992318
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.083854
backpointers: 19.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.117213
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.028256
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.040340
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 130 launches for forward, 130 launches for backward
dengamma value 1.154055
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 117 launches for forward, 117 launches for backward
dengamma value 1.104128
backpointers: 38.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.052442
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.075892
 Epoch[ 3 of 3]-Minibatch[ 111- 120 of 8192]: SamplesSeen = 7094; TrainLossPerSample =  0.07677641; EvalErr[0]PerSample = 0.32111644; TotalTime = 1.35950s; TotalTimePerSample = 0.19164ms; SamplesPerSecond = 5218
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 105 launches for forward, 105 launches for backward
dengamma value 1.051146
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.075785
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.054600
backpointers: 26.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.056107
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 108 launches for forward, 108 launches for backward
dengamma value 1.045453
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.047608
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.052784
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 0.977057
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 0.998293
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.053047
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.063802
backpointers: 38.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.039332
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.011984
backpointers: 28.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 121 launches for forward, 121 launches for backward
dengamma value 1.086482
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 133 launches for forward, 133 launches for backward
dengamma value 0.997638
backpointers: 26.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.094072
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.058531
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.054121
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.028720
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.040245
backpointers: 41.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.064497
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.051749
 Epoch[ 3 of 3]-Minibatch[ 121- 130 of 8192]: SamplesSeen = 6246; TrainLossPerSample =  0.08554733; EvalErr[0]PerSample = 0.32196606; TotalTime = 1.23266s; TotalTimePerSample = 0.19735ms; SamplesPerSecond = 5067
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.009673
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.054432
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 141 launches for forward, 141 launches for backward
dengamma value 1.046565
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.075888
backpointers: 27.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.040106
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 0.970556
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.008954
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.132209
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.071974
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.992598
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 0.947651
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.062592
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.062592
Finished Epoch[ 3 of 3]: [Training Set] TrainLossPerSample = 0.082648888; EvalErrPerSample = 0.32607052; AvgLearningRatePerSample = 1.999999995e-06; EpochTime=15.94079
CNTKCommandTrainEnd: sequenceTrain
__COMPLETED__

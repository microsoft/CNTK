CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/cntk_sequence.cntk currentDirectory=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu DataDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug 16 2016 09:41:56
		Last modified date: Fri Aug 12 07:32:43 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
		Built by philly on f67b30a647de
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData
08/16/2016 09:59:20: -------------------------------------------------------------------
08/16/2016 09:59:20: Build info: 

08/16/2016 09:59:20: 		Built time: Aug 16 2016 09:41:56
08/16/2016 09:59:20: 		Last modified date: Fri Aug 12 07:32:43 2016
08/16/2016 09:59:20: 		Build type: release
08/16/2016 09:59:20: 		Build target: GPU
08/16/2016 09:59:20: 		With 1bit-SGD: no
08/16/2016 09:59:20: 		Math lib: mkl
08/16/2016 09:59:20: 		CUDA_PATH: /usr/local/cuda-7.5
08/16/2016 09:59:20: 		CUB_PATH: /usr/local/cub-1.4.1
08/16/2016 09:59:20: 		CUDNN_PATH: /usr/local/cudnn-4.0
08/16/2016 09:59:20: 		Build Branch: HEAD
08/16/2016 09:59:20: 		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
08/16/2016 09:59:20: 		Built by philly on f67b30a647de
08/16/2016 09:59:20: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
08/16/2016 09:59:20: -------------------------------------------------------------------
08/16/2016 09:59:21: -------------------------------------------------------------------
08/16/2016 09:59:21: GPU info:

08/16/2016 09:59:21: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:59:21: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:59:21: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:59:21: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:59:21: -------------------------------------------------------------------

08/16/2016 09:59:21: Running on localhost at 2016/08/16 09:59:21
08/16/2016 09:59:21: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/cntk_sequence.cntk  currentDirectory=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData  RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu  DataDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining  OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu  DeviceId=0  timestamping=true



08/16/2016 09:59:21: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/16/2016 09:59:21: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "$RunDir$/models/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.sequence.0"
    editPath  = "$ConfigDir$/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "$DataDir$/model.overalltying"
            transpFile = "$DataDir$/model.transprob"
        ]
        lattices = [
            denlatTocFile = "$DataDir$/*.lats.toc"
        ]
    ]
]
currentDirectory=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData
RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu
DataDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 09:59:21: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/16/2016 09:59:21: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/16/2016 09:59:21: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
            labelMappingFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/model.overalltying"
            transpFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/*.lats.toc"
        ]
    ]
]
currentDirectory=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData
RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu
DataDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 09:59:21: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/16/2016 09:59:21: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData
configparameters: cntk_sequence.cntk:DataDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
            labelMappingFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/model.overalltying"
            transpFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
08/16/2016 09:59:21: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/16/2016 09:59:21: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
08/16/2016 09:59:21: Precision = "float"
08/16/2016 09:59:21: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech
08/16/2016 09:59:21: CNTKCommandTrainInfo: dptPre1 : 2
08/16/2016 09:59:21: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech
08/16/2016 09:59:21: CNTKCommandTrainInfo: dptPre2 : 2
08/16/2016 09:59:21: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech
08/16/2016 09:59:21: CNTKCommandTrainInfo: speechTrain : 4
08/16/2016 09:59:21: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence
08/16/2016 09:59:21: CNTKCommandTrainInfo: sequenceTrain : 3
08/16/2016 09:59:21: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11

08/16/2016 09:59:21: ##############################################################################
08/16/2016 09:59:21: #                                                                            #
08/16/2016 09:59:21: # Action "train"                                                             #
08/16/2016 09:59:21: #                                                                            #
08/16/2016 09:59:21: ##############################################################################

08/16/2016 09:59:21: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 09:59:22: Creating virgin network.
Node 'globalMean' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalInvStd' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalPrior' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- 0.000000.
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- 0.000000.
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- uniform(seed=1, range=0.050000*1.000000, onCPU=false).
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=2, range=0.050000*1.000000, onCPU=false).
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- uniform(seed=3, range=0.050000*1.000000, onCPU=false).
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- uniform(seed=4, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 09:59:22: Created model with 19 nodes on GPU 0.

08/16/2016 09:59:22: Training criterion node(s):
08/16/2016 09:59:22: 	ce = CrossEntropyWithSoftmax

08/16/2016 09:59:22: Evaluation criterion node(s):
08/16/2016 09:59:22: 	err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 29 matrices, 11 are shared as 5, and 18 are not shared.

	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *] }
	{ HL1.t : [512 x *] (gradient)
	  HL1.y : [512 x 1 x *] }
	{ HL1.z : [512 x 1 x *] (gradient)
	  OL.t : [132 x 1 x *] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] (gradient) }


08/16/2016 09:59:22: Training 254084 parameters in 4 out of 4 parameter tensors and 10 nodes with gradient:

08/16/2016 09:59:22: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 09:59:22: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:22: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 09:59:22: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 09:59:22: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 09:59:22: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 09:59:22: Starting minibatch loop.
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.74183846 * 2560; err = 0.80195313 * 2560; time = 0.1117s; samplesPerSecond = 22923.5
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.91124763 * 2560; err = 0.70898438 * 2560; time = 0.0081s; samplesPerSecond = 314535.0
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.58015976 * 2560; err = 0.66640625 * 2560; time = 0.0079s; samplesPerSecond = 324543.6
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.27427139 * 2560; err = 0.58750000 * 2560; time = 0.0079s; samplesPerSecond = 325327.2
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.05503616 * 2560; err = 0.56093750 * 2560; time = 0.0078s; samplesPerSecond = 326697.3
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.91055145 * 2560; err = 0.52812500 * 2560; time = 0.0079s; samplesPerSecond = 325575.5
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.81562653 * 2560; err = 0.51171875 * 2560; time = 0.0078s; samplesPerSecond = 327114.7
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.68803253 * 2560; err = 0.48476562 * 2560; time = 0.0078s; samplesPerSecond = 327701.0
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.57382050 * 2560; err = 0.45429687 * 2560; time = 0.0078s; samplesPerSecond = 327910.8
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62090149 * 2560; err = 0.47304687 * 2560; time = 0.0078s; samplesPerSecond = 326947.6
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.59272461 * 2560; err = 0.47500000 * 2560; time = 0.0078s; samplesPerSecond = 328036.9
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51520386 * 2560; err = 0.44531250 * 2560; time = 0.0078s; samplesPerSecond = 328247.2
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.49181976 * 2560; err = 0.45039062 * 2560; time = 0.0078s; samplesPerSecond = 328331.4
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.53703613 * 2560; err = 0.44804688 * 2560; time = 0.0079s; samplesPerSecond = 323273.1
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.43095398 * 2560; err = 0.41640625 * 2560; time = 0.0078s; samplesPerSecond = 327910.8
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.41503601 * 2560; err = 0.40078125 * 2560; time = 0.0078s; samplesPerSecond = 327952.9
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.38913574 * 2560; err = 0.41132812 * 2560; time = 0.0078s; samplesPerSecond = 327491.4
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.41207886 * 2560; err = 0.42226562 * 2560; time = 0.0078s; samplesPerSecond = 327826.9
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.39968262 * 2560; err = 0.40664062 * 2560; time = 0.0078s; samplesPerSecond = 327617.1
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42729187 * 2560; err = 0.42617187 * 2560; time = 0.0078s; samplesPerSecond = 327701.0
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.41336365 * 2560; err = 0.42343750 * 2560; time = 0.0078s; samplesPerSecond = 328289.3
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.33186951 * 2560; err = 0.39960937 * 2560; time = 0.0078s; samplesPerSecond = 327701.0
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.28581238 * 2560; err = 0.38710937 * 2560; time = 0.0078s; samplesPerSecond = 328964.3
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.34127502 * 2560; err = 0.40976563 * 2560; time = 0.0078s; samplesPerSecond = 328879.8
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.32666016 * 2560; err = 0.39726563 * 2560; time = 0.0079s; samplesPerSecond = 325079.4
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.21437378 * 2560; err = 0.37265625 * 2560; time = 0.0079s; samplesPerSecond = 325948.6
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23749695 * 2560; err = 0.37343750 * 2560; time = 0.0080s; samplesPerSecond = 321325.5
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29956665 * 2560; err = 0.39023438 * 2560; time = 0.0079s; samplesPerSecond = 325038.1
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.21198120 * 2560; err = 0.37382813 * 2560; time = 0.0078s; samplesPerSecond = 327114.7
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20528259 * 2560; err = 0.36718750 * 2560; time = 0.0079s; samplesPerSecond = 324667.1
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23613586 * 2560; err = 0.37343750 * 2560; time = 0.0078s; samplesPerSecond = 327240.2
08/16/2016 09:59:22:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.25615234 * 2560; err = 0.38164063 * 2560; time = 0.0079s; samplesPerSecond = 325575.5
08/16/2016 09:59:22: Finished Epoch[ 1 of 2]: [Training] ce = 1.62945061 * 81920; err = 0.46030273 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.452488s
08/16/2016 09:59:22: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/16/2016 09:59:22: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 09:59:22: Starting minibatch loop.
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.23230953 * 2560; err = 0.38320312 * 2560; time = 0.0110s; samplesPerSecond = 232685.0
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.20511341 * 2560; err = 0.37421875 * 2560; time = 0.0089s; samplesPerSecond = 288288.3
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.28783760 * 2560; err = 0.37421875 * 2560; time = 0.0087s; samplesPerSecond = 293881.3
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.22809334 * 2560; err = 0.37421875 * 2560; time = 0.0087s; samplesPerSecond = 293679.0
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.18090286 * 2560; err = 0.35468750 * 2560; time = 0.0087s; samplesPerSecond = 293712.7
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.28175354 * 2560; err = 0.37695312 * 2560; time = 0.0087s; samplesPerSecond = 293847.6
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.22251205 * 2560; err = 0.37382813 * 2560; time = 0.0088s; samplesPerSecond = 292104.1
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.17863007 * 2560; err = 0.36328125 * 2560; time = 0.0088s; samplesPerSecond = 292370.9
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.23061218 * 2560; err = 0.35742188 * 2560; time = 0.0089s; samplesPerSecond = 287124.3
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18048782 * 2560; err = 0.37578125 * 2560; time = 0.0089s; samplesPerSecond = 288190.9
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.19648056 * 2560; err = 0.35976562 * 2560; time = 0.0088s; samplesPerSecond = 290810.0
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18896942 * 2560; err = 0.35429688 * 2560; time = 0.0088s; samplesPerSecond = 291339.5
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.16628113 * 2560; err = 0.35937500 * 2560; time = 0.0088s; samplesPerSecond = 291538.5
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.12856445 * 2560; err = 0.35195312 * 2560; time = 0.0088s; samplesPerSecond = 291206.9
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10083466 * 2560; err = 0.32617188 * 2560; time = 0.0087s; samplesPerSecond = 292705.2
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09875183 * 2560; err = 0.33906250 * 2560; time = 0.0089s; samplesPerSecond = 287446.7
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.18634949 * 2560; err = 0.35820313 * 2560; time = 0.0087s; samplesPerSecond = 292973.2
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.15709991 * 2560; err = 0.35195312 * 2560; time = 0.0087s; samplesPerSecond = 293141.0
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10971069 * 2560; err = 0.34960938 * 2560; time = 0.0087s; samplesPerSecond = 294151.4
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11317139 * 2560; err = 0.35000000 * 2560; time = 0.0088s; samplesPerSecond = 290183.6
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.08727722 * 2560; err = 0.32578125 * 2560; time = 0.0087s; samplesPerSecond = 292638.3
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.12296143 * 2560; err = 0.34101562 * 2560; time = 0.0088s; samplesPerSecond = 291837.7
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12966003 * 2560; err = 0.35078125 * 2560; time = 0.0088s; samplesPerSecond = 291937.5
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27489319 * 2560; err = 0.39257812 * 2560; time = 0.0088s; samplesPerSecond = 291605.0
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.17423401 * 2560; err = 0.35156250 * 2560; time = 0.0089s; samplesPerSecond = 288873.8
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13240051 * 2560; err = 0.35625000 * 2560; time = 0.0088s; samplesPerSecond = 292538.0
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.13792114 * 2560; err = 0.34335938 * 2560; time = 0.0087s; samplesPerSecond = 292604.9
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.13433228 * 2560; err = 0.33710937 * 2560; time = 0.0088s; samplesPerSecond = 291107.6
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.05835876 * 2560; err = 0.33710937 * 2560; time = 0.0088s; samplesPerSecond = 292437.7
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.09596558 * 2560; err = 0.33476563 * 2560; time = 0.0088s; samplesPerSecond = 291937.5
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08180847 * 2560; err = 0.33242187 * 2560; time = 0.0088s; samplesPerSecond = 291206.9
08/16/2016 09:59:22:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06572876 * 2560; err = 0.33632812 * 2560; time = 0.0085s; samplesPerSecond = 299520.3
08/16/2016 09:59:22: Finished Epoch[ 2 of 2]: [Training] ce = 1.16156273 * 81920; err = 0.35460205 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.286218s
08/16/2016 09:59:22: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech'
08/16/2016 09:59:22: CNTKCommandTrainEnd: dptPre1

08/16/2016 09:59:22: Action "train" complete.


08/16/2016 09:59:22: ##############################################################################
08/16/2016 09:59:22: #                                                                            #
08/16/2016 09:59:22: # Action "edit"                                                              #
08/16/2016 09:59:22: #                                                                            #
08/16/2016 09:59:22: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=5, range=0.050000*1.000000, onCPU=false).
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=6, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 09:59:23: Action "edit" complete.


08/16/2016 09:59:23: ##############################################################################
08/16/2016 09:59:23: #                                                                            #
08/16/2016 09:59:23: # Action "train"                                                             #
08/16/2016 09:59:23: #                                                                            #
08/16/2016 09:59:23: ##############################################################################

08/16/2016 09:59:23: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 09:59:23: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 09:59:23: Loaded model with 24 nodes on GPU 0.

08/16/2016 09:59:23: Training criterion node(s):
08/16/2016 09:59:23: 	ce = CrossEntropyWithSoftmax

08/16/2016 09:59:23: Evaluation criterion node(s):
08/16/2016 09:59:23: 	err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 39 matrices, 19 are shared as 8, and 20 are not shared.

	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *3] (gradient)
	  OL.z : [132 x 1 x *3] (gradient) }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *3] }
	{ HL1.t : [512 x *3] (gradient)
	  HL1.y : [512 x 1 x *3] }
	{ HL1.z : [512 x 1 x *3] (gradient)
	  HL2.t : [512 x 1 x *3] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *3] }
	{ HL2.t : [512 x 1 x *3] (gradient)
	  HL2.y : [512 x 1 x *3] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *3] (gradient)
	  HL2.z : [512 x 1 x *3] (gradient)
	  OL.t : [132 x 1 x *3] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *3] }


08/16/2016 09:59:23: Training 516740 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

08/16/2016 09:59:23: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 09:59:23: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:23: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:59:23: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:23: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 09:59:23: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 09:59:23: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 09:59:23: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 09:59:23: Starting minibatch loop.
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 4.30124588 * 2560; err = 0.80703125 * 2560; time = 0.0153s; samplesPerSecond = 167396.8
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.75448074 * 2560; err = 0.69960937 * 2560; time = 0.0124s; samplesPerSecond = 206935.6
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.20926208 * 2560; err = 0.58515625 * 2560; time = 0.0123s; samplesPerSecond = 207556.3
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.88578110 * 2560; err = 0.50117188 * 2560; time = 0.0123s; samplesPerSecond = 207809.1
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.71906204 * 2560; err = 0.47773437 * 2560; time = 0.0123s; samplesPerSecond = 208011.7
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.60130463 * 2560; err = 0.44648437 * 2560; time = 0.0123s; samplesPerSecond = 207842.8
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.56077118 * 2560; err = 0.45000000 * 2560; time = 0.0123s; samplesPerSecond = 207388.2
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.47116547 * 2560; err = 0.42460938 * 2560; time = 0.0123s; samplesPerSecond = 208367.2
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.38874512 * 2560; err = 0.40781250 * 2560; time = 0.0124s; samplesPerSecond = 205804.3
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41911163 * 2560; err = 0.42539063 * 2560; time = 0.0124s; samplesPerSecond = 206086.0
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.38730774 * 2560; err = 0.42148438 * 2560; time = 0.0124s; samplesPerSecond = 206601.6
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.36617889 * 2560; err = 0.41015625 * 2560; time = 0.0124s; samplesPerSecond = 206584.9
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.33381653 * 2560; err = 0.40781250 * 2560; time = 0.0124s; samplesPerSecond = 206651.6
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.39802246 * 2560; err = 0.40546875 * 2560; time = 0.0124s; samplesPerSecond = 206169.0
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.33336182 * 2560; err = 0.40195313 * 2560; time = 0.0125s; samplesPerSecond = 205408.0
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.33834229 * 2560; err = 0.40195313 * 2560; time = 0.0125s; samplesPerSecond = 205556.4
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.26663208 * 2560; err = 0.37578125 * 2560; time = 0.0124s; samplesPerSecond = 206385.0
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28086243 * 2560; err = 0.39296875 * 2560; time = 0.0124s; samplesPerSecond = 206751.7
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.29481506 * 2560; err = 0.39531250 * 2560; time = 0.0124s; samplesPerSecond = 206768.4
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.27625122 * 2560; err = 0.39375000 * 2560; time = 0.0124s; samplesPerSecond = 206401.7
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.26905518 * 2560; err = 0.38984375 * 2560; time = 0.0124s; samplesPerSecond = 205721.6
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.21494751 * 2560; err = 0.36250000 * 2560; time = 0.0124s; samplesPerSecond = 205837.4
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.20699158 * 2560; err = 0.36914062 * 2560; time = 0.0124s; samplesPerSecond = 206701.7
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.25002136 * 2560; err = 0.37851563 * 2560; time = 0.0227s; samplesPerSecond = 112909.6
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.22617187 * 2560; err = 0.37656250 * 2560; time = 0.0153s; samplesPerSecond = 167276.5
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.14840393 * 2560; err = 0.35468750 * 2560; time = 0.0134s; samplesPerSecond = 191116.1
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.16649780 * 2560; err = 0.35468750 * 2560; time = 0.0127s; samplesPerSecond = 201527.2
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.22885742 * 2560; err = 0.36992188 * 2560; time = 0.0127s; samplesPerSecond = 201876.8
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.16533203 * 2560; err = 0.36484375 * 2560; time = 0.0124s; samplesPerSecond = 205771.2
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.17502136 * 2560; err = 0.35664062 * 2560; time = 0.0124s; samplesPerSecond = 205787.8
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.16159058 * 2560; err = 0.35195312 * 2560; time = 0.0124s; samplesPerSecond = 206301.9
08/16/2016 09:59:23:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.17113953 * 2560; err = 0.35429688 * 2560; time = 0.0124s; samplesPerSecond = 207103.0
08/16/2016 09:59:23: Finished Epoch[ 1 of 2]: [Training] ce = 1.49907970 * 81920; err = 0.42547607 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.598834s
08/16/2016 09:59:23: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/16/2016 09:59:23: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 09:59:23: Starting minibatch loop.
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.14215403 * 2560; err = 0.34882812 * 2560; time = 0.0136s; samplesPerSecond = 188193.8
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.17049246 * 2560; err = 0.36328125 * 2560; time = 0.0124s; samplesPerSecond = 206252.0
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.24373856 * 2560; err = 0.37460938 * 2560; time = 0.0123s; samplesPerSecond = 207606.8
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.18655586 * 2560; err = 0.36445312 * 2560; time = 0.0123s; samplesPerSecond = 207792.2
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.13848000 * 2560; err = 0.35039063 * 2560; time = 0.0124s; samplesPerSecond = 206885.4
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.21884155 * 2560; err = 0.36757812 * 2560; time = 0.0123s; samplesPerSecond = 207405.0
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14372940 * 2560; err = 0.35000000 * 2560; time = 0.0123s; samplesPerSecond = 207842.8
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.12769089 * 2560; err = 0.34960938 * 2560; time = 0.0123s; samplesPerSecond = 207438.6
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.14114227 * 2560; err = 0.33554688 * 2560; time = 0.0123s; samplesPerSecond = 207674.2
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12445145 * 2560; err = 0.34843750 * 2560; time = 0.0123s; samplesPerSecond = 207573.2
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.14137878 * 2560; err = 0.34101562 * 2560; time = 0.0123s; samplesPerSecond = 207287.4
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.12705154 * 2560; err = 0.33867188 * 2560; time = 0.0123s; samplesPerSecond = 207522.7
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10779419 * 2560; err = 0.34531250 * 2560; time = 0.0123s; samplesPerSecond = 207472.2
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.07003021 * 2560; err = 0.32500000 * 2560; time = 0.0124s; samplesPerSecond = 206918.8
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.05308990 * 2560; err = 0.31406250 * 2560; time = 0.0123s; samplesPerSecond = 207961.0
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.06392975 * 2560; err = 0.33085938 * 2560; time = 0.0123s; samplesPerSecond = 207371.4
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.14430847 * 2560; err = 0.35507813 * 2560; time = 0.0123s; samplesPerSecond = 207405.0
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.14809570 * 2560; err = 0.35859375 * 2560; time = 0.0123s; samplesPerSecond = 207842.8
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.08184509 * 2560; err = 0.33515625 * 2560; time = 0.0123s; samplesPerSecond = 207842.8
08/16/2016 09:59:23:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.07637024 * 2560; err = 0.33359375 * 2560; time = 0.0123s; samplesPerSecond = 207354.6
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.06249695 * 2560; err = 0.32500000 * 2560; time = 0.0124s; samplesPerSecond = 206952.3
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.09361877 * 2560; err = 0.33320312 * 2560; time = 0.0123s; samplesPerSecond = 207623.7
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12118683 * 2560; err = 0.34843750 * 2560; time = 0.0123s; samplesPerSecond = 207522.7
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.13457642 * 2560; err = 0.35195312 * 2560; time = 0.0123s; samplesPerSecond = 208113.2
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.09024963 * 2560; err = 0.33984375 * 2560; time = 0.0123s; samplesPerSecond = 208096.2
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07457275 * 2560; err = 0.33164063 * 2560; time = 0.0123s; samplesPerSecond = 208656.0
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.05975952 * 2560; err = 0.32070312 * 2560; time = 0.0123s; samplesPerSecond = 207421.8
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09778137 * 2560; err = 0.33242187 * 2560; time = 0.0123s; samplesPerSecond = 207809.1
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.01963196 * 2560; err = 0.32539062 * 2560; time = 0.0123s; samplesPerSecond = 207707.9
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.07533875 * 2560; err = 0.33515625 * 2560; time = 0.0123s; samplesPerSecond = 207606.8
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06417236 * 2560; err = 0.33007812 * 2560; time = 0.0123s; samplesPerSecond = 207775.3
08/16/2016 09:59:24:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04990234 * 2560; err = 0.33359375 * 2560; time = 0.0123s; samplesPerSecond = 208197.8
08/16/2016 09:59:24: Finished Epoch[ 2 of 2]: [Training] ce = 1.11232681 * 81920; err = 0.34179688 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.399382s
08/16/2016 09:59:24: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech'
08/16/2016 09:59:24: CNTKCommandTrainEnd: dptPre2

08/16/2016 09:59:24: Action "train" complete.


08/16/2016 09:59:24: ##############################################################################
08/16/2016 09:59:24: #                                                                            #
08/16/2016 09:59:24: # Action "edit"                                                              #
08/16/2016 09:59:24: #                                                                            #
08/16/2016 09:59:24: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=7, range=0.050000*1.000000, onCPU=false).
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=8, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 09:59:24: Action "edit" complete.


08/16/2016 09:59:24: ##############################################################################
08/16/2016 09:59:24: #                                                                            #
08/16/2016 09:59:24: # Action "train"                                                             #
08/16/2016 09:59:24: #                                                                            #
08/16/2016 09:59:24: ##############################################################################

08/16/2016 09:59:24: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 09:59:24: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 09:59:24: Loaded model with 29 nodes on GPU 0.

08/16/2016 09:59:24: Training criterion node(s):
08/16/2016 09:59:24: 	ce = CrossEntropyWithSoftmax

08/16/2016 09:59:24: Evaluation criterion node(s):
08/16/2016 09:59:24: 	err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 49 matrices, 27 are shared as 11, and 22 are not shared.

	{ HL2.t : [512 x 1 x *6] (gradient)
	  HL2.y : [512 x 1 x *6] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *6] (gradient)
	  HL2.z : [512 x 1 x *6] (gradient)
	  HL3.t : [512 x 1 x *6] }
	{ HL3.W : [512 x 512] (gradient)
	  HL3.z : [512 x 1 x *6] }
	{ HL3.t : [512 x 1 x *6] (gradient)
	  HL3.y : [512 x 1 x *6] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *6] (gradient)
	  HL3.z : [512 x 1 x *6] (gradient)
	  OL.t : [132 x 1 x *6] }
	{ HL1.t : [512 x *6] (gradient)
	  HL1.y : [512 x 1 x *6] }
	{ HL1.z : [512 x 1 x *6] (gradient)
	  HL2.t : [512 x 1 x *6] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *6] }
	{ HL3.b : [512 x 1] (gradient)
	  HL3.y : [512 x 1 x *6] (gradient)
	  OL.z : [132 x 1 x *6] (gradient) }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *6] }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *6] }


08/16/2016 09:59:24: Training 779396 parameters in 8 out of 8 parameter tensors and 20 nodes with gradient:

08/16/2016 09:59:24: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 09:59:24: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:24: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:59:24: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:24: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:59:24: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:24: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 09:59:24: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 09:59:24: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 09:59:24: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 09:59:24: Starting minibatch loop.
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.97086372 * 2560; err = 0.81445312 * 2560; time = 0.0194s; samplesPerSecond = 132149.5
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.63975792 * 2560; err = 0.63320312 * 2560; time = 0.0167s; samplesPerSecond = 152982.0
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.02565231 * 2560; err = 0.54257813 * 2560; time = 0.0167s; samplesPerSecond = 152945.4
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74204865 * 2560; err = 0.47500000 * 2560; time = 0.0168s; samplesPerSecond = 152535.3
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.58343964 * 2560; err = 0.45156250 * 2560; time = 0.0167s; samplesPerSecond = 153504.8
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.47893143 * 2560; err = 0.42343750 * 2560; time = 0.0167s; samplesPerSecond = 152954.5
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.43405457 * 2560; err = 0.40898438 * 2560; time = 0.0167s; samplesPerSecond = 153440.4
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35973663 * 2560; err = 0.39648438 * 2560; time = 0.0167s; samplesPerSecond = 153100.9
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.28108978 * 2560; err = 0.37968750 * 2560; time = 0.0168s; samplesPerSecond = 152699.1
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.29773560 * 2560; err = 0.39765625 * 2560; time = 0.0168s; samplesPerSecond = 152462.6
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.28441925 * 2560; err = 0.39062500 * 2560; time = 0.0168s; samplesPerSecond = 152744.6
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27777252 * 2560; err = 0.38164063 * 2560; time = 0.0167s; samplesPerSecond = 152863.2
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.23615112 * 2560; err = 0.37421875 * 2560; time = 0.0168s; samplesPerSecond = 152580.8
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.31171112 * 2560; err = 0.38671875 * 2560; time = 0.0168s; samplesPerSecond = 152680.9
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.25573883 * 2560; err = 0.37773438 * 2560; time = 0.0168s; samplesPerSecond = 152726.4
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.27382965 * 2560; err = 0.38398437 * 2560; time = 0.0168s; samplesPerSecond = 152190.7
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.20634155 * 2560; err = 0.36406250 * 2560; time = 0.0168s; samplesPerSecond = 152680.9
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.20973816 * 2560; err = 0.36562500 * 2560; time = 0.0168s; samplesPerSecond = 152790.2
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.20688782 * 2560; err = 0.36718750 * 2560; time = 0.0168s; samplesPerSecond = 152735.5
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.20260315 * 2560; err = 0.37226562 * 2560; time = 0.0168s; samplesPerSecond = 152381.0
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.20553894 * 2560; err = 0.37187500 * 2560; time = 0.0168s; samplesPerSecond = 152362.8
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.14160156 * 2560; err = 0.34726563 * 2560; time = 0.0168s; samplesPerSecond = 152580.8
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.15316467 * 2560; err = 0.35273437 * 2560; time = 0.0168s; samplesPerSecond = 152435.4
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.19352417 * 2560; err = 0.35468750 * 2560; time = 0.0168s; samplesPerSecond = 152145.5
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.17192078 * 2560; err = 0.35937500 * 2560; time = 0.0168s; samplesPerSecond = 152589.9
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.08281860 * 2560; err = 0.33867188 * 2560; time = 0.0168s; samplesPerSecond = 152154.5
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.11028442 * 2560; err = 0.34453125 * 2560; time = 0.0168s; samplesPerSecond = 152726.4
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.17454224 * 2560; err = 0.35312500 * 2560; time = 0.0168s; samplesPerSecond = 152635.3
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.11068115 * 2560; err = 0.34531250 * 2560; time = 0.0168s; samplesPerSecond = 152172.6
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.12955627 * 2560; err = 0.34296875 * 2560; time = 0.0168s; samplesPerSecond = 152208.8
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12482300 * 2560; err = 0.34570312 * 2560; time = 0.0168s; samplesPerSecond = 152254.1
08/16/2016 09:59:24:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12771912 * 2560; err = 0.34453125 * 2560; time = 0.0168s; samplesPerSecond = 152254.1
08/16/2016 09:59:24: Finished Epoch[ 1 of 4]: [Training] ce = 1.40639620 * 81920; err = 0.40274658 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.639354s
08/16/2016 09:59:25: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.1'

08/16/2016 09:59:25: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 09:59:25: Starting minibatch loop.
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.51739798 * 5120; err = 0.41425781 * 5120; time = 0.0292s; samplesPerSecond = 175198.5
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.25793447 * 5120; err = 0.37539062 * 5120; time = 0.0262s; samplesPerSecond = 195241.0
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.18638287 * 5120; err = 0.36718750 * 5120; time = 0.0262s; samplesPerSecond = 195211.2
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.12794571 * 5120; err = 0.34218750 * 5120; time = 0.0262s; samplesPerSecond = 195494.5
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14070625 * 5120; err = 0.34570312 * 5120; time = 0.0262s; samplesPerSecond = 195099.6
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.14582825 * 5120; err = 0.34765625 * 5120; time = 0.0263s; samplesPerSecond = 194884.3
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.11193542 * 5120; err = 0.34414062 * 5120; time = 0.0262s; samplesPerSecond = 195419.8
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08574600 * 5120; err = 0.33789062 * 5120; time = 0.0262s; samplesPerSecond = 195741.1
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.21058807 * 5120; err = 0.37363281 * 5120; time = 0.0262s; samplesPerSecond = 195397.5
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.09668579 * 5120; err = 0.34335938 * 5120; time = 0.0262s; samplesPerSecond = 195390.0
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05845032 * 5120; err = 0.32675781 * 5120; time = 0.0262s; samplesPerSecond = 195621.4
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.10728455 * 5120; err = 0.34726563 * 5120; time = 0.0261s; samplesPerSecond = 195883.4
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08716888 * 5120; err = 0.33593750 * 5120; time = 0.0261s; samplesPerSecond = 195816.0
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.06778870 * 5120; err = 0.31855469 * 5120; time = 0.0262s; samplesPerSecond = 195651.3
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04079590 * 5120; err = 0.32910156 * 5120; time = 0.0261s; samplesPerSecond = 195845.9
08/16/2016 09:59:25:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06249542 * 5120; err = 0.32968750 * 5120; time = 0.0261s; samplesPerSecond = 196025.9
08/16/2016 09:59:25: Finished Epoch[ 2 of 4]: [Training] ce = 1.14407091 * 81920; err = 0.34866943 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.425899s
08/16/2016 09:59:25: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.2'

08/16/2016 09:59:25: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/16/2016 09:59:25: Starting minibatch loop.
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.11238871 * 5120; err = 0.34804687 * 5120; time = 0.0269s; samplesPerSecond = 189988.5
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.09456148 * 5120; err = 0.34121094 * 5120; time = 0.0262s; samplesPerSecond = 195442.2
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.10800076 * 5120; err = 0.34667969 * 5120; time = 0.0262s; samplesPerSecond = 195636.4
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.16617966 * 5120; err = 0.35566406 * 5120; time = 0.0262s; samplesPerSecond = 195516.9
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14173546 * 5120; err = 0.34550781 * 5120; time = 0.0262s; samplesPerSecond = 195681.3
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07876053 * 5120; err = 0.33359375 * 5120; time = 0.0262s; samplesPerSecond = 195248.4
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08043213 * 5120; err = 0.33437500 * 5120; time = 0.0262s; samplesPerSecond = 195516.9
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07423630 * 5120; err = 0.33007812 * 5120; time = 0.0261s; samplesPerSecond = 195808.5
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.02659454 * 5120; err = 0.31113281 * 5120; time = 0.0262s; samplesPerSecond = 195599.0
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.04602737 * 5120; err = 0.31855469 * 5120; time = 0.0262s; samplesPerSecond = 195718.7
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05524902 * 5120; err = 0.33613281 * 5120; time = 0.0262s; samplesPerSecond = 195487.0
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07627411 * 5120; err = 0.33613281 * 5120; time = 0.0262s; samplesPerSecond = 195703.7
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05101776 * 5120; err = 0.31660156 * 5120; time = 0.0262s; samplesPerSecond = 195606.5
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.03016815 * 5120; err = 0.32480469 * 5120; time = 0.0262s; samplesPerSecond = 195741.1
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04644623 * 5120; err = 0.32929687 * 5120; time = 0.0261s; samplesPerSecond = 195860.9
08/16/2016 09:59:25:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02751465 * 5120; err = 0.32265625 * 5120; time = 0.0262s; samplesPerSecond = 195561.7
08/16/2016 09:59:25: Finished Epoch[ 3 of 4]: [Training] ce = 1.07597418 * 81920; err = 0.33315430 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.423092s
08/16/2016 09:59:25: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.3'

08/16/2016 09:59:26: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/16/2016 09:59:26: Starting minibatch loop.
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.03003817 * 5120; err = 0.31289062 * 5120; time = 0.0270s; samplesPerSecond = 189685.8
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.04547925 * 4926; err = 0.32947625 * 4926; time = 0.0703s; samplesPerSecond = 70111.0
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01249580 * 5120; err = 0.32246094 * 5120; time = 0.0262s; samplesPerSecond = 195397.5
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 0.99796486 * 5120; err = 0.31425781 * 5120; time = 0.0262s; samplesPerSecond = 195390.0
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 0.99781761 * 5120; err = 0.31464844 * 5120; time = 0.0261s; samplesPerSecond = 195823.5
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00107079 * 5120; err = 0.31855469 * 5120; time = 0.0262s; samplesPerSecond = 195434.8
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.02518806 * 5120; err = 0.31972656 * 5120; time = 0.0261s; samplesPerSecond = 196018.4
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00891876 * 5120; err = 0.31660156 * 5120; time = 0.0261s; samplesPerSecond = 196085.9
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99774780 * 5120; err = 0.30585937 * 5120; time = 0.0261s; samplesPerSecond = 195853.4
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.00037842 * 5120; err = 0.30722656 * 5120; time = 0.0262s; samplesPerSecond = 195419.8
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.02586746 * 5120; err = 0.31816406 * 5120; time = 0.0261s; samplesPerSecond = 195905.9
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06024628 * 5120; err = 0.33574219 * 5120; time = 0.0262s; samplesPerSecond = 195382.6
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.98301010 * 5120; err = 0.30214844 * 5120; time = 0.0262s; samplesPerSecond = 195404.9
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.96488800 * 5120; err = 0.30156250 * 5120; time = 0.0262s; samplesPerSecond = 195196.3
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.99069977 * 5120; err = 0.31640625 * 5120; time = 0.0261s; samplesPerSecond = 195913.4
08/16/2016 09:59:26:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.97961731 * 5120; err = 0.29921875 * 5120; time = 0.0262s; samplesPerSecond = 195263.3
08/16/2016 09:59:26: Finished Epoch[ 4 of 4]: [Training] ce = 1.00739784 * 81920; err = 0.31477051 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.468838s
08/16/2016 09:59:26: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech'
08/16/2016 09:59:26: CNTKCommandTrainEnd: speechTrain

08/16/2016 09:59:26: Action "train" complete.


08/16/2016 09:59:26: ##############################################################################
08/16/2016 09:59:26: #                                                                            #
08/16/2016 09:59:26: # Action "edit"                                                              #
08/16/2016 09:59:26: #                                                                            #
08/16/2016 09:59:26: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 09:59:26: Action "edit" complete.


08/16/2016 09:59:26: ##############################################################################
08/16/2016 09:59:26: #                                                                            #
08/16/2016 09:59:26: # Action "train"                                                             #
08/16/2016 09:59:26: #                                                                            #
08/16/2016 09:59:26: ##############################################################################

08/16/2016 09:59:26: CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/model.overalltying', '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list', '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/TestData/CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames

08/16/2016 09:59:26: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0'.

Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *9]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *9]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *9], [363 x 1], [363 x 1] -> [363 x *9]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *9] -> [512 x *9]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *9] -> [132 x 1 x *9]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *9], [132 x 1 x *9], [132 x 1 x *9] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *9], [132 x 1 x *9] -> [1]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 09:59:26: Loaded model with 29 nodes on GPU 0.

08/16/2016 09:59:26: Training criterion node(s):
08/16/2016 09:59:26: 	ce = SequenceWithSoftmax

08/16/2016 09:59:26: Evaluation criterion node(s):
08/16/2016 09:59:26: 	err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 50 matrices, 29 are shared as 12, and 21 are not shared.

	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *9] }
	{ HL1.t : [512 x *9] (gradient)
	  HL1.y : [512 x 1 x *9] }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *9] }
	{ HL3.W : [512 x 512] (gradient)
	  HL3.z : [512 x 1 x *9] }
	{ HL3.t : [512 x 1 x *9] (gradient)
	  HL3.y : [512 x 1 x *9] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *9] (gradient)
	  HL3.z : [512 x 1 x *9] (gradient)
	  OL.t : [132 x 1 x *9] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *9] }
	{ HL2.t : [512 x 1 x *9] (gradient)
	  HL2.y : [512 x 1 x *9] }
	{ HL3.b : [512 x 1] (gradient)
	  HL3.y : [512 x 1 x *9] (gradient)
	  OL.z : [132 x 1 x *9] (gradient) }
	{ OL.t : [132 x 1 x *9] (gradient)
	  scaledLogLikelihood : [132 x 1 x *9] (gradient) }
	{ HL1.z : [512 x 1 x *9] (gradient)
	  HL2.t : [512 x 1 x *9] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *9] (gradient)
	  HL2.z : [512 x 1 x *9] (gradient)
	  HL3.t : [512 x 1 x *9] }


08/16/2016 09:59:26: Training 779396 parameters in 8 out of 8 parameter tensors and 21 nodes with gradient:

08/16/2016 09:59:26: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 09:59:26: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:26: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:59:26: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:26: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:59:26: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:59:26: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 09:59:26: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 09:59:26: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-10
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

08/16/2016 09:59:26: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 09:59:31: Starting minibatch loop.
dengamma value 1.016951
dengamma value 1.090763
dengamma value 1.030578
dengamma value 1.078396
dengamma value 1.051528
dengamma value 1.075720
dengamma value 1.028965
dengamma value 1.025573
dengamma value 1.157795
dengamma value 0.955094
dengamma value 1.013404
dengamma value 1.062532
dengamma value 1.050060
dengamma value 1.085075
dengamma value 1.035518
dengamma value 1.088676
dengamma value 1.068275
dengamma value 1.081556
dengamma value 1.011559
dengamma value 1.045462
dengamma value 1.058040
dengamma value 1.078263
08/16/2016 09:59:32:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08468075 * 5566; err = 0.33093784 * 5566; time = 1.3574s; samplesPerSecond = 4100.5
dengamma value 1.000341
dengamma value 1.049229
dengamma value 1.033738
dengamma value 1.017619
dengamma value 1.036176
dengamma value 1.063524
dengamma value 1.122385
dengamma value 1.051660
dengamma value 1.040061
dengamma value 1.007486
dengamma value 1.001422
dengamma value 1.079357
dengamma value 1.034550
dengamma value 1.006944
dengamma value 1.054820
dengamma value 1.000686
dengamma value 1.009433
dengamma value 1.057346
dengamma value 1.029017
dengamma value 1.100386
dengamma value 1.041619
dengamma value 1.085854
dengamma value 1.049163
dengamma value 1.221116
dengamma value 1.062740
dengamma value 1.062362
08/16/2016 09:59:33:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07930380 * 7398; err = 0.31711273 * 7398; time = 0.6184s; samplesPerSecond = 11962.3
dengamma value 1.121956
dengamma value 1.003287
dengamma value 1.041682
dengamma value 0.988532
dengamma value 1.057405
dengamma value 1.078456
dengamma value 1.078765
dengamma value 1.073846
dengamma value 1.100777
dengamma value 1.102242
dengamma value 1.028463
dengamma value 1.079809
dengamma value 1.055759
dengamma value 0.942416
dengamma value 1.103102
dengamma value 1.038075
dengamma value 1.120044
dengamma value 1.016724
dengamma value 1.028796
dengamma value 1.056334
dengamma value 1.055807
dengamma value 1.053802
dengamma value 1.057694
dengamma value 1.108763
dengamma value 1.101425
08/16/2016 09:59:33:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.07753850 * 6300; err = 0.34031746 * 6300; time = 0.5236s; samplesPerSecond = 12033.0
dengamma value 1.107473
dengamma value 1.205362
dengamma value 1.070284
dengamma value 1.063702
dengamma value 1.046532
dengamma value 0.992090
dengamma value 1.003756
dengamma value 1.055656
dengamma value 1.113884
dengamma value 1.053838
dengamma value 1.044706
dengamma value 1.074876
dengamma value 1.086498
dengamma value 1.045390
dengamma value 1.115207
dengamma value 1.057181
dengamma value 1.008082
dengamma value 1.056726
dengamma value 1.019678
dengamma value 0.988532
dengamma value 0.931805
dengamma value 1.055384
08/16/2016 09:59:34:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.07894525 * 5636; err = 0.32665011 * 5636; time = 0.4634s; samplesPerSecond = 12162.4
dengamma value 1.046053
dengamma value 1.032466
dengamma value 1.046131
dengamma value 1.071244
dengamma value 1.064012
dengamma value 1.082939
dengamma value 1.074270
dengamma value 1.037943
dengamma value 0.953982
dengamma value 1.039899
dengamma value 1.022986
dengamma value 1.064703
dengamma value 1.056453
dengamma value 0.966001
dengamma value 1.056884
dengamma value 0.983751
dengamma value 1.096840
dengamma value 1.020773
dengamma value 1.057210
dengamma value 0.977982
08/16/2016 09:59:34:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08023927 * 6970; err = 0.34921090 * 6970; time = 0.5794s; samplesPerSecond = 12029.0
dengamma value 1.064818
dengamma value 1.043452
dengamma value 1.077275
dengamma value 1.042999
dengamma value 1.068105
dengamma value 1.072082
dengamma value 1.053097
dengamma value 1.076511
dengamma value 1.057454
dengamma value 1.057286
dengamma value 1.017794
dengamma value 1.064837
dengamma value 1.034165
dengamma value 1.067063
dengamma value 1.014472
dengamma value 1.092647
dengamma value 1.054529
dengamma value 1.002981
dengamma value 1.126155
dengamma value 1.009531
dengamma value 1.010978
dengamma value 1.068980
08/16/2016 09:59:35:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.07982592 * 6996; err = 0.32004002 * 6996; time = 0.6079s; samplesPerSecond = 11507.8
dengamma value 1.080229
dengamma value 1.027715
dengamma value 1.056990
dengamma value 1.050864
dengamma value 1.004432
dengamma value 1.014527
dengamma value 1.045878
dengamma value 0.981322
dengamma value 1.136079
dengamma value 1.001208
dengamma value 1.051288
dengamma value 1.014801
dengamma value 1.096011
dengamma value 1.092254
dengamma value 1.093714
dengamma value 0.976186
dengamma value 1.000405
dengamma value 1.044027
dengamma value 1.035734
dengamma value 1.129123
dengamma value 1.039699
dengamma value 1.065553
dengamma value 1.095157
dengamma value 1.029285
dengamma value 1.020676
08/16/2016 09:59:35:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08487790 * 6180; err = 0.33414239 * 6180; time = 0.5310s; samplesPerSecond = 11637.4
dengamma value 1.045464
dengamma value 1.067789
dengamma value 1.142396
dengamma value 1.009637
dengamma value 1.075449
dengamma value 1.039409
dengamma value 0.995406
dengamma value 1.065594
dengamma value 1.034967
dengamma value 1.049933
dengamma value 1.035808
dengamma value 1.039026
dengamma value 1.100130
dengamma value 0.975159
dengamma value 1.109057
dengamma value 1.051146
dengamma value 1.044200
dengamma value 1.108704
dengamma value 1.062042
dengamma value 1.126588
08/16/2016 09:59:36:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08340803 * 4860; err = 0.33600823 * 4860; time = 0.4055s; samplesPerSecond = 11985.4
dengamma value 1.040750
dengamma value 0.972598
dengamma value 1.026896
dengamma value 1.065175
dengamma value 1.034522
dengamma value 1.091507
dengamma value 1.012595
dengamma value 1.077661
dengamma value 1.020183
dengamma value 0.998050
dengamma value 1.048212
dengamma value 0.955666
dengamma value 1.027402
dengamma value 1.077979
dengamma value 1.052710
dengamma value 1.102155
dengamma value 1.069645
dengamma value 1.048981
dengamma value 1.042925
dengamma value 1.015464
dengamma value 1.111772
dengamma value 1.068376
08/16/2016 09:59:36:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.07605808 * 6046; err = 0.32699305 * 6046; time = 0.5182s; samplesPerSecond = 11666.8
dengamma value 1.040386
dengamma value 1.007380
dengamma value 1.017758
dengamma value 0.964054
dengamma value 1.029415
dengamma value 1.102677
dengamma value 1.043826
dengamma value 0.998165
dengamma value 1.036032
dengamma value 1.067882
dengamma value 0.996039
dengamma value 1.030009
dengamma value 1.057537
dengamma value 1.086534
dengamma value 1.004075
dengamma value 1.061849
dengamma value 1.015197
dengamma value 1.008634
dengamma value 1.002380
dengamma value 1.010348
dengamma value 0.999857
dengamma value 1.069942
dengamma value 1.028589
dengamma value 1.041727
08/16/2016 09:59:37:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08570117 * 6942; err = 0.34644195 * 6942; time = 0.5579s; samplesPerSecond = 12442.2
dengamma value 1.018619
dengamma value 1.046682
dengamma value 1.003926
dengamma value 0.938060
dengamma value 1.073180
dengamma value 1.092333
dengamma value 1.136608
dengamma value 1.023432
dengamma value 1.033466
dengamma value 1.108754
dengamma value 1.063739
dengamma value 1.007792
dengamma value 1.075606
dengamma value 1.032966
dengamma value 1.107917
dengamma value 1.016783
dengamma value 1.044823
dengamma value 1.067759
dengamma value 1.024058
dengamma value 1.004260
dengamma value 0.982422
dengamma value 1.064614
dengamma value 1.070432
08/16/2016 09:59:37:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08741906 * 5784; err = 0.33022130 * 5784; time = 0.4641s; samplesPerSecond = 12463.4
dengamma value 1.080328
dengamma value 1.086655
dengamma value 1.013888
dengamma value 1.012087
dengamma value 1.125584
dengamma value 1.005361
dengamma value 0.969809
dengamma value 0.963508
dengamma value 0.944890
dengamma value 1.039242
dengamma value 1.039696
dengamma value 0.993007
dengamma value 1.068278
dengamma value 1.038109
dengamma value 1.073677
dengamma value 0.994489
dengamma value 1.024373
dengamma value 1.046639
dengamma value 1.066258
dengamma value 0.992722
dengamma value 1.099014
08/16/2016 09:59:38:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08745384 * 6258; err = 0.34324065 * 6258; time = 0.5142s; samplesPerSecond = 12171.5
dengamma value 1.101366
dengamma value 1.094228
dengamma value 1.080228
dengamma value 1.082982
dengamma value 1.024713
dengamma value 1.118944
dengamma value 1.075450
dengamma value 1.018848
dengamma value 1.044026
dengamma value 1.024038
dengamma value 1.027898
dengamma value 1.065873
dengamma value 1.048962
dengamma value 1.082500
dengamma value 0.969462
dengamma value 1.006500
dengamma value 1.077539
dengamma value 1.016960
dengamma value 1.104984
dengamma value 1.005075
dengamma value 1.036383
dengamma value 0.972230
08/16/2016 09:59:38:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08172140 * 6116; err = 0.32946370 * 6116; time = 0.5145s; samplesPerSecond = 11887.6
dengamma value 1.044913
dengamma value 1.055020
dengamma value 1.002380
dengamma value 1.055052
08/16/2016 09:59:39: Finished Epoch[ 1 of 3]: [Training] ce = 0.08221044 * 82574; err = 0.33285296 * 82574; totalSamplesSeen = 82574; learningRatePerSample = 2e-06; epochTime=12.148s
08/16/2016 09:59:39: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.1'

08/16/2016 09:59:39: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 82146), data subset 0 of 1, with 1 datapasses

08/16/2016 09:59:39: Starting minibatch loop.
dengamma value 1.038088
dengamma value 1.002548
dengamma value 0.987010
dengamma value 1.071360
dengamma value 1.070828
dengamma value 1.009418
dengamma value 1.064635
dengamma value 1.013010
dengamma value 1.030044
dengamma value 1.030622
dengamma value 1.044549
dengamma value 1.010495
dengamma value 1.051460
dengamma value 1.090772
dengamma value 0.973568
dengamma value 0.998958
dengamma value 1.023571
dengamma value 1.025588
dengamma value 1.069013
dengamma value 1.050955
dengamma value 1.065861
dengamma value 1.098596
08/16/2016 09:59:39:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08266546 * 5826; err = 0.34637830 * 5826; time = 0.5012s; samplesPerSecond = 11625.2
dengamma value 1.064950
dengamma value 1.105561
dengamma value 1.088102
dengamma value 1.057782
dengamma value 1.052365
dengamma value 1.019249
dengamma value 1.086992
dengamma value 1.084409
dengamma value 1.099333
dengamma value 1.052746
dengamma value 1.043194
dengamma value 0.962219
dengamma value 1.080683
dengamma value 1.084176
dengamma value 1.038184
dengamma value 1.044506
dengamma value 1.042780
dengamma value 1.028939
dengamma value 1.045591
dengamma value 1.019333
08/16/2016 09:59:40:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07862259 * 6380; err = 0.31771160 * 6380; time = 0.5561s; samplesPerSecond = 11473.3
dengamma value 1.097878
dengamma value 1.030249
dengamma value 1.117329
dengamma value 0.947982
dengamma value 1.039146
dengamma value 1.078896
dengamma value 1.112872
dengamma value 1.039877
dengamma value 1.075539
dengamma value 1.035017
dengamma value 0.996563
dengamma value 1.089636
dengamma value 1.032593
dengamma value 1.060184
dengamma value 1.022234
dengamma value 1.025989
dengamma value 1.060254
dengamma value 1.087375
dengamma value 1.042716
dengamma value 1.045934
dengamma value 1.090363
dengamma value 1.062508
dengamma value 0.998471
08/16/2016 09:59:40:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.07571975 * 6574; err = 0.32278674 * 6574; time = 0.5966s; samplesPerSecond = 11019.4
dengamma value 1.086696
dengamma value 1.048196
dengamma value 1.054776
dengamma value 1.056882
dengamma value 0.933055
dengamma value 0.965800
dengamma value 1.138787
dengamma value 1.083072
dengamma value 1.030130
dengamma value 1.030360
dengamma value 1.056761
dengamma value 0.972244
dengamma value 1.055198
dengamma value 1.018788
dengamma value 0.972026
dengamma value 1.083465
dengamma value 1.074164
dengamma value 1.103243
dengamma value 0.994083
dengamma value 0.986000
dengamma value 1.081591
dengamma value 1.127210
dengamma value 1.010590
08/16/2016 09:59:41:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08651959 * 6324; err = 0.34203036 * 6324; time = 0.5153s; samplesPerSecond = 12273.1
dengamma value 0.961097
dengamma value 0.942765
dengamma value 0.942351
dengamma value 1.054443
dengamma value 1.085019
dengamma value 1.104924
dengamma value 1.047166
dengamma value 1.155987
dengamma value 1.035481
dengamma value 0.987519
dengamma value 1.017748
dengamma value 1.022225
dengamma value 0.962975
dengamma value 1.105984
dengamma value 1.043334
dengamma value 1.000560
dengamma value 0.944519
dengamma value 0.998488
dengamma value 1.026647
dengamma value 1.055243
08/16/2016 09:59:41:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08918198 * 4800; err = 0.36812500 * 4800; time = 0.3841s; samplesPerSecond = 12496.6
dengamma value 1.001891
dengamma value 1.050395
dengamma value 1.050062
dengamma value 1.061193
dengamma value 1.048663
dengamma value 1.126828
dengamma value 1.100739
dengamma value 1.027145
dengamma value 1.072075
dengamma value 1.017510
dengamma value 1.076090
dengamma value 1.017353
dengamma value 0.993484
dengamma value 1.005100
dengamma value 1.069697
dengamma value 1.028585
dengamma value 1.027644
dengamma value 1.021442
dengamma value 1.020226
dengamma value 1.042390
dengamma value 1.027079
dengamma value 1.061462
08/16/2016 09:59:42:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08016600 * 6176; err = 0.35443653 * 6176; time = 0.4943s; samplesPerSecond = 12493.9
dengamma value 1.096101
dengamma value 1.062995
dengamma value 1.078059
dengamma value 0.989344
dengamma value 1.080757
dengamma value 1.055379
dengamma value 1.096505
dengamma value 0.957997
dengamma value 1.066838
dengamma value 1.035295
dengamma value 1.065538
dengamma value 1.023607
dengamma value 1.060317
dengamma value 1.002619
dengamma value 1.075739
dengamma value 1.046245
dengamma value 1.101097
dengamma value 1.151892
dengamma value 1.041482
dengamma value 0.967724
dengamma value 1.085342
dengamma value 1.054153
dengamma value 1.041504
08/16/2016 09:59:42:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08858833 * 5534; err = 0.31387785 * 5534; time = 0.4926s; samplesPerSecond = 11234.5
dengamma value 1.067130
dengamma value 1.007158
dengamma value 0.986670
dengamma value 1.029443
dengamma value 1.114901
dengamma value 0.969158
dengamma value 1.074201
dengamma value 1.066679
dengamma value 1.056504
dengamma value 1.052486
dengamma value 1.104449
dengamma value 1.115204
dengamma value 1.058134
dengamma value 1.084407
dengamma value 1.046963
dengamma value 1.046735
dengamma value 1.054714
dengamma value 0.997923
dengamma value 1.068547
dengamma value 1.044441
dengamma value 1.006314
dengamma value 1.075537
08/16/2016 09:59:43:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08162021 * 5936; err = 0.31822776 * 5936; time = 0.5603s; samplesPerSecond = 10594.6
dengamma value 0.916216
dengamma value 1.074995
dengamma value 1.078435
dengamma value 1.057328
dengamma value 1.078485
dengamma value 1.014861
dengamma value 1.078280
dengamma value 1.042853
dengamma value 1.037249
dengamma value 1.071795
dengamma value 0.911832
dengamma value 1.077340
dengamma value 1.124653
dengamma value 1.091120
dengamma value 1.008635
dengamma value 1.089288
dengamma value 1.092013
dengamma value 1.083488
dengamma value 1.100742
dengamma value 1.108284
dengamma value 1.063566
08/16/2016 09:59:43:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08116978 * 5248; err = 0.33250762 * 5248; time = 0.4433s; samplesPerSecond = 11838.6
dengamma value 1.047409
dengamma value 1.037768
dengamma value 1.056159
dengamma value 1.024341
dengamma value 1.056013
dengamma value 1.070255
dengamma value 1.048169
dengamma value 0.997240
dengamma value 1.063288
dengamma value 1.048935
dengamma value 1.120314
dengamma value 1.079426
dengamma value 0.977880
dengamma value 1.088436
dengamma value 1.104661
dengamma value 1.038362
dengamma value 1.099800
dengamma value 1.101263
dengamma value 1.057283
dengamma value 1.034506
dengamma value 1.008389
dengamma value 1.062621
dengamma value 1.054342
dengamma value 1.062158
dengamma value 1.028869
dengamma value 1.072664
08/16/2016 09:59:44:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08035083 * 6888; err = 0.31939605 * 6888; time = 0.5963s; samplesPerSecond = 11550.9
dengamma value 1.051606
dengamma value 1.077176
dengamma value 1.040242
dengamma value 1.052068
dengamma value 1.030234
dengamma value 1.035224
dengamma value 1.019526
dengamma value 1.093799
dengamma value 1.070660
dengamma value 1.031426
dengamma value 1.032620
dengamma value 1.101631
dengamma value 1.078673
dengamma value 1.010443
dengamma value 1.098603
dengamma value 1.014724
dengamma value 1.011566
dengamma value 1.126488
dengamma value 1.057342
dengamma value 1.039591
dengamma value 1.021315
dengamma value 1.023771
dengamma value 0.984565
dengamma value 1.040249
08/16/2016 09:59:44:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08466605 * 6572; err = 0.34342666 * 6572; time = 0.5451s; samplesPerSecond = 12056.4
dengamma value 0.998906
dengamma value 1.085306
dengamma value 1.021620
dengamma value 1.092687
dengamma value 1.054521
dengamma value 1.014457
dengamma value 1.014129
dengamma value 1.077565
dengamma value 1.044229
dengamma value 1.004763
dengamma value 1.057981
dengamma value 1.067377
dengamma value 1.072525
dengamma value 1.052167
dengamma value 1.031651
dengamma value 1.044801
dengamma value 1.029338
dengamma value 1.108843
dengamma value 1.001231
dengamma value 1.062691
dengamma value 1.052591
dengamma value 1.131819
dengamma value 1.072210
dengamma value 1.030625
08/16/2016 09:59:45:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08517949 * 6622; err = 0.31289641 * 6622; time = 0.5941s; samplesPerSecond = 11146.7
dengamma value 1.032055
dengamma value 1.096506
dengamma value 1.021111
dengamma value 0.987919
dengamma value 1.039445
dengamma value 1.029018
dengamma value 0.952959
dengamma value 0.995627
dengamma value 0.949031
dengamma value 1.053269
dengamma value 1.073244
dengamma value 1.135889
dengamma value 1.109631
dengamma value 1.096985
dengamma value 1.029104
dengamma value 1.048600
dengamma value 1.014164
dengamma value 1.039531
dengamma value 1.034862
dengamma value 0.976176
dengamma value 1.050905
dengamma value 1.092453
dengamma value 1.038136
08/16/2016 09:59:45:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08575490 * 5824; err = 0.32417582 * 5824; time = 0.4584s; samplesPerSecond = 12706.1
dengamma value 0.981325
dengamma value 1.041492
dengamma value 1.111667
dengamma value 0.966546
dengamma value 1.016518
dengamma value 1.091933
dengamma value 1.068772
dengamma value 1.161398
dengamma value 1.039845
08/16/2016 09:59:46: Finished Epoch[ 2 of 3]: [Training] ce = 0.08283421 * 81776; err = 0.33025582 * 81776; totalSamplesSeen = 164350; learningRatePerSample = 2e-06; epochTime=7.02008s
08/16/2016 09:59:46: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.2'

08/16/2016 09:59:46: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163922), data subset 0 of 1, with 1 datapasses

08/16/2016 09:59:46: Starting minibatch loop.
dengamma value 1.085631
dengamma value 1.032653
dengamma value 1.072817
dengamma value 1.078261
dengamma value 1.092765
dengamma value 1.007783
dengamma value 1.032480
dengamma value 1.003907
dengamma value 1.020699
dengamma value 1.011118
dengamma value 1.048709
dengamma value 1.039067
dengamma value 1.028426
dengamma value 1.057734
dengamma value 1.057475
dengamma value 1.128096
dengamma value 1.013283
dengamma value 1.114727
dengamma value 1.028415
dengamma value 1.071979
dengamma value 1.005469
dengamma value 1.054217
dengamma value 1.103122
08/16/2016 09:59:46:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08242372 * 5074; err = 0.33011431 * 5074; time = 0.4574s; samplesPerSecond = 11094.1
dengamma value 1.012585
dengamma value 1.099718
dengamma value 1.057921
dengamma value 1.070138
dengamma value 0.994067
dengamma value 0.995537
dengamma value 0.996810
dengamma value 0.999128
dengamma value 1.112330
dengamma value 1.067682
dengamma value 1.038603
dengamma value 1.025632
dengamma value 1.044260
dengamma value 1.046131
dengamma value 1.090714
dengamma value 1.093417
dengamma value 1.034906
dengamma value 1.044763
dengamma value 1.087702
dengamma value 0.970573
dengamma value 1.046709
dengamma value 1.023588
08/16/2016 09:59:47:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07981886 * 7136; err = 0.32315022 * 7136; time = 0.5896s; samplesPerSecond = 12103.5
dengamma value 1.093834
dengamma value 1.015774
dengamma value 0.996985
dengamma value 1.012523
dengamma value 1.077096
dengamma value 1.027190
dengamma value 1.119522
dengamma value 0.996340
dengamma value 1.021191
dengamma value 1.065130
dengamma value 1.088257
dengamma value 1.057078
dengamma value 1.100466
dengamma value 1.021776
dengamma value 1.070165
dengamma value 1.022536
dengamma value 1.051410
dengamma value 1.018535
dengamma value 1.111403
dengamma value 1.114549
dengamma value 1.034455
dengamma value 1.007732
dengamma value 1.024432
08/16/2016 09:59:47:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.09111519 * 5504; err = 0.34520349 * 5504; time = 0.4844s; samplesPerSecond = 11363.1
dengamma value 1.050855
dengamma value 1.066213
dengamma value 1.055944
dengamma value 1.078349
dengamma value 1.063846
dengamma value 1.001603
dengamma value 1.064124
dengamma value 1.076174
dengamma value 1.023789
dengamma value 1.123867
dengamma value 1.043842
dengamma value 1.062468
dengamma value 1.052183
dengamma value 1.097915
dengamma value 0.979307
dengamma value 1.049688
dengamma value 1.006878
dengamma value 1.024591
dengamma value 0.941949
dengamma value 1.133573
dengamma value 1.066064
08/16/2016 09:59:48:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08181854 * 6028; err = 0.33560053 * 6028; time = 0.5298s; samplesPerSecond = 11376.9
dengamma value 1.066585
dengamma value 1.100234
dengamma value 1.069437
dengamma value 1.066658
dengamma value 0.988324
dengamma value 0.980409
dengamma value 1.052009
dengamma value 1.002361
dengamma value 1.044768
dengamma value 1.112311
dengamma value 1.071654
dengamma value 1.030606
dengamma value 1.069438
dengamma value 1.058430
dengamma value 1.054340
dengamma value 1.002708
dengamma value 1.019374
dengamma value 1.149197
dengamma value 1.016561
dengamma value 1.046652
dengamma value 1.031633
08/16/2016 09:59:48:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08364014 * 6028; err = 0.33560053 * 6028; time = 0.4928s; samplesPerSecond = 12231.2
dengamma value 1.028070
dengamma value 1.045467
dengamma value 1.017207
dengamma value 0.993784
dengamma value 1.053291
dengamma value 1.102319
dengamma value 1.089345
dengamma value 1.061480
dengamma value 0.981645
dengamma value 1.175551
dengamma value 0.994262
dengamma value 0.991496
dengamma value 1.063669
dengamma value 1.044150
dengamma value 1.042425
dengamma value 1.087742
dengamma value 0.998621
dengamma value 0.983086
dengamma value 1.061653
dengamma value 1.021335
dengamma value 1.065320
dengamma value 1.047158
dengamma value 1.009269
dengamma value 1.023060
08/16/2016 09:59:49:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08396004 * 6782; err = 0.33190799 * 6782; time = 0.5628s; samplesPerSecond = 12051.5
dengamma value 1.063373
dengamma value 1.057358
dengamma value 1.109836
dengamma value 1.019116
dengamma value 1.095504
dengamma value 1.046314
dengamma value 1.105244
dengamma value 1.103004
dengamma value 1.081641
dengamma value 1.028557
dengamma value 1.078995
dengamma value 1.099618
dengamma value 1.001807
dengamma value 1.129626
dengamma value 1.129011
dengamma value 1.004430
dengamma value 1.055570
dengamma value 1.120236
dengamma value 1.067422
dengamma value 0.997786
dengamma value 1.023166
08/16/2016 09:59:49:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07731833 * 5458; err = 0.30230854 * 5458; time = 0.5094s; samplesPerSecond = 10714.3
dengamma value 1.066885
dengamma value 1.029330
dengamma value 1.084841
dengamma value 1.060538
dengamma value 1.228047
dengamma value 1.095044
dengamma value 0.978831
dengamma value 1.073194
dengamma value 1.084648
dengamma value 1.046759
dengamma value 1.047481
dengamma value 1.077822
dengamma value 1.123007
dengamma value 1.162763
dengamma value 1.047474
dengamma value 1.068705
dengamma value 1.023569
dengamma value 1.023528
dengamma value 1.008829
dengamma value 1.108584
dengamma value 1.081726
dengamma value 1.112577
dengamma value 1.090438
dengamma value 1.095271
dengamma value 1.017312
08/16/2016 09:59:50:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.07864294 * 6610; err = 0.29924357 * 6610; time = 0.5699s; samplesPerSecond = 11598.1
dengamma value 1.075826
dengamma value 1.067948
dengamma value 1.110302
dengamma value 1.081711
dengamma value 1.067880
dengamma value 0.978330
dengamma value 1.029284
dengamma value 1.073561
dengamma value 1.084059
dengamma value 1.066804
dengamma value 1.036852
dengamma value 0.953758
dengamma value 1.118915
dengamma value 1.105659
dengamma value 1.013800
dengamma value 1.045754
dengamma value 0.912958
dengamma value 1.062305
dengamma value 1.108428
dengamma value 1.055655
dengamma value 1.026025
dengamma value 1.084758
dengamma value 1.040212
08/16/2016 09:59:50:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08365357 * 5854; err = 0.32302699 * 5854; time = 0.4711s; samplesPerSecond = 12426.1
dengamma value 1.062826
dengamma value 1.129052
dengamma value 1.093953
dengamma value 0.883005
dengamma value 1.052997
dengamma value 1.060440
dengamma value 0.975450
dengamma value 1.086084
dengamma value 1.026298
dengamma value 1.029125
dengamma value 1.029763
dengamma value 1.016837
dengamma value 1.105651
dengamma value 1.054375
dengamma value 1.113862
dengamma value 1.079821
dengamma value 1.052164
dengamma value 1.105429
dengamma value 1.042696
dengamma value 0.973192
dengamma value 1.029195
dengamma value 1.063837
dengamma value 1.029671
08/16/2016 09:59:51:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08812171 * 4674; err = 0.33483098 * 4674; time = 0.4003s; samplesPerSecond = 11675.3
dengamma value 0.997764
dengamma value 1.035159
dengamma value 1.058941
dengamma value 1.075590
dengamma value 1.030439
dengamma value 1.078578
dengamma value 1.044169
dengamma value 1.009967
dengamma value 1.125321
dengamma value 1.044931
dengamma value 1.036468
dengamma value 1.062922
dengamma value 1.070043
dengamma value 1.009147
dengamma value 0.922847
dengamma value 0.981278
dengamma value 0.998950
dengamma value 1.053180
dengamma value 1.037255
dengamma value 0.980541
dengamma value 1.083944
08/16/2016 09:59:51:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08040503 * 6248; err = 0.33034571 * 6248; time = 0.5212s; samplesPerSecond = 11987.2
dengamma value 0.975813
dengamma value 1.051268
dengamma value 1.058537
dengamma value 1.076220
dengamma value 1.143107
dengamma value 1.025791
dengamma value 1.054328
dengamma value 1.050157
dengamma value 1.095524
dengamma value 1.000419
dengamma value 1.005942
dengamma value 1.012887
dengamma value 1.061799
dengamma value 1.055634
dengamma value 0.992318
dengamma value 1.083854
dengamma value 1.117213
dengamma value 1.028256
dengamma value 1.040341
dengamma value 1.154055
dengamma value 1.104128
dengamma value 1.052442
dengamma value 1.075892
08/16/2016 09:59:52:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.07677648 * 7094; err = 0.32111644 * 7094; time = 0.5991s; samplesPerSecond = 11841.0
dengamma value 1.051146
dengamma value 1.075785
dengamma value 1.054600
dengamma value 1.056107
dengamma value 1.045453
dengamma value 1.047608
dengamma value 1.052785
dengamma value 0.977057
dengamma value 0.998293
dengamma value 1.053047
dengamma value 1.063802
dengamma value 1.039332
dengamma value 1.011984
dengamma value 1.086482
dengamma value 0.997638
dengamma value 1.094073
dengamma value 1.058531
dengamma value 1.054121
dengamma value 1.028720
dengamma value 1.040245
dengamma value 1.064497
dengamma value 1.051749
08/16/2016 09:59:52:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08554733 * 6246; err = 0.32196606 * 6246; time = 0.5431s; samplesPerSecond = 11500.5
dengamma value 1.009673
dengamma value 1.054432
dengamma value 1.046565
dengamma value 1.075888
dengamma value 1.040106
dengamma value 0.970557
dengamma value 1.008954
dengamma value 1.132209
dengamma value 1.071974
dengamma value 0.992598
dengamma value 0.947651
dengamma value 1.062592
dengamma value 1.062592
08/16/2016 09:59:53: Finished Epoch[ 3 of 3]: [Training] ce = 0.08264888 * 81970; err = 0.32607051 * 81970; totalSamplesSeen = 246320; learningRatePerSample = 2e-06; epochTime=7.02045s
08/16/2016 09:59:53: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence'
08/16/2016 09:59:53: CNTKCommandTrainEnd: sequenceTrain

08/16/2016 09:59:53: Action "train" complete.

08/16/2016 09:59:53: __COMPLETED__
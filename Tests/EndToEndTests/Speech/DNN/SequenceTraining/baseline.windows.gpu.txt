=== Running /cygdrive/E/NetScale/CNTK/git_repos/cplx_master2/x64/debug/cntk.exe configFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/cntk_dpt.config RunDir=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining DeviceId=0
-------------------------------------------------------------------
Build info: 

		Built time: Nov 18 2015 10:44:50
		Last modified date: Fri Nov 13 10:07:10 2015
		Built by amitaga on Amitaga-Win-DT3           
		Build Path: E:\NetScale\CNTK\git_repos\cplx_master2\MachineLearning\CNTK\
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
-------------------------------------------------------------------
running on Amitaga-Win-DT3 at 2015/11/18 21:52:32
command line: 
E:\NetScale\CNTK\git_repos\cplx_master2\x64\debug\cntk.exe configFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/cntk_dpt.config RunDir=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining DeviceId=0 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision=float
deviceId=$DeviceId$
command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros=$ConfigDir$/macros.txt
GlobalMean=GlobalStats/mean.363
GlobalInvStd=GlobalStats/var.363
GlobalPrior=GlobalStats/prior.132
traceLevel=1
Truncated=false
SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]
DPT_Pre1=[
    action=train
    modelPath=$RunDir$/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/dnn_1layer.txt
    ]
]
AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=$RunDir$/models/Pre1/cntkSpeech
    NewModel=$RunDir$/models/Pre2/cntkSpeech.0
    editPath=$ConfigDir$/add_layer.mel
]
DPT_Pre2=[
    action=train
    modelPath=$RunDir$/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/dnn_1layer.txt
    ]
]
AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=$RunDir$/models/Pre2/cntkSpeech
    NewModel=$RunDir$/models/cntkSpeech.0
    editPath=$ConfigDir$/add_layer.mel
]
speechTrain=[
    action=train
    modelPath=$RunDir$/models/cntkSpeech
    deviceId=$DeviceId$
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]
reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=$DataDir$/glob_0000.scp
  ]
  labels=[
      mlfFile=$DataDir$/glob_0000.mlf
      labelMappingFile=$DataDir$/state.list
      labelDim=132
      labelType=Category
  ]
]
replaceCriterionNode=[
    action=edit
    CurrModel=$RunDir$/models/cntkSpeech
    NewModel=$RunDir$/models/cntkSpeech.sequence.0
    editPath=$ConfigDir$/replace_ce_with_sequence_criterion.mel
]
sequenceTrain=[
    action=train
    modelPath=$RunDir$/models/cntkSpeech.sequence
    deviceId=$DeviceId$
    traceLevel=1
    NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/nonexistentfile.txt
    ]
  SGD=[
      epochSize=81920
      minibatchSize=10
      learningRatesPerSample=0.000002
      momentumPerSample=0.999589
      dropoutRate=0.0
      maxEpochs=3
      hsmoothingWeight=0.95
      frameDropThresh=1e-10
      numMBsToShowResult=10
      gradientClippingWithTruncation=true
      clippingThresholdPerSample=1.0
  ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      frameMode=false
      nbruttsineachrecurrentiter=2
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=$DataDir$/glob_0000.scp
      ]
      labels=[
          mlfFile=$DataDir$/glob_0000.mlf
          labelMappingFile=$DataDir$/state.list
          labelDim=132
          labelType=Category
      ]
      hmms=[
          phoneFile=$DataDir$/model.overalltying
          transpFile=$DataDir$/model.transprob
      ]
      lattices=[
          denlatTocFile=$DataDir$/*.lats.toc
      ]
    ]
]
RunDir=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu
DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision=float
deviceId=0
command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/macros.txt
GlobalMean=GlobalStats/mean.363
GlobalInvStd=GlobalStats/var.363
GlobalPrior=GlobalStats/prior.132
traceLevel=1
Truncated=false
SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]
DPT_Pre1=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/dnn_1layer.txt
    ]
]
AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NewModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0
    editPath=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/add_layer.mel
]
DPT_Pre2=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/dnn_1layer.txt
    ]
]
AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NewModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0
    editPath=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/add_layer.mel
]
speechTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    deviceId=0
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]
reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp
  ]
  labels=[
      mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
      labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
      labelDim=132
      labelType=Category
  ]
]
replaceCriterionNode=[
    action=edit
    CurrModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    NewModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0
    editPath=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel
]
sequenceTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence
    deviceId=0
    traceLevel=1
    NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/nonexistentfile.txt
    ]
  SGD=[
      epochSize=81920
      minibatchSize=10
      learningRatesPerSample=0.000002
      momentumPerSample=0.999589
      dropoutRate=0.0
      maxEpochs=3
      hsmoothingWeight=0.95
      frameDropThresh=1e-10
      numMBsToShowResult=10
      gradientClippingWithTruncation=true
      clippingThresholdPerSample=1.0
  ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      frameMode=false
      nbruttsineachrecurrentiter=2
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp
      ]
      labels=[
          mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
          labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
          labelDim=132
          labelType=Category
      ]
      hmms=[
          phoneFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/model.overalltying
          transpFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/model.transprob
      ]
      lattices=[
          denlatTocFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/*.lats.toc
      ]
    ]
]
RunDir=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu
DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.config:AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NewModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0
    editPath=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/add_layer.mel
]

configparameters: cntk_dpt.config:AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NewModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0
    editPath=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/add_layer.mel
]

configparameters: cntk_dpt.config:command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_dpt.config:ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining
configparameters: cntk_dpt.config:DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
configparameters: cntk_dpt.config:deviceId=0
configparameters: cntk_dpt.config:DPT_Pre1=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/dnn_1layer.txt
    ]
]

configparameters: cntk_dpt.config:DPT_Pre2=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/dnn_1layer.txt
    ]
]

configparameters: cntk_dpt.config:GlobalInvStd=GlobalStats/var.363
configparameters: cntk_dpt.config:GlobalMean=GlobalStats/mean.363
configparameters: cntk_dpt.config:GlobalPrior=GlobalStats/prior.132
configparameters: cntk_dpt.config:ndlMacros=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/macros.txt
configparameters: cntk_dpt.config:precision=float
configparameters: cntk_dpt.config:reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp
  ]
  labels=[
      mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
      labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
      labelDim=132
      labelType=Category
  ]
]

configparameters: cntk_dpt.config:replaceCriterionNode=[
    action=edit
    CurrModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    NewModel=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0
    editPath=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel
]

configparameters: cntk_dpt.config:RunDir=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu
configparameters: cntk_dpt.config:sequenceTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence
    deviceId=0
    traceLevel=1
    NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/nonexistentfile.txt
    ]
  SGD=[
      epochSize=81920
      minibatchSize=10
      learningRatesPerSample=0.000002
      momentumPerSample=0.999589
      dropoutRate=0.0
      maxEpochs=3
      hsmoothingWeight=0.95
      frameDropThresh=1e-10
      numMBsToShowResult=10
      gradientClippingWithTruncation=true
      clippingThresholdPerSample=1.0
  ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      frameMode=false
      nbruttsineachrecurrentiter=2
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp
      ]
      labels=[
          mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
          labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
          labelDim=132
          labelType=Category
      ]
      hmms=[
          phoneFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/model.overalltying
          transpFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/model.transprob
      ]
      lattices=[
          denlatTocFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/*.lats.toc
      ]
    ]
]

configparameters: cntk_dpt.config:SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]

configparameters: cntk_dpt.config:speechTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech
    deviceId=0
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\DNN\SequenceTraining/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]

configparameters: cntk_dpt.config:traceLevel=1
configparameters: cntk_dpt.config:Truncated=false
<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: DPT_Pre1 AddLayer2 DPT_Pre2 AddLayer3 speechTrain replaceCriterionNode sequenceTrain 
precision = float
CNTKModelPath: C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre1/cntkSpeech
CNTKCommandTrainInfo: DPT_Pre1 : 2
CNTKModelPath: C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech
CNTKCommandTrainInfo: DPT_Pre2 : 2
CNTKModelPath: C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech
CNTKCommandTrainInfo: speechTrain : 4
CNTKModelPath: C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence
CNTKCommandTrainInfo: sequenceTrain : 3
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11
CNTKCommandTrainBegin: DPT_Pre1
NDLBuilder Using GPU 0
reading script file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
htkmlfreader: reading MLF file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 1], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 1], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 1] = InputValue
HL1.W[512, 363] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 1] = InputValue

Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.

SetUniformRandomValue (GPU): creating curand object with seed 1
GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  3.89978180; EvalErr[0]PerSample = 0.84375000; TotalTime = 0.48773s; TotalTimePerSample = 0.19052ms; SamplesPerSecond = 5248
 Epoch[ 1 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.96755676; EvalErr[0]PerSample = 0.72031250; TotalTime = 0.21008s; TotalTimePerSample = 0.08206ms; SamplesPerSecond = 12185
 Epoch[ 1 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.55723495; EvalErr[0]PerSample = 0.65859375; TotalTime = 0.20945s; TotalTimePerSample = 0.08182ms; SamplesPerSecond = 12222
 Epoch[ 1 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.29642792; EvalErr[0]PerSample = 0.61992187; TotalTime = 0.20787s; TotalTimePerSample = 0.08120ms; SamplesPerSecond = 12315
 Epoch[ 1 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.02396469; EvalErr[0]PerSample = 0.55117187; TotalTime = 0.21016s; TotalTimePerSample = 0.08209ms; SamplesPerSecond = 12181
 Epoch[ 1 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.87309265; EvalErr[0]PerSample = 0.51484375; TotalTime = 0.21184s; TotalTimePerSample = 0.08275ms; SamplesPerSecond = 12084
 Epoch[ 1 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.78157196; EvalErr[0]PerSample = 0.50507813; TotalTime = 0.21247s; TotalTimePerSample = 0.08299ms; SamplesPerSecond = 12048
 Epoch[ 1 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.75391235; EvalErr[0]PerSample = 0.50781250; TotalTime = 0.21202s; TotalTimePerSample = 0.08282ms; SamplesPerSecond = 12074
 Epoch[ 1 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.66460266; EvalErr[0]PerSample = 0.45742187; TotalTime = 0.21201s; TotalTimePerSample = 0.08282ms; SamplesPerSecond = 12074
 Epoch[ 1 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.62184143; EvalErr[0]PerSample = 0.47968750; TotalTime = 0.21245s; TotalTimePerSample = 0.08299ms; SamplesPerSecond = 12049
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.65328064; EvalErr[0]PerSample = 0.47265625; TotalTime = 0.21265s; TotalTimePerSample = 0.08306ms; SamplesPerSecond = 12038
 Epoch[ 1 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.50686951; EvalErr[0]PerSample = 0.44921875; TotalTime = 0.21170s; TotalTimePerSample = 0.08270ms; SamplesPerSecond = 12092
 Epoch[ 1 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.46723938; EvalErr[0]PerSample = 0.42304687; TotalTime = 0.21177s; TotalTimePerSample = 0.08272ms; SamplesPerSecond = 12088
 Epoch[ 1 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.49163513; EvalErr[0]PerSample = 0.44140625; TotalTime = 0.21084s; TotalTimePerSample = 0.08236ms; SamplesPerSecond = 12141
 Epoch[ 1 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.46437683; EvalErr[0]PerSample = 0.43398437; TotalTime = 0.21110s; TotalTimePerSample = 0.08246ms; SamplesPerSecond = 12126
 Epoch[ 1 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.43047485; EvalErr[0]PerSample = 0.43867187; TotalTime = 0.21159s; TotalTimePerSample = 0.08265ms; SamplesPerSecond = 12099
 Epoch[ 1 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.42106018; EvalErr[0]PerSample = 0.41992188; TotalTime = 0.21012s; TotalTimePerSample = 0.08208ms; SamplesPerSecond = 12183
 Epoch[ 1 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.46538086; EvalErr[0]PerSample = 0.42421875; TotalTime = 0.21078s; TotalTimePerSample = 0.08234ms; SamplesPerSecond = 12145
 Epoch[ 1 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.47427673; EvalErr[0]PerSample = 0.44062500; TotalTime = 0.21170s; TotalTimePerSample = 0.08269ms; SamplesPerSecond = 12092
 Epoch[ 1 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.42847290; EvalErr[0]PerSample = 0.44023438; TotalTime = 0.21308s; TotalTimePerSample = 0.08323ms; SamplesPerSecond = 12014
 Epoch[ 1 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.34078064; EvalErr[0]PerSample = 0.41171875; TotalTime = 0.21073s; TotalTimePerSample = 0.08232ms; SamplesPerSecond = 12148
 Epoch[ 1 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.39474487; EvalErr[0]PerSample = 0.42734375; TotalTime = 0.21241s; TotalTimePerSample = 0.08297ms; SamplesPerSecond = 12052
 Epoch[ 1 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.40151062; EvalErr[0]PerSample = 0.41250000; TotalTime = 0.20998s; TotalTimePerSample = 0.08202ms; SamplesPerSecond = 12191
 Epoch[ 1 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.39345703; EvalErr[0]PerSample = 0.42734375; TotalTime = 0.21312s; TotalTimePerSample = 0.08325ms; SamplesPerSecond = 12011
 Epoch[ 1 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.32485046; EvalErr[0]PerSample = 0.40156250; TotalTime = 0.21135s; TotalTimePerSample = 0.08256ms; SamplesPerSecond = 12112
 Epoch[ 1 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27032471; EvalErr[0]PerSample = 0.39765625; TotalTime = 0.21193s; TotalTimePerSample = 0.08279ms; SamplesPerSecond = 12079
 Epoch[ 1 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.32375488; EvalErr[0]PerSample = 0.39257813; TotalTime = 0.21210s; TotalTimePerSample = 0.08285ms; SamplesPerSecond = 12070
 Epoch[ 1 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.25393982; EvalErr[0]PerSample = 0.38320312; TotalTime = 0.21005s; TotalTimePerSample = 0.08205ms; SamplesPerSecond = 12187
 Epoch[ 1 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23377075; EvalErr[0]PerSample = 0.36953125; TotalTime = 0.21336s; TotalTimePerSample = 0.08334ms; SamplesPerSecond = 11998
 Epoch[ 1 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20861511; EvalErr[0]PerSample = 0.35976562; TotalTime = 0.21224s; TotalTimePerSample = 0.08291ms; SamplesPerSecond = 12061
 Epoch[ 1 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23675232; EvalErr[0]PerSample = 0.36757812; TotalTime = 0.21247s; TotalTimePerSample = 0.08300ms; SamplesPerSecond = 12048
 Epoch[ 1 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22960205; EvalErr[0]PerSample = 0.37460938; TotalTime = 0.18833s; TotalTimePerSample = 0.07357ms; SamplesPerSecond = 13593
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.6517237; EvalErrPerSample = 0.46774903; AvgLearningRatePerSample = 0.003125000047; EpochTime=10.440252
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21869726; EvalErr[0]PerSample = 0.36992188; TotalTime = 0.21394s; TotalTimePerSample = 0.08357ms; SamplesPerSecond = 11965
 Epoch[ 2 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18345709; EvalErr[0]PerSample = 0.36679688; TotalTime = 0.21228s; TotalTimePerSample = 0.08292ms; SamplesPerSecond = 12059
 Epoch[ 2 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17220440; EvalErr[0]PerSample = 0.35898438; TotalTime = 0.21039s; TotalTimePerSample = 0.08218ms; SamplesPerSecond = 12168
 Epoch[ 2 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20035286; EvalErr[0]PerSample = 0.35781250; TotalTime = 0.21065s; TotalTimePerSample = 0.08228ms; SamplesPerSecond = 12152
 Epoch[ 2 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19499779; EvalErr[0]PerSample = 0.37460938; TotalTime = 0.21058s; TotalTimePerSample = 0.08226ms; SamplesPerSecond = 12157
 Epoch[ 2 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16373482; EvalErr[0]PerSample = 0.34687500; TotalTime = 0.21231s; TotalTimePerSample = 0.08294ms; SamplesPerSecond = 12057
 Epoch[ 2 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13869247; EvalErr[0]PerSample = 0.34804687; TotalTime = 0.21096s; TotalTimePerSample = 0.08241ms; SamplesPerSecond = 12134
 Epoch[ 2 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19293823; EvalErr[0]PerSample = 0.36992188; TotalTime = 0.21336s; TotalTimePerSample = 0.08334ms; SamplesPerSecond = 11998
 Epoch[ 2 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23978348; EvalErr[0]PerSample = 0.37539062; TotalTime = 0.21206s; TotalTimePerSample = 0.08284ms; SamplesPerSecond = 12072
 Epoch[ 2 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18622742; EvalErr[0]PerSample = 0.36406250; TotalTime = 0.21355s; TotalTimePerSample = 0.08342ms; SamplesPerSecond = 11987
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16710815; EvalErr[0]PerSample = 0.35703125; TotalTime = 0.21215s; TotalTimePerSample = 0.08287ms; SamplesPerSecond = 12067
 Epoch[ 2 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.24683685; EvalErr[0]PerSample = 0.38554688; TotalTime = 0.21120s; TotalTimePerSample = 0.08250ms; SamplesPerSecond = 12121
 Epoch[ 2 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18601685; EvalErr[0]PerSample = 0.35273437; TotalTime = 0.21167s; TotalTimePerSample = 0.08268ms; SamplesPerSecond = 12094
 Epoch[ 2 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21721497; EvalErr[0]PerSample = 0.37617187; TotalTime = 0.21162s; TotalTimePerSample = 0.08266ms; SamplesPerSecond = 12097
 Epoch[ 2 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19934692; EvalErr[0]PerSample = 0.36953125; TotalTime = 0.21175s; TotalTimePerSample = 0.08272ms; SamplesPerSecond = 12089
 Epoch[ 2 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.15099945; EvalErr[0]PerSample = 0.34257813; TotalTime = 0.22467s; TotalTimePerSample = 0.08776ms; SamplesPerSecond = 11394
 Epoch[ 2 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14984589; EvalErr[0]PerSample = 0.35703125; TotalTime = 0.32181s; TotalTimePerSample = 0.12571ms; SamplesPerSecond = 7954
 Epoch[ 2 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19028320; EvalErr[0]PerSample = 0.35898438; TotalTime = 0.29716s; TotalTimePerSample = 0.11608ms; SamplesPerSecond = 8614
 Epoch[ 2 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16434784; EvalErr[0]PerSample = 0.36406250; TotalTime = 0.28060s; TotalTimePerSample = 0.10961ms; SamplesPerSecond = 9123
 Epoch[ 2 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08853760; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.26880s; TotalTimePerSample = 0.10500ms; SamplesPerSecond = 9523
 Epoch[ 2 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.15194244; EvalErr[0]PerSample = 0.35039063; TotalTime = 0.24705s; TotalTimePerSample = 0.09651ms; SamplesPerSecond = 10362
 Epoch[ 2 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16113434; EvalErr[0]PerSample = 0.35625000; TotalTime = 0.24688s; TotalTimePerSample = 0.09644ms; SamplesPerSecond = 10369
 Epoch[ 2 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18479004; EvalErr[0]PerSample = 0.36757812; TotalTime = 0.22797s; TotalTimePerSample = 0.08905ms; SamplesPerSecond = 11229
 Epoch[ 2 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14554138; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.22080s; TotalTimePerSample = 0.08625ms; SamplesPerSecond = 11594
 Epoch[ 2 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.15263367; EvalErr[0]PerSample = 0.35390625; TotalTime = 0.21885s; TotalTimePerSample = 0.08549ms; SamplesPerSecond = 11697
 Epoch[ 2 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08563538; EvalErr[0]PerSample = 0.33437500; TotalTime = 0.21488s; TotalTimePerSample = 0.08394ms; SamplesPerSecond = 11913
 Epoch[ 2 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10797424; EvalErr[0]PerSample = 0.34882812; TotalTime = 0.21251s; TotalTimePerSample = 0.08301ms; SamplesPerSecond = 12046
 Epoch[ 2 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.07031860; EvalErr[0]PerSample = 0.33593750; TotalTime = 0.21711s; TotalTimePerSample = 0.08481ms; SamplesPerSecond = 11791
 Epoch[ 2 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09429016; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.20782s; TotalTimePerSample = 0.08118ms; SamplesPerSecond = 12318
 Epoch[ 2 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14634094; EvalErr[0]PerSample = 0.35351563; TotalTime = 0.21410s; TotalTimePerSample = 0.08363ms; SamplesPerSecond = 11956
 Epoch[ 2 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10476990; EvalErr[0]PerSample = 0.34335938; TotalTime = 0.22753s; TotalTimePerSample = 0.08888ms; SamplesPerSecond = 11251
 Epoch[ 2 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.07355957; EvalErr[0]PerSample = 0.32695313; TotalTime = 0.18968s; TotalTimePerSample = 0.07409ms; SamplesPerSecond = 13496
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1603298; EvalErrPerSample = 0.35574952; AvgLearningRatePerSample = 0.003125000047; EpochTime=7.250721
CNTKCommandTrainEnd: DPT_Pre1


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[1, 1] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[132, 0])
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: DPT_Pre2
NDLBuilder Using GPU 0
reading script file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
htkmlfreader: reading MLF file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/Pre2/cntkSpeech.0.


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  4.49739113; EvalErr[0]PerSample = 0.80429688; TotalTime = 0.26983s; TotalTimePerSample = 0.10540ms; SamplesPerSecond = 9487
 Epoch[ 1 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.83226433; EvalErr[0]PerSample = 0.68125000; TotalTime = 0.24381s; TotalTimePerSample = 0.09524ms; SamplesPerSecond = 10499
 Epoch[ 1 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.25921097; EvalErr[0]PerSample = 0.59921875; TotalTime = 0.24409s; TotalTimePerSample = 0.09535ms; SamplesPerSecond = 10488
 Epoch[ 1 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.91240921; EvalErr[0]PerSample = 0.51210937; TotalTime = 0.24460s; TotalTimePerSample = 0.09555ms; SamplesPerSecond = 10465
 Epoch[ 1 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.69259949; EvalErr[0]PerSample = 0.46679688; TotalTime = 0.24347s; TotalTimePerSample = 0.09511ms; SamplesPerSecond = 10514
 Epoch[ 1 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.59069672; EvalErr[0]PerSample = 0.45312500; TotalTime = 0.24354s; TotalTimePerSample = 0.09513ms; SamplesPerSecond = 10511
 Epoch[ 1 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.48813324; EvalErr[0]PerSample = 0.43789062; TotalTime = 0.24421s; TotalTimePerSample = 0.09540ms; SamplesPerSecond = 10482
 Epoch[ 1 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.48960571; EvalErr[0]PerSample = 0.43515625; TotalTime = 0.32635s; TotalTimePerSample = 0.12748ms; SamplesPerSecond = 7844
 Epoch[ 1 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.45628204; EvalErr[0]PerSample = 0.42187500; TotalTime = 0.32522s; TotalTimePerSample = 0.12704ms; SamplesPerSecond = 7871
 Epoch[ 1 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41567383; EvalErr[0]PerSample = 0.40820313; TotalTime = 0.30454s; TotalTimePerSample = 0.11896ms; SamplesPerSecond = 8406
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.42048950; EvalErr[0]PerSample = 0.41406250; TotalTime = 0.29101s; TotalTimePerSample = 0.11368ms; SamplesPerSecond = 8796
 Epoch[ 1 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.34279480; EvalErr[0]PerSample = 0.39726563; TotalTime = 0.28014s; TotalTimePerSample = 0.10943ms; SamplesPerSecond = 9138
 Epoch[ 1 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.31633148; EvalErr[0]PerSample = 0.38789062; TotalTime = 0.26722s; TotalTimePerSample = 0.10438ms; SamplesPerSecond = 9580
 Epoch[ 1 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33296814; EvalErr[0]PerSample = 0.39804688; TotalTime = 0.25561s; TotalTimePerSample = 0.09985ms; SamplesPerSecond = 10015
 Epoch[ 1 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.32084351; EvalErr[0]PerSample = 0.39609375; TotalTime = 0.24941s; TotalTimePerSample = 0.09742ms; SamplesPerSecond = 10264
 Epoch[ 1 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27189636; EvalErr[0]PerSample = 0.38125000; TotalTime = 0.24707s; TotalTimePerSample = 0.09651ms; SamplesPerSecond = 10361
 Epoch[ 1 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.29380188; EvalErr[0]PerSample = 0.38554688; TotalTime = 0.24985s; TotalTimePerSample = 0.09760ms; SamplesPerSecond = 10246
 Epoch[ 1 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.31463013; EvalErr[0]PerSample = 0.38984375; TotalTime = 0.24803s; TotalTimePerSample = 0.09689ms; SamplesPerSecond = 10321
 Epoch[ 1 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33578796; EvalErr[0]PerSample = 0.40664062; TotalTime = 0.24792s; TotalTimePerSample = 0.09684ms; SamplesPerSecond = 10326
 Epoch[ 1 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.32202454; EvalErr[0]PerSample = 0.41484375; TotalTime = 0.24857s; TotalTimePerSample = 0.09710ms; SamplesPerSecond = 10298
 Epoch[ 1 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23669434; EvalErr[0]PerSample = 0.37460938; TotalTime = 0.24867s; TotalTimePerSample = 0.09714ms; SamplesPerSecond = 10294
 Epoch[ 1 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27109985; EvalErr[0]PerSample = 0.38906250; TotalTime = 0.24844s; TotalTimePerSample = 0.09705ms; SamplesPerSecond = 10304
 Epoch[ 1 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.26419678; EvalErr[0]PerSample = 0.37578125; TotalTime = 0.24982s; TotalTimePerSample = 0.09759ms; SamplesPerSecond = 10247
 Epoch[ 1 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23778992; EvalErr[0]PerSample = 0.37265625; TotalTime = 0.24921s; TotalTimePerSample = 0.09735ms; SamplesPerSecond = 10272
 Epoch[ 1 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21040344; EvalErr[0]PerSample = 0.36757812; TotalTime = 0.24707s; TotalTimePerSample = 0.09651ms; SamplesPerSecond = 10361
 Epoch[ 1 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18387146; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.24878s; TotalTimePerSample = 0.09718ms; SamplesPerSecond = 10290
 Epoch[ 1 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23827515; EvalErr[0]PerSample = 0.37148437; TotalTime = 0.24980s; TotalTimePerSample = 0.09758ms; SamplesPerSecond = 10248
 Epoch[ 1 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18418274; EvalErr[0]PerSample = 0.36328125; TotalTime = 0.24695s; TotalTimePerSample = 0.09647ms; SamplesPerSecond = 10366
 Epoch[ 1 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16851501; EvalErr[0]PerSample = 0.35234375; TotalTime = 0.24995s; TotalTimePerSample = 0.09764ms; SamplesPerSecond = 10242
 Epoch[ 1 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14337463; EvalErr[0]PerSample = 0.34375000; TotalTime = 0.24678s; TotalTimePerSample = 0.09640ms; SamplesPerSecond = 10373
 Epoch[ 1 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17227478; EvalErr[0]PerSample = 0.34882812; TotalTime = 0.24908s; TotalTimePerSample = 0.09730ms; SamplesPerSecond = 10277
 Epoch[ 1 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18431091; EvalErr[0]PerSample = 0.36835937; TotalTime = 0.22697s; TotalTimePerSample = 0.08866ms; SamplesPerSecond = 11278
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.5125258; EvalErrPerSample = 0.42452392; AvgLearningRatePerSample = 0.003125000047; EpochTime=11.751371
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17448177; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.24994s; TotalTimePerSample = 0.09763ms; SamplesPerSecond = 10242
 Epoch[ 2 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14536781; EvalErr[0]PerSample = 0.35664062; TotalTime = 0.24794s; TotalTimePerSample = 0.09685ms; SamplesPerSecond = 10324
 Epoch[ 2 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.15722904; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.24943s; TotalTimePerSample = 0.09743ms; SamplesPerSecond = 10263
 Epoch[ 2 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14344521; EvalErr[0]PerSample = 0.34804687; TotalTime = 0.24811s; TotalTimePerSample = 0.09692ms; SamplesPerSecond = 10318
 Epoch[ 2 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14842377; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.24918s; TotalTimePerSample = 0.09734ms; SamplesPerSecond = 10273
 Epoch[ 2 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14489059; EvalErr[0]PerSample = 0.34218750; TotalTime = 0.24994s; TotalTimePerSample = 0.09763ms; SamplesPerSecond = 10242
 Epoch[ 2 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09631195; EvalErr[0]PerSample = 0.33984375; TotalTime = 0.24887s; TotalTimePerSample = 0.09721ms; SamplesPerSecond = 10286
 Epoch[ 2 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16026917; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.24918s; TotalTimePerSample = 0.09734ms; SamplesPerSecond = 10273
 Epoch[ 2 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16528091; EvalErr[0]PerSample = 0.36015625; TotalTime = 0.24944s; TotalTimePerSample = 0.09744ms; SamplesPerSecond = 10262
 Epoch[ 2 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12257309; EvalErr[0]PerSample = 0.34492187; TotalTime = 0.24807s; TotalTimePerSample = 0.09690ms; SamplesPerSecond = 10319
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12313004; EvalErr[0]PerSample = 0.34765625; TotalTime = 0.24722s; TotalTimePerSample = 0.09657ms; SamplesPerSecond = 10355
 Epoch[ 2 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18492050; EvalErr[0]PerSample = 0.36171875; TotalTime = 0.24969s; TotalTimePerSample = 0.09753ms; SamplesPerSecond = 10252
 Epoch[ 2 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13058014; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.24860s; TotalTimePerSample = 0.09711ms; SamplesPerSecond = 10297
 Epoch[ 2 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16725922; EvalErr[0]PerSample = 0.35781250; TotalTime = 0.24710s; TotalTimePerSample = 0.09652ms; SamplesPerSecond = 10360
 Epoch[ 2 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12244568; EvalErr[0]PerSample = 0.34648438; TotalTime = 0.24695s; TotalTimePerSample = 0.09647ms; SamplesPerSecond = 10366
 Epoch[ 2 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09480591; EvalErr[0]PerSample = 0.33671875; TotalTime = 0.24847s; TotalTimePerSample = 0.09706ms; SamplesPerSecond = 10303
 Epoch[ 2 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11218109; EvalErr[0]PerSample = 0.34140625; TotalTime = 0.24865s; TotalTimePerSample = 0.09713ms; SamplesPerSecond = 10295
 Epoch[ 2 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11966095; EvalErr[0]PerSample = 0.33398438; TotalTime = 0.24891s; TotalTimePerSample = 0.09723ms; SamplesPerSecond = 10284
 Epoch[ 2 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10485687; EvalErr[0]PerSample = 0.33671875; TotalTime = 0.24762s; TotalTimePerSample = 0.09672ms; SamplesPerSecond = 10338
 Epoch[ 2 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06019897; EvalErr[0]PerSample = 0.32617188; TotalTime = 0.24990s; TotalTimePerSample = 0.09762ms; SamplesPerSecond = 10244
 Epoch[ 2 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10600891; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.24865s; TotalTimePerSample = 0.09713ms; SamplesPerSecond = 10295
 Epoch[ 2 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13724976; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.24982s; TotalTimePerSample = 0.09759ms; SamplesPerSecond = 10247
 Epoch[ 2 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12464752; EvalErr[0]PerSample = 0.34609375; TotalTime = 0.24959s; TotalTimePerSample = 0.09749ms; SamplesPerSecond = 10256
 Epoch[ 2 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10831604; EvalErr[0]PerSample = 0.33593750; TotalTime = 0.24961s; TotalTimePerSample = 0.09751ms; SamplesPerSecond = 10255
 Epoch[ 2 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09707031; EvalErr[0]PerSample = 0.34023437; TotalTime = 0.24871s; TotalTimePerSample = 0.09715ms; SamplesPerSecond = 10293
 Epoch[ 2 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.04812317; EvalErr[0]PerSample = 0.32773438; TotalTime = 0.24965s; TotalTimePerSample = 0.09752ms; SamplesPerSecond = 10254
 Epoch[ 2 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.04979248; EvalErr[0]PerSample = 0.33398438; TotalTime = 0.24888s; TotalTimePerSample = 0.09722ms; SamplesPerSecond = 10286
 Epoch[ 2 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.03223572; EvalErr[0]PerSample = 0.31835938; TotalTime = 0.24889s; TotalTimePerSample = 0.09722ms; SamplesPerSecond = 10285
 Epoch[ 2 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.05677490; EvalErr[0]PerSample = 0.32773438; TotalTime = 0.24869s; TotalTimePerSample = 0.09714ms; SamplesPerSecond = 10293
 Epoch[ 2 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10880737; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.24921s; TotalTimePerSample = 0.09735ms; SamplesPerSecond = 10272
 Epoch[ 2 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08513489; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.24894s; TotalTimePerSample = 0.09724ms; SamplesPerSecond = 10283
 Epoch[ 2 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.04244080; EvalErr[0]PerSample = 0.31757812; TotalTime = 0.22858s; TotalTimePerSample = 0.08929ms; SamplesPerSecond = 11199
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1148411; EvalErrPerSample = 0.34190676; AvgLearningRatePerSample = 0.003125000047; EpochTime=7.97598
CNTKCommandTrainEnd: DPT_Pre2


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[1, 1] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[132, 0])
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
htkmlfreader: reading MLF file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.0.


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 13 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  4.12455330; EvalErr[0]PerSample = 0.82734375; TotalTime = 0.32415s; TotalTimePerSample = 0.12662ms; SamplesPerSecond = 7897
 Epoch[ 1 of 4]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.55599785; EvalErr[0]PerSample = 0.63007813; TotalTime = 0.29259s; TotalTimePerSample = 0.11429ms; SamplesPerSecond = 8749
 Epoch[ 1 of 4]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.03516159; EvalErr[0]PerSample = 0.53945312; TotalTime = 0.27389s; TotalTimePerSample = 0.10699ms; SamplesPerSecond = 9346
 Epoch[ 1 of 4]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.73739853; EvalErr[0]PerSample = 0.47500000; TotalTime = 0.27591s; TotalTimePerSample = 0.10778ms; SamplesPerSecond = 9278
 Epoch[ 1 of 4]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.54207916; EvalErr[0]PerSample = 0.43515625; TotalTime = 0.27502s; TotalTimePerSample = 0.10743ms; SamplesPerSecond = 9308
 Epoch[ 1 of 4]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.44409790; EvalErr[0]PerSample = 0.41328125; TotalTime = 0.27480s; TotalTimePerSample = 0.10734ms; SamplesPerSecond = 9316
 Epoch[ 1 of 4]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.36059418; EvalErr[0]PerSample = 0.40898438; TotalTime = 0.27681s; TotalTimePerSample = 0.10813ms; SamplesPerSecond = 9248
 Epoch[ 1 of 4]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.35930023; EvalErr[0]PerSample = 0.40117188; TotalTime = 0.27712s; TotalTimePerSample = 0.10825ms; SamplesPerSecond = 9237
 Epoch[ 1 of 4]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.34254303; EvalErr[0]PerSample = 0.38632813; TotalTime = 0.27796s; TotalTimePerSample = 0.10858ms; SamplesPerSecond = 9210
 Epoch[ 1 of 4]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.30505676; EvalErr[0]PerSample = 0.38320312; TotalTime = 0.27674s; TotalTimePerSample = 0.10810ms; SamplesPerSecond = 9250
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 4]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.30881348; EvalErr[0]PerSample = 0.38476563; TotalTime = 0.27577s; TotalTimePerSample = 0.10772ms; SamplesPerSecond = 9283
 Epoch[ 1 of 4]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23755188; EvalErr[0]PerSample = 0.37304688; TotalTime = 0.27790s; TotalTimePerSample = 0.10855ms; SamplesPerSecond = 9212
 Epoch[ 1 of 4]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21070251; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.27583s; TotalTimePerSample = 0.10774ms; SamplesPerSecond = 9281
 Epoch[ 1 of 4]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.24008789; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.27513s; TotalTimePerSample = 0.10747ms; SamplesPerSecond = 9304
 Epoch[ 1 of 4]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23422089; EvalErr[0]PerSample = 0.36835937; TotalTime = 0.27838s; TotalTimePerSample = 0.10874ms; SamplesPerSecond = 9195
 Epoch[ 1 of 4]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19425964; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.27700s; TotalTimePerSample = 0.10820ms; SamplesPerSecond = 9241
 Epoch[ 1 of 4]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21415710; EvalErr[0]PerSample = 0.36289063; TotalTime = 0.27623s; TotalTimePerSample = 0.10790ms; SamplesPerSecond = 9267
 Epoch[ 1 of 4]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.24289856; EvalErr[0]PerSample = 0.37031250; TotalTime = 0.27798s; TotalTimePerSample = 0.10859ms; SamplesPerSecond = 9209
 Epoch[ 1 of 4]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.26465454; EvalErr[0]PerSample = 0.38359375; TotalTime = 0.27969s; TotalTimePerSample = 0.10926ms; SamplesPerSecond = 9152
 Epoch[ 1 of 4]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22050476; EvalErr[0]PerSample = 0.38085938; TotalTime = 0.27653s; TotalTimePerSample = 0.10802ms; SamplesPerSecond = 9257
 Epoch[ 1 of 4]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17745056; EvalErr[0]PerSample = 0.35507813; TotalTime = 0.27754s; TotalTimePerSample = 0.10841ms; SamplesPerSecond = 9223
 Epoch[ 1 of 4]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19851379; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.27840s; TotalTimePerSample = 0.10875ms; SamplesPerSecond = 9195
 Epoch[ 1 of 4]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21453857; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.28119s; TotalTimePerSample = 0.10984ms; SamplesPerSecond = 9104
 Epoch[ 1 of 4]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18011475; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.27824s; TotalTimePerSample = 0.10869ms; SamplesPerSecond = 9200
 Epoch[ 1 of 4]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16693726; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.27669s; TotalTimePerSample = 0.10808ms; SamplesPerSecond = 9252
 Epoch[ 1 of 4]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12398987; EvalErr[0]PerSample = 0.35234375; TotalTime = 0.27729s; TotalTimePerSample = 0.10832ms; SamplesPerSecond = 9232
 Epoch[ 1 of 4]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18822021; EvalErr[0]PerSample = 0.36328125; TotalTime = 0.27797s; TotalTimePerSample = 0.10858ms; SamplesPerSecond = 9209
 Epoch[ 1 of 4]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13831482; EvalErr[0]PerSample = 0.35078125; TotalTime = 0.27752s; TotalTimePerSample = 0.10840ms; SamplesPerSecond = 9224
 Epoch[ 1 of 4]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12718811; EvalErr[0]PerSample = 0.33984375; TotalTime = 0.27547s; TotalTimePerSample = 0.10761ms; SamplesPerSecond = 9293
 Epoch[ 1 of 4]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11155701; EvalErr[0]PerSample = 0.34179688; TotalTime = 0.27706s; TotalTimePerSample = 0.10823ms; SamplesPerSecond = 9239
 Epoch[ 1 of 4]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13423157; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.27756s; TotalTimePerSample = 0.10842ms; SamplesPerSecond = 9223
 Epoch[ 1 of 4]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12716675; EvalErr[0]PerSample = 0.34414062; TotalTime = 0.25874s; TotalTimePerSample = 0.10107ms; SamplesPerSecond = 9893
Finished Epoch[ 1 of 4]: [Training Set] TrainLossPerSample = 1.4082143; EvalErrPerSample = 0.4008545; AvgLearningRatePerSample = 0.003125000047; EpochTime=12.48899
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.20089607; EvalErr[0]PerSample = 0.36757812; TotalTime = 0.61756s; TotalTimePerSample = 0.12062ms; SamplesPerSecond = 8290
 Epoch[ 2 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.15295639; EvalErr[0]PerSample = 0.34550781; TotalTime = 0.52375s; TotalTimePerSample = 0.10230ms; SamplesPerSecond = 9775
 Epoch[ 2 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.09945831; EvalErr[0]PerSample = 0.33613281; TotalTime = 0.48134s; TotalTimePerSample = 0.09401ms; SamplesPerSecond = 10637
 Epoch[ 2 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.09916496; EvalErr[0]PerSample = 0.33867188; TotalTime = 0.44879s; TotalTimePerSample = 0.08765ms; SamplesPerSecond = 11408
 Epoch[ 2 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.17260475; EvalErr[0]PerSample = 0.36230469; TotalTime = 0.43010s; TotalTimePerSample = 0.08400ms; SamplesPerSecond = 11904
 Epoch[ 2 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.15717964; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.43205s; TotalTimePerSample = 0.08439ms; SamplesPerSecond = 11850
 Epoch[ 2 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.14431229; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.43042s; TotalTimePerSample = 0.08407ms; SamplesPerSecond = 11895
 Epoch[ 2 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.10515747; EvalErr[0]PerSample = 0.34394531; TotalTime = 0.43067s; TotalTimePerSample = 0.08411ms; SamplesPerSecond = 11888
 Epoch[ 2 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.15175400; EvalErr[0]PerSample = 0.35449219; TotalTime = 0.43096s; TotalTimePerSample = 0.08417ms; SamplesPerSecond = 11880
 Epoch[ 2 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11654053; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.43163s; TotalTimePerSample = 0.08430ms; SamplesPerSecond = 11861
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11851807; EvalErr[0]PerSample = 0.34472656; TotalTime = 0.43148s; TotalTimePerSample = 0.08427ms; SamplesPerSecond = 11866
 Epoch[ 2 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11374054; EvalErr[0]PerSample = 0.34492187; TotalTime = 0.43055s; TotalTimePerSample = 0.08409ms; SamplesPerSecond = 11891
 Epoch[ 2 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04686737; EvalErr[0]PerSample = 0.32265625; TotalTime = 0.43254s; TotalTimePerSample = 0.08448ms; SamplesPerSecond = 11837
 Epoch[ 2 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02721252; EvalErr[0]PerSample = 0.32246094; TotalTime = 0.43153s; TotalTimePerSample = 0.08428ms; SamplesPerSecond = 11864
 Epoch[ 2 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08386230; EvalErr[0]PerSample = 0.33144531; TotalTime = 0.43223s; TotalTimePerSample = 0.08442ms; SamplesPerSecond = 11845
 Epoch[ 2 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06164856; EvalErr[0]PerSample = 0.32558594; TotalTime = 0.39020s; TotalTimePerSample = 0.07621ms; SamplesPerSecond = 13121
Finished Epoch[ 2 of 4]: [Training Set] TrainLossPerSample = 1.1157421; EvalErrPerSample = 0.34266359; AvgLearningRatePerSample = 0.003125000047; EpochTime=7.29819
Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.12331724; EvalErr[0]PerSample = 0.34121094; TotalTime = 0.43712s; TotalTimePerSample = 0.08538ms; SamplesPerSecond = 11712
 Epoch[ 3 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.07871103; EvalErr[0]PerSample = 0.33652344; TotalTime = 0.43249s; TotalTimePerSample = 0.08447ms; SamplesPerSecond = 11838
 Epoch[ 3 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06784973; EvalErr[0]PerSample = 0.33183594; TotalTime = 0.43036s; TotalTimePerSample = 0.08406ms; SamplesPerSecond = 11896
 Epoch[ 3 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08440666; EvalErr[0]PerSample = 0.33398438; TotalTime = 0.43342s; TotalTimePerSample = 0.08465ms; SamplesPerSecond = 11812
 Epoch[ 3 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.07466774; EvalErr[0]PerSample = 0.33320312; TotalTime = 0.43556s; TotalTimePerSample = 0.08507ms; SamplesPerSecond = 11755
 Epoch[ 3 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05427513; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.43089s; TotalTimePerSample = 0.08416ms; SamplesPerSecond = 11882
 Epoch[ 3 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06873093; EvalErr[0]PerSample = 0.32773438; TotalTime = 0.43195s; TotalTimePerSample = 0.08436ms; SamplesPerSecond = 11853
 Epoch[ 3 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08097610; EvalErr[0]PerSample = 0.33007813; TotalTime = 0.43112s; TotalTimePerSample = 0.08420ms; SamplesPerSecond = 11875
 Epoch[ 3 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05431290; EvalErr[0]PerSample = 0.32792969; TotalTime = 0.43103s; TotalTimePerSample = 0.08419ms; SamplesPerSecond = 11878
 Epoch[ 3 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06173096; EvalErr[0]PerSample = 0.32695313; TotalTime = 0.43234s; TotalTimePerSample = 0.08444ms; SamplesPerSecond = 11842
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 3 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04505692; EvalErr[0]PerSample = 0.32792969; TotalTime = 0.43066s; TotalTimePerSample = 0.08411ms; SamplesPerSecond = 11888
 Epoch[ 3 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08151245; EvalErr[0]PerSample = 0.33574219; TotalTime = 0.43359s; TotalTimePerSample = 0.08469ms; SamplesPerSecond = 11808
 Epoch[ 3 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.10628204; EvalErr[0]PerSample = 0.33437500; TotalTime = 0.43099s; TotalTimePerSample = 0.08418ms; SamplesPerSecond = 11879
 Epoch[ 3 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05827026; EvalErr[0]PerSample = 0.32636719; TotalTime = 0.43373s; TotalTimePerSample = 0.08471ms; SamplesPerSecond = 11804
 Epoch[ 3 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05841064; EvalErr[0]PerSample = 0.33574219; TotalTime = 0.43182s; TotalTimePerSample = 0.08434ms; SamplesPerSecond = 11856
 Epoch[ 3 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04437714; EvalErr[0]PerSample = 0.32773438; TotalTime = 0.39095s; TotalTimePerSample = 0.07636ms; SamplesPerSecond = 13096
Finished Epoch[ 3 of 4]: [Training Set] TrainLossPerSample = 1.0714306; EvalErrPerSample = 0.33178711; AvgLearningRatePerSample = 0.003125000047; EpochTime=6.933627
Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04450397; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.43157s; TotalTimePerSample = 0.08429ms; SamplesPerSecond = 11863
 Epoch[ 4 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 4926; TrainLossPerSample =  1.02895867; EvalErr[0]PerSample = 0.31567194; TotalTime = 1.24307s; TotalTimePerSample = 0.25235ms; SamplesPerSecond = 3962
 Epoch[ 4 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00198059; EvalErr[0]PerSample = 0.31601563; TotalTime = 0.43062s; TotalTimePerSample = 0.08410ms; SamplesPerSecond = 11889
 Epoch[ 4 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00561543; EvalErr[0]PerSample = 0.31777344; TotalTime = 0.43180s; TotalTimePerSample = 0.08434ms; SamplesPerSecond = 11857
 Epoch[ 4 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00148926; EvalErr[0]PerSample = 0.31601563; TotalTime = 0.43056s; TotalTimePerSample = 0.08409ms; SamplesPerSecond = 11891
 Epoch[ 4 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00593376; EvalErr[0]PerSample = 0.31406250; TotalTime = 0.43000s; TotalTimePerSample = 0.08398ms; SamplesPerSecond = 11906
 Epoch[ 4 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.98752327; EvalErr[0]PerSample = 0.30722656; TotalTime = 0.43154s; TotalTimePerSample = 0.08429ms; SamplesPerSecond = 11864
 Epoch[ 4 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.01428757; EvalErr[0]PerSample = 0.31992188; TotalTime = 0.43123s; TotalTimePerSample = 0.08422ms; SamplesPerSecond = 11873
 Epoch[ 4 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99691544; EvalErr[0]PerSample = 0.31621094; TotalTime = 0.43344s; TotalTimePerSample = 0.08466ms; SamplesPerSecond = 11812
 Epoch[ 4 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.96604996; EvalErr[0]PerSample = 0.30937500; TotalTime = 0.43112s; TotalTimePerSample = 0.08420ms; SamplesPerSecond = 11875
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 4 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99062958; EvalErr[0]PerSample = 0.30527344; TotalTime = 0.43286s; TotalTimePerSample = 0.08454ms; SamplesPerSecond = 11828
 Epoch[ 4 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99886856; EvalErr[0]PerSample = 0.30976562; TotalTime = 0.43124s; TotalTimePerSample = 0.08423ms; SamplesPerSecond = 11872
 Epoch[ 4 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00958328; EvalErr[0]PerSample = 0.31523438; TotalTime = 0.43323s; TotalTimePerSample = 0.08462ms; SamplesPerSecond = 11818
 Epoch[ 4 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.97942047; EvalErr[0]PerSample = 0.31171875; TotalTime = 0.43291s; TotalTimePerSample = 0.08455ms; SamplesPerSecond = 11826
 Epoch[ 4 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.94226837; EvalErr[0]PerSample = 0.30136719; TotalTime = 0.43127s; TotalTimePerSample = 0.08423ms; SamplesPerSecond = 11871
 Epoch[ 4 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.96711578; EvalErr[0]PerSample = 0.30175781; TotalTime = 0.40390s; TotalTimePerSample = 0.07889ms; SamplesPerSecond = 12676
Finished Epoch[ 4 of 4]: [Training Set] TrainLossPerSample = 0.99611807; EvalErrPerSample = 0.31303713; AvgLearningRatePerSample = 0.003125000047; EpochTime=7.762103
CNTKCommandTrainEnd: speechTrain


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 13 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[0, 0] = SequenceWithSoftmax(labels[132, 0], OL.z[132, 0], ScaledLogLikelihood[132, 0])
ScaledLogLikelihood[132, 0] = Minus(OL.z[132, 0], logPrior[132, 1])
logPrior[132, 1] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[512, 0])
HL3.y[512, 0] = Sigmoid(HL3.z[512, 0])
HL3.z[512, 0] = Plus(HL3.t[512, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[512, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[1, 1] = SequenceWithSoftmax(labels[132, 0], OL.z[132, 0], ScaledLogLikelihood[132, 0])
ScaledLogLikelihood[132, 0] = Minus(OL.z[132, 0], logPrior[132, 1])
logPrior[132, 1] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[512, 0])
HL3.y[512, 0] = Sigmoid(HL3.z[512, 0])
HL3.z[512, 0] = Plus(HL3.t[512, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[512, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[1, 1] = SequenceWithSoftmax(labels[132, 0], OL.z[132, 0], ScaledLogLikelihood[132, 0])
ScaledLogLikelihood[132, 0] = Minus(OL.z[132, 0], logPrior[132, 1])
logPrior[132, 1] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[512, 0])
HL3.y[512, 0] = Sigmoid(HL3.z[512, 0])
HL3.z[512, 0] = Plus(HL3.t[512, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[512, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading 'E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/model.overalltying', 'E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list', 'E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
htkmlfreader: reading MLF file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data\CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames
Starting from checkpoint. Load Network From File C:\cygwin64\tmp\cntk-test-20151118135231.525296\Speech\DNN_SequenceTraining@debug_gpu/models/cntkSpeech.sequence.0.


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

SEwithSoftmax[0, 0] = SequenceWithSoftmax(labels[132, 0], OL.z[0, 0], ScaledLogLikelihood[0, 0])
ScaledLogLikelihood[0, 0] = Minus(OL.z[0, 0], logPrior[0, 0])
logPrior[0, 0] = Log(GlobalPrior[132, 1])
GlobalPrior[132, 1] = LearnableParameter
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 15 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node SEwithSoftmax. 28 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

Validating for node SEwithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]
Validating --> SEwithSoftmax = SequenceWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0], ScaledLogLikelihood[132, MBSize 0]) -> [1, 1]

13 out of 28 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-010
Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms
getcachedidmap: reading 'CY2SCH100112501_1779035656.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060091323_1311139777.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050080342_1705448690.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040081222_2063295388.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060050506_415705324.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050070531_1705536174.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060032122_1764296812.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050081506_1705673846.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080062603_966497326.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040081012_2063351544.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060062605_1536194576.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110070817_767763687.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110070803_1061569312.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090121023_160495828.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110071501_1059265218.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050121418_1697698502.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080071106_1143001388.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH020071229_644244140.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070062507_2087712345.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060082605_403727641.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050121330_1705081299.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040121540_2063741403.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080110609_1362153092.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH110121120_1050992515.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070101111_1071091997.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070081604_866394296.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH010061231_1369712653.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060121820_1476601534.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050111321_1705093424.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040120646_805508469.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080071106_1143018888.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040080404_2063355153.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH100092303_1769985203.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050051513_1110722876.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH010080244_1211145048.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090100302_1080239467.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070081708_1067766888.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040030304_1783125393.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070061608_1068884372.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090052313_1116838701.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH030121606_1629053528.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080061318_1080287029.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070061523_1068809263.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040051114_2063220700.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040080519_2063294856.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060042707_1939966343.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080041801_1024690842.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090081411_995585687.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH080040406_982069998.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050111011_1704954393.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090090404_1123576357.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070060121_1068947138.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040121221_2063738606.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070111908_1069919341.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050120821_1705096268.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050091619_1705363705.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040070546_2063363669.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040090837_1205589951.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070040324_1092265200.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050090827_1393311596.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070110213_1069879356.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050051308_1443335642.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040120414_2063653575.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060062620_1679599206.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070032203_1093924684.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH010061544_691164888.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH090090208_1064021467.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040011112_1027256127.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH030061032_1628532606.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH100092119_1769162281.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH070080320_1067791763.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050090541_1705321237.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060050603_416427387.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050110617_547221844.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040031340_1350166299.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH040121314_2063679450.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH050120722_1705080393.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH100090709_1765693687.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060060407_1233548343.numden.lats.symlist'
getcachedidmap: reading 'CY2SCH060091920_1713759019.numden.lats.symlist'

Starting minibatch loop.
copyhmms: 1.196350 ms
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.015387
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.051343
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.072787
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.063094
backpointers: 41.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.076285
backpointers: 29.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.095859
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 0.976427
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.196782
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.015319
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.105372
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.034096
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.155336
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.050430
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.030251
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.107270
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.037082
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.125835
backpointers: 41.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.042350
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.060958
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 1.064541
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.000843
 Epoch[ 1 of 3]-Minibatch[   1-  10 of 8192]: SamplesSeen = 4628; TrainLossPerSample =  0.08358561; EvalErr[0]PerSample = 0.33059637; TotalTime = 36.82927s; TotalTimePerSample = 7.95792ms; SamplesPerSecond = 125
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.099889
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.039597
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.115739
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 0.957857
backpointers: 23.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.122265
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.111263
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.065039
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.066569
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.987920
backpointers: 41.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.044044
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.004607
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.062701
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.035316
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.026637
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.144678
backpointers: 28.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.165378
backpointers: 26.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.132944
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.081424
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.108471
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 161 launches for forward, 161 launches for backward
dengamma value 1.075180
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.088663
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.078510
 Epoch[ 1 of 3]-Minibatch[  11-  20 of 8192]: SamplesSeen = 5946; TrainLossPerSample =  0.08214612; EvalErr[0]PerSample = 0.31449714; TotalTime = 2.62019s; TotalTimePerSample = 0.44066ms; SamplesPerSecond = 2269
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.048358
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.103563
backpointers: 22.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.099066
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.136579
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.077474
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.060762
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.101553
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.107761
backpointers: 40.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.022390
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.081607
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.010834
backpointers: 44.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.077299
backpointers: 43.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.083294
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.126290
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.066091
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.078891
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.046375
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 0.986374
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.084432
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.116274
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.091088
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.074867
 Epoch[ 1 of 3]-Minibatch[  21-  30 of 8192]: SamplesSeen = 5916; TrainLossPerSample =  0.08367419; EvalErr[0]PerSample = 0.32437458; TotalTime = 2.19803s; TotalTimePerSample = 0.37154ms; SamplesPerSecond = 2691
backpointers: 24.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.030191
backpointers: 38.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.047688
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.041505
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 124 launches for forward, 124 launches for backward
dengamma value 1.137154
backpointers: 26.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.101086
backpointers: 38.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.032440
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.077590
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.102024
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 106 launches for forward, 106 launches for backward
dengamma value 1.104773
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.157433
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.050714
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.003643
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.107600
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.107183
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.954740
backpointers: 40.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.009407
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.066747
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.065564
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.048961
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.026667
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.096718
backpointers: 38.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.068000
 Epoch[ 1 of 3]-Minibatch[  31-  40 of 8192]: SamplesSeen = 6386; TrainLossPerSample =  0.08031854; EvalErr[0]PerSample = 0.32680864; TotalTime = 2.25731s; TotalTimePerSample = 0.35348ms; SamplesPerSecond = 2829
backpointers: 32.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.044291
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.082913
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 111 launches for forward, 111 launches for backward
dengamma value 1.073895
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.133403
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.078159
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.102889
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.082325
backpointers: 38.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.099348
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.167385
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.058515
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.078929
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.050607
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.091514
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.052261
backpointers: 32.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 98 launches for forward, 98 launches for backward
dengamma value 1.109094
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 161 launches for forward, 161 launches for backward
dengamma value 1.121752
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.057149
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.042336
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 101 launches for forward, 101 launches for backward
dengamma value 1.051527
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 0.977551
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.104787
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.159152
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.068097
 Epoch[ 1 of 3]-Minibatch[  41-  50 of 8192]: SamplesSeen = 6734; TrainLossPerSample =  0.08171632; EvalErr[0]PerSample = 0.29551530; TotalTime = 2.45227s; TotalTimePerSample = 0.36416ms; SamplesPerSecond = 2746
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 129 launches for forward, 129 launches for backward
dengamma value 1.086780
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.130269
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.073455
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.091998
backpointers: 40.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.035754
backpointers: 36.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 0.985550
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.059557
backpointers: 24.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 114 launches for forward, 114 launches for backward
dengamma value 1.123901
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.083224
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.059212
backpointers: 26.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.097844
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.036471
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.081966
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 0.984412
backpointers: 38.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.105340
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.047261
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.033234
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.141741
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.066171
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.077317
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.127820
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.077791
backpointers: 33.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.080292
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 0.981080
 Epoch[ 1 of 3]-Minibatch[  51-  60 of 8192]: SamplesSeen = 6202; TrainLossPerSample =  0.08549173; EvalErr[0]PerSample = 0.31360851; TotalTime = 2.35095s; TotalTimePerSample = 0.37906ms; SamplesPerSecond = 2638
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.130581
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.117521
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.055970
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 99 launches for forward, 99 launches for backward
dengamma value 1.068845
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.151987
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.030249
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.106854
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.101331
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.060793
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.107020
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.105107
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.079484
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.108909
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.004101
backpointers: 40.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 0.950285
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 89 launches for forward, 89 launches for backward
dengamma value 1.125628
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.085928
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.042281
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.065240
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.049562
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.073483
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.089740
backpointers: 39.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.053381
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.018081
 Epoch[ 1 of 3]-Minibatch[  61-  70 of 8192]: SamplesSeen = 6362; TrainLossPerSample =  0.07997486; EvalErr[0]PerSample = 0.32568375; TotalTime = 2.31597s; TotalTimePerSample = 0.36403ms; SamplesPerSecond = 2747
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.075601
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.083481
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 99 launches for forward, 99 launches for backward
dengamma value 1.123550
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.028559
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.061858
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.100839
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.023886
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.043098
backpointers: 33.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.025118
backpointers: 24.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.136169
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.116578
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.054926
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.064200
backpointers: 45.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.068297
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 105 launches for forward, 105 launches for backward
dengamma value 1.059706
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.095217
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.115496
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.135736
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.060187
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.092560
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.066627
 Epoch[ 1 of 3]-Minibatch[  71-  80 of 8192]: SamplesSeen = 5608; TrainLossPerSample =  0.07502958; EvalErr[0]PerSample = 0.31847361; TotalTime = 1.99358s; TotalTimePerSample = 0.35549ms; SamplesPerSecond = 2813
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.063055
backpointers: 38.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.042724
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 141 launches for forward, 141 launches for backward
dengamma value 1.059763
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.021321
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.097744
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.028584
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.103969
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.997650
backpointers: 29.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.112675
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.092727
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.043912
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.129032
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.116614
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.042492
backpointers: 24.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 121 launches for forward, 121 launches for backward
dengamma value 1.061597
backpointers: 39.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.015698
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.053582
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.033995
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 129 launches for forward, 129 launches for backward
dengamma value 1.093077
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.080366
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.104077
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.044763
backpointers: 26.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.020228
 Epoch[ 1 of 3]-Minibatch[  81-  90 of 8192]: SamplesSeen = 6594; TrainLossPerSample =  0.08151794; EvalErr[0]PerSample = 0.33651805; TotalTime = 2.73963s; TotalTimePerSample = 0.41547ms; SamplesPerSecond = 2406
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.028670
backpointers: 46.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.886836
backpointers: 44.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.157957
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.014026
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.116295
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.039071
backpointers: 28.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.099671
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 120 launches for forward, 120 launches for backward
dengamma value 1.101143
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.113298
backpointers: 26.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.109584
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 118 launches for forward, 118 launches for backward
dengamma value 1.079916
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.063747
backpointers: 25.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.095497
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.076887
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.141736
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.035007
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.117461
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.135798
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.057881
backpointers: 27.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.067153
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.072834
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.006985
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.110780
 Epoch[ 1 of 3]-Minibatch[  91- 100 of 8192]: SamplesSeen = 6364; TrainLossPerSample =  0.08032955; EvalErr[0]PerSample = 0.31615336; TotalTime = 2.55502s; TotalTimePerSample = 0.40148ms; SamplesPerSecond = 2490
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 110 launches for forward, 110 launches for backward
dengamma value 1.061657
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.052374
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 0.987764
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.062911
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.061221
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.142910
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.130698
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.082887
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.044966
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.143583
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.073300
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.157146
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.095505
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.008495
backpointers: 27.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.116105
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.059661
backpointers: 38.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.161594
backpointers: 26.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.098735
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.146980
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.140211
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.096159
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.051981
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.046266
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 170 launches for forward, 170 launches for backward
dengamma value 1.118023
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.042709
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.001388
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.012911
 Epoch[ 1 of 3]-Minibatch[ 101- 110 of 8192]: SamplesSeen = 6536; TrainLossPerSample =  0.08613263; EvalErr[0]PerSample = 0.31548348; TotalTime = 2.34864s; TotalTimePerSample = 0.35934ms; SamplesPerSecond = 2782
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.087017
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.059792
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.046501
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.073579
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.095312
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.055991
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.085752
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.068742
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.091071
backpointers: 41.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.032232
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.084744
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.105438
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.041391
backpointers: 36.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 0.937120
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.102653
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.053025
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.061745
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.016491
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.044234
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.025076
backpointers: 32.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 88 launches for forward, 88 launches for backward
dengamma value 1.032975
 Epoch[ 1 of 3]-Minibatch[ 111- 120 of 8192]: SamplesSeen = 6208; TrainLossPerSample =  0.08295637; EvalErr[0]PerSample = 0.31765464; TotalTime = 2.28511s; TotalTimePerSample = 0.36809ms; SamplesPerSecond = 2716
backpointers: 41.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.085692
backpointers: 27.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.075794
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.118478
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.067378
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 0.945208
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.050391
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.078381
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.099215
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.086438
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 103 launches for forward, 103 launches for backward
dengamma value 1.049932
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.063138
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 108 launches for forward, 108 launches for backward
dengamma value 1.130924
backpointers: 39.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.028988
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 130 launches for forward, 130 launches for backward
dengamma value 1.191101
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.027579
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.088230
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.089141
backpointers: 26.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.083345
backpointers: 29.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.072232
backpointers: 24.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.090651
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.135706
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.127724
 Epoch[ 1 of 3]-Minibatch[ 121- 130 of 8192]: SamplesSeen = 6326; TrainLossPerSample =  0.08117240; EvalErr[0]PerSample = 0.29687006; TotalTime = 2.28708s; TotalTimePerSample = 0.36154ms; SamplesPerSecond = 2765
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.085506
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.071655
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.116561
backpointers: 22.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 168 launches for forward, 168 launches for backward
dengamma value 1.135969
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.003626
backpointers: 40.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.027538
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.104102
Finished Epoch[ 1 of 3]: [Training Set] TrainLossPerSample = 0.081567034; EvalErrPerSample = 0.31603935; AvgLearningRatePerSample = 1.999999995e-006; EpochTime=202.07524
Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81936), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.145242
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.085012
backpointers: 29.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.160179
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 132 launches for forward, 132 launches for backward
dengamma value 1.053052
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.116928
backpointers: 39.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.056254
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.095588
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.143948
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.044307
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 9 launches for forward, 9 launches for backward
dengamma value 1.043959
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.020891
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.092130
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.081158
backpointers: 25.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.091468
backpointers: 25.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.055750
backpointers: 39.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.017632
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.057541
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 98 launches for forward, 98 launches for backward
dengamma value 1.110858
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.063133
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.071745
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.083208
backpointers: 24.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 158 launches for forward, 158 launches for backward
dengamma value 1.134414
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.035728
backpointers: 24.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.045233
 Epoch[ 2 of 3]-Minibatch[   1-  10 of 8192]: SamplesSeen = 6182; TrainLossPerSample =  0.08368398; EvalErr[0]PerSample = 0.30071174; TotalTime = 2.72346s; TotalTimePerSample = 0.44055ms; SamplesPerSecond = 2269
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.046536
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.004586
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.288806
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.035090
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.095160
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 1.119750
backpointers: 41.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.107882
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.040208
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.045552
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.080889
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.044524
backpointers: 22.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.125896
backpointers: 20.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 125 launches for forward, 125 launches for backward
dengamma value 1.135680
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.026725
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.036926
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.092974
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.024887
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.115769
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.044357
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.026637
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.048265
backpointers: 26.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.113920
 Epoch[ 2 of 3]-Minibatch[  11-  20 of 8192]: SamplesSeen = 5736; TrainLossPerSample =  0.08501333; EvalErr[0]PerSample = 0.28417015; TotalTime = 2.21186s; TotalTimePerSample = 0.38561ms; SamplesPerSecond = 2593
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.010748
backpointers: 26.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.110666
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.085324
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.081510
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.057740
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.163440
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.024172
backpointers: 21.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 126 launches for forward, 126 launches for backward
dengamma value 1.126877
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.054233
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.054708
backpointers: 39.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.047825
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.056014
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.107508
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.188774
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.061890
backpointers: 23.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 175 launches for forward, 175 launches for backward
dengamma value 1.168044
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.112633
backpointers: 22.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.068623
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.072696
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.098664
backpointers: 43.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.974260
backpointers: 36.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 0.964481
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.066706
 Epoch[ 2 of 3]-Minibatch[  21-  30 of 8192]: SamplesSeen = 6244; TrainLossPerSample =  0.08160518; EvalErr[0]PerSample = 0.30172966; TotalTime = 2.39364s; TotalTimePerSample = 0.38335ms; SamplesPerSecond = 2608
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.105943
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.116022
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.088509
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.058934
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.161254
backpointers: 44.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.097798
backpointers: 41.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.077121
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 142 launches for forward, 142 launches for backward
dengamma value 1.044863
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.070741
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.016574
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.093447
backpointers: 26.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.126311
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.119959
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.129718
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.017900
backpointers: 23.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 137 launches for forward, 137 launches for backward
dengamma value 1.132437
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.024505
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.020736
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.078649
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.095957
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.025882
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.065037
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.176505
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.076837
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.056696
 Epoch[ 2 of 3]-Minibatch[  31-  40 of 8192]: SamplesSeen = 6280; TrainLossPerSample =  0.08490154; EvalErr[0]PerSample = 0.31242038; TotalTime = 2.21168s; TotalTimePerSample = 0.35218ms; SamplesPerSecond = 2839
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.018347
backpointers: 38.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.082831
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.075497
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.085046
backpointers: 38.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.063649
backpointers: 25.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.074264
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.067420
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.053149
backpointers: 20.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 1.117731
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.072480
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.076274
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.046603
backpointers: 20.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.115489
backpointers: 30.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.067307
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.103777
backpointers: 21.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.085866
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.144046
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.088565
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 141 launches for forward, 141 launches for backward
dengamma value 1.087098
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.063796
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.100174
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 105 launches for forward, 105 launches for backward
dengamma value 1.084196
backpointers: 23.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.102490
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.084687
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.092789
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 113 launches for forward, 113 launches for backward
dengamma value 1.103272
 Epoch[ 2 of 3]-Minibatch[  41-  50 of 8192]: SamplesSeen = 7428; TrainLossPerSample =  0.07733347; EvalErr[0]PerSample = 0.29806139; TotalTime = 2.65324s; TotalTimePerSample = 0.35719ms; SamplesPerSecond = 2799
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 117 launches for forward, 117 launches for backward
dengamma value 1.142144
backpointers: 25.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.081073
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.051410
backpointers: 24.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.134920
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.054180
backpointers: 42.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.033419
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.037870
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.091124
backpointers: 39.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.059701
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 97 launches for forward, 97 launches for backward
dengamma value 1.061054
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 108 launches for forward, 108 launches for backward
dengamma value 1.035654
backpointers: 39.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.057788
backpointers: 21.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.026473
backpointers: 42.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 10 launches for forward, 10 launches for backward
dengamma value 1.151943
backpointers: 27.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 103 launches for forward, 103 launches for backward
dengamma value 1.083800
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.082665
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.089776
backpointers: 40.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 0.975704
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 130 launches for forward, 130 launches for backward
dengamma value 1.056808
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.071527
backpointers: 38.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.119046
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.022266
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.057294
 Epoch[ 2 of 3]-Minibatch[  51-  60 of 8192]: SamplesSeen = 6994; TrainLossPerSample =  0.08150286; EvalErr[0]PerSample = 0.31855876; TotalTime = 2.29934s; TotalTimePerSample = 0.32876ms; SamplesPerSecond = 3041
backpointers: 24.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 169 launches for forward, 169 launches for backward
dengamma value 1.113186
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.079921
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.175955
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 120 launches for forward, 120 launches for backward
dengamma value 1.024600
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.053710
backpointers: 22.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.130235
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 0.975800
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.141175
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.185706
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.065715
backpointers: 41.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.152167
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.111953
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.060055
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.027962
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.074742
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 12 launches for forward, 12 launches for backward
dengamma value 1.093271
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.048562
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.041243
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.095344
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.088717
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.070026
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.152819
backpointers: 25.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.080982
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.101065
 Epoch[ 2 of 3]-Minibatch[  61-  70 of 8192]: SamplesSeen = 6572; TrainLossPerSample =  0.07725126; EvalErr[0]PerSample = 0.30340840; TotalTime = 2.31177s; TotalTimePerSample = 0.35176ms; SamplesPerSecond = 2842
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.097197
backpointers: 39.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.021022
backpointers: 27.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.078317
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.085192
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.072534
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.117073
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.200122
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.072119
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.074173
backpointers: 32.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.060047
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.006856
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.043448
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.018330
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.178684
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.084694
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.941980
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.006588
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.032836
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.074308
backpointers: 24.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.085413
backpointers: 33.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.216896
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.015795
 Epoch[ 2 of 3]-Minibatch[  71-  80 of 8192]: SamplesSeen = 5506; TrainLossPerSample =  0.08773960; EvalErr[0]PerSample = 0.32909553; TotalTime = 1.92360s; TotalTimePerSample = 0.34936ms; SamplesPerSecond = 2862
backpointers: 43.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.002740
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.074720
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.015641
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.082433
backpointers: 25.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.124583
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.121300
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.129819
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.078518
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.040683
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.013661
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.096475
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 0.958585
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.076417
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.027448
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.066752
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.115725
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.114337
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.090009
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.107314
backpointers: 40.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.006038
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.001244
 Epoch[ 2 of 3]-Minibatch[  81-  90 of 8192]: SamplesSeen = 5628; TrainLossPerSample =  0.08559584; EvalErr[0]PerSample = 0.33688699; TotalTime = 1.96671s; TotalTimePerSample = 0.34945ms; SamplesPerSecond = 2861
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.039761
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.123938
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.071136
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.098651
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.086919
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.986629
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.035707
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.000715
backpointers: 27.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.104032
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.055806
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 146 launches for forward, 146 launches for backward
dengamma value 1.166729
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.092039
backpointers: 37.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.083523
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.073980
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.020984
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.043832
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.045460
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 113 launches for forward, 113 launches for backward
dengamma value 1.060372
backpointers: 53.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.059040
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 22 launches for forward, 22 launches for backward
dengamma value 1.135645
backpointers: 32.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.018242
backpointers: 31.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 97 launches for forward, 97 launches for backward
dengamma value 1.030202
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.055369
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 0.988205
 Epoch[ 2 of 3]-Minibatch[  91- 100 of 8192]: SamplesSeen = 6032; TrainLossPerSample =  0.08217202; EvalErr[0]PerSample = 0.32625995; TotalTime = 2.03856s; TotalTimePerSample = 0.33796ms; SamplesPerSecond = 2958
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.040056
backpointers: 37.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.059124
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.058682
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.091049
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.067363
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.119150
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.081032
backpointers: 43.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 0.965229
backpointers: 23.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.122860
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.133967
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.037526
backpointers: 36.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 0.968076
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.059440
backpointers: 22.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.140410
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.086110
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.997839
backpointers: 24.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.165703
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.048398
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.104210
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.028737
 Epoch[ 2 of 3]-Minibatch[ 101- 110 of 8192]: SamplesSeen = 4790; TrainLossPerSample =  0.08802488; EvalErr[0]PerSample = 0.33799582; TotalTime = 1.77687s; TotalTimePerSample = 0.37095ms; SamplesPerSecond = 2695
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.015772
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 0.967842
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.055026
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.060577
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.081279
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.012655
backpointers: 29.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.010069
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.089346
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.075223
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 0.957264
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.129906
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 111 launches for forward, 111 launches for backward
dengamma value 1.057574
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.077032
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.016718
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.128840
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 108 launches for forward, 108 launches for backward
dengamma value 1.068581
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.133210
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.167371
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 0.961193
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.143450
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.034745
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.034677
 Epoch[ 2 of 3]-Minibatch[ 111- 120 of 8192]: SamplesSeen = 5986; TrainLossPerSample =  0.08562215; EvalErr[0]PerSample = 0.34513866; TotalTime = 2.04642s; TotalTimePerSample = 0.34187ms; SamplesPerSecond = 2925
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.027943
backpointers: 45.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.006940
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 39 launches for forward, 39 launches for backward
dengamma value 1.054680
backpointers: 25.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 119 launches for forward, 119 launches for backward
dengamma value 1.089151
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.029769
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.217527
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.101724
backpointers: 44.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.113608
backpointers: 26.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 98 launches for forward, 98 launches for backward
dengamma value 1.074514
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.081673
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.094728
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.087523
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.003731
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 0.994219
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.028819
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.075156
backpointers: 41.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 110 launches for forward, 110 launches for backward
dengamma value 1.049089
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.053537
backpointers: 26.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.120696
backpointers: 22.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.045499
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.108837
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.059376
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.091137
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 0.994257
 Epoch[ 2 of 3]-Minibatch[ 121- 130 of 8192]: SamplesSeen = 7022; TrainLossPerSample =  0.08639331; EvalErr[0]PerSample = 0.31444033; TotalTime = 2.51956s; TotalTimePerSample = 0.35881ms; SamplesPerSecond = 2786
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 126 launches for forward, 126 launches for backward
dengamma value 1.034703
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.093763
backpointers: 38.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.188881
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.072499
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.043478
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.085861
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.015125
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.085830
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.085830
Finished Epoch[ 2 of 3]: [Training Set] TrainLossPerSample = 0.083325267; EvalErrPerSample = 0.3151876; AvgLearningRatePerSample = 1.999999995e-006; EpochTime=29.801465
Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898 
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 164102), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.088197
backpointers: 24.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 119 launches for forward, 119 launches for backward
dengamma value 1.100274
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.084374
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.104666
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.080939
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.060389
backpointers: 30.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.150334
backpointers: 42.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.039812
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.112423
backpointers: 35.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 0.985965
backpointers: 36.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.117210
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.071346
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.071001
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 128 launches for forward, 128 launches for backward
dengamma value 1.100134
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.077029
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.060612
backpointers: 32.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.105358
backpointers: 29.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.011032
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.095015
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.093480
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.050025
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.071393
backpointers: 38.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.042980
backpointers: 25.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.039124
 Epoch[ 3 of 3]-Minibatch[   1-  10 of 8192]: SamplesSeen = 6292; TrainLossPerSample =  0.08358907; EvalErr[0]PerSample = 0.30101716; TotalTime = 2.63482s; TotalTimePerSample = 0.41876ms; SamplesPerSecond = 2388
backpointers: 25.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 138 launches for forward, 138 launches for backward
dengamma value 1.160758
backpointers: 30.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.035082
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 0.988546
backpointers: 29.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.101547
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.082563
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 133 launches for forward, 133 launches for backward
dengamma value 1.025480
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.047410
backpointers: 41.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 1.136445
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.040095
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 0.976965
backpointers: 41.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 0.961430
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.100491
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.068102
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 38 launches for forward, 38 launches for backward
dengamma value 1.070083
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.033162
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.068287
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.011897
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 88 launches for forward, 88 launches for backward
dengamma value 1.077012
backpointers: 28.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 16 launches for forward, 16 launches for backward
dengamma value 1.112298
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.034657
backpointers: 42.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.074293
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.025731
 Epoch[ 3 of 3]-Minibatch[  11-  20 of 8192]: SamplesSeen = 6596; TrainLossPerSample =  0.07963294; EvalErr[0]PerSample = 0.33110976; TotalTime = 2.53063s; TotalTimePerSample = 0.38366ms; SamplesPerSecond = 2606
backpointers: 36.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.085887
backpointers: 37.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.006123
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.100072
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.118208
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.071417
backpointers: 57.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 15 launches for forward, 15 launches for backward
dengamma value 1.167362
backpointers: 32.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 77 launches for forward, 77 launches for backward
dengamma value 1.031790
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.079441
backpointers: 34.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.055116
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 138 launches for forward, 138 launches for backward
dengamma value 1.062575
backpointers: 26.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.106028
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.058904
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 102 launches for forward, 102 launches for backward
dengamma value 1.022302
backpointers: 25.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.037796
backpointers: 31.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.120612
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.954849
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.048381
backpointers: 36.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.034218
backpointers: 28.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.031126
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 56 launches for forward, 56 launches for backward
dengamma value 1.173674
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.070757
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.203380
 Epoch[ 3 of 3]-Minibatch[  21-  30 of 8192]: SamplesSeen = 5666; TrainLossPerSample =  0.08600030; EvalErr[0]PerSample = 0.29844688; TotalTime = 2.11760s; TotalTimePerSample = 0.37374ms; SamplesPerSecond = 2675
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.131507
backpointers: 38.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.027065
backpointers: 37.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.060685
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 90 launches for forward, 90 launches for backward
dengamma value 1.093865
backpointers: 35.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.081544
backpointers: 32.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.078910
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.105020
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 67 launches for forward, 67 launches for backward
dengamma value 1.070809
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.094553
backpointers: 28.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.076389
backpointers: 22.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.105419
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.050209
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.002027
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 114 launches for forward, 114 launches for backward
dengamma value 1.128992
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 1.124045
backpointers: 32.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.039761
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 137 launches for forward, 137 launches for backward
dengamma value 1.086744
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.040163
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.089048
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.037200
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.085009
backpointers: 44.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.031806
 Epoch[ 3 of 3]-Minibatch[  31-  40 of 8192]: SamplesSeen = 6626; TrainLossPerSample =  0.08130104; EvalErr[0]PerSample = 0.28327800; TotalTime = 2.34838s; TotalTimePerSample = 0.35442ms; SamplesPerSecond = 2821
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.094679
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.037562
backpointers: 41.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.033161
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.093461
backpointers: 25.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 124 launches for forward, 124 launches for backward
dengamma value 1.125289
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 21 launches for forward, 21 launches for backward
dengamma value 1.151998
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 24 launches for forward, 24 launches for backward
dengamma value 1.093540
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.099577
backpointers: 38.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 12 launches for forward, 12 launches for backward
dengamma value 1.029092
backpointers: 27.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.087039
backpointers: 21.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 150 launches for forward, 150 launches for backward
dengamma value 1.145239
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.075084
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.043946
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.067017
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.024815
backpointers: 28.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.064146
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.083589
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.064345
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.111978
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.106094
backpointers: 32.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 1.074091
backpointers: 27.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 1.080812
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.068420
backpointers: 27.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.059111
 Epoch[ 3 of 3]-Minibatch[  41-  50 of 8192]: SamplesSeen = 5652; TrainLossPerSample =  0.08144237; EvalErr[0]PerSample = 0.30042463; TotalTime = 2.20424s; TotalTimePerSample = 0.38999ms; SamplesPerSecond = 2564
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.092134
backpointers: 44.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.047652
backpointers: 31.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.002004
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.048934
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.037261
backpointers: 22.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 130 launches for forward, 130 launches for backward
dengamma value 1.150848
backpointers: 31.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.047778
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.048185
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.106731
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.022934
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.029545
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.055188
backpointers: 29.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.074436
backpointers: 27.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 75 launches for forward, 75 launches for backward
dengamma value 1.073219
backpointers: 39.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.089149
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.040302
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.009003
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.036303
backpointers: 31.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 110 launches for forward, 110 launches for backward
dengamma value 1.098994
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.043439
backpointers: 39.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.056252
 Epoch[ 3 of 3]-Minibatch[  51-  60 of 8192]: SamplesSeen = 6588; TrainLossPerSample =  0.08578500; EvalErr[0]PerSample = 0.33986035; TotalTime = 2.33630s; TotalTimePerSample = 0.35463ms; SamplesPerSecond = 2819
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.065623
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 127 launches for forward, 127 launches for backward
dengamma value 1.096644
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.041698
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 0.983649
backpointers: 19.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.153678
backpointers: 28.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.073628
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.081920
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.037372
backpointers: 37.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.007501
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.093147
backpointers: 40.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.082585
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.026000
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.028268
backpointers: 23.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.123083
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 109 launches for forward, 109 launches for backward
dengamma value 1.135424
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.056123
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.042214
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.079012
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.108924
backpointers: 33.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.058956
backpointers: 35.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.028283
 Epoch[ 3 of 3]-Minibatch[  61-  70 of 8192]: SamplesSeen = 6328; TrainLossPerSample =  0.08631525; EvalErr[0]PerSample = 0.30941846; TotalTime = 2.31193s; TotalTimePerSample = 0.36535ms; SamplesPerSecond = 2737
backpointers: 38.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.072575
backpointers: 22.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 128 launches for forward, 128 launches for backward
dengamma value 1.097221
backpointers: 40.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.011787
backpointers: 30.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 109 launches for forward, 109 launches for backward
dengamma value 1.103157
backpointers: 33.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 50 launches for forward, 50 launches for backward
dengamma value 1.126538
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.070615
backpointers: 24.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 100 launches for forward, 100 launches for backward
dengamma value 1.089780
backpointers: 28.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 121 launches for forward, 121 launches for backward
dengamma value 1.109849
backpointers: 37.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.052071
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 0.991507
backpointers: 44.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.060412
backpointers: 35.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.032442
backpointers: 37.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 43 launches for forward, 43 launches for backward
dengamma value 1.022123
backpointers: 28.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.104983
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.048826
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 47 launches for forward, 47 launches for backward
dengamma value 1.083207
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.072539
backpointers: 27.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.192497
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.086666
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.062849
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.065174
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.108937
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.036068
backpointers: 26.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 136 launches for forward, 136 launches for backward
dengamma value 1.100262
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.077704
 Epoch[ 3 of 3]-Minibatch[  71-  80 of 8192]: SamplesSeen = 6980; TrainLossPerSample =  0.08290494; EvalErr[0]PerSample = 0.28839542; TotalTime = 2.58266s; TotalTimePerSample = 0.37001ms; SamplesPerSecond = 2702
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.050160
backpointers: 44.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 0.978864
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 0.951252
backpointers: 45.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 13 launches for forward, 13 launches for backward
dengamma value 1.137689
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 36 launches for forward, 36 launches for backward
dengamma value 1.125302
backpointers: 38.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.023794
backpointers: 36.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 115 launches for forward, 115 launches for backward
dengamma value 1.281458
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.102675
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.066576
backpointers: 30.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.116553
backpointers: 35.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 109 launches for forward, 109 launches for backward
dengamma value 0.979398
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.075125
backpointers: 37.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.059821
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 1.015062
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.119501
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.061463
backpointers: 38.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.002527
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.284401
backpointers: 27.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.185396
backpointers: 34.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.049221
backpointers: 36.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.136681
backpointers: 35.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.981109
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.022417
 Epoch[ 3 of 3]-Minibatch[  81-  90 of 8192]: SamplesSeen = 6774; TrainLossPerSample =  0.07985366; EvalErr[0]PerSample = 0.31251845; TotalTime = 2.29484s; TotalTimePerSample = 0.33877ms; SamplesPerSecond = 2951
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 26 launches for forward, 26 launches for backward
dengamma value 0.991020
backpointers: 28.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.080265
backpointers: 35.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 25 launches for forward, 25 launches for backward
dengamma value 1.125530
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.028283
backpointers: 38.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 0.931542
backpointers: 26.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 107 launches for forward, 107 launches for backward
dengamma value 1.073510
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 81 launches for forward, 81 launches for backward
dengamma value 1.040909
backpointers: 40.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 23 launches for forward, 23 launches for backward
dengamma value 1.068603
backpointers: 38.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 0.930335
backpointers: 28.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 134 launches for forward, 134 launches for backward
dengamma value 1.068783
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.012925
backpointers: 34.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.095008
backpointers: 33.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.032397
backpointers: 41.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.980707
backpointers: 28.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.071785
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 27 launches for forward, 27 launches for backward
dengamma value 0.933892
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.054113
backpointers: 28.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.016794
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 96 launches for forward, 96 launches for backward
dengamma value 1.071612
backpointers: 32.2% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.069752
backpointers: 26.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.103920
backpointers: 31.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 73 launches for forward, 73 launches for backward
dengamma value 1.082665
 Epoch[ 3 of 3]-Minibatch[  91- 100 of 8192]: SamplesSeen = 6146; TrainLossPerSample =  0.08853478; EvalErr[0]PerSample = 0.35388871; TotalTime = 2.52464s; TotalTimePerSample = 0.41078ms; SamplesPerSecond = 2434
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.017763
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 34 launches for forward, 34 launches for backward
dengamma value 1.131387
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
backpointers: 25.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.059096
backpointers: 32.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.147637
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 0.959775
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.082525
backpointers: 40.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.071139
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.150287
backpointers: 35.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.107909
backpointers: 26.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.082605
backpointers: 34.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.038306
backpointers: 39.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.014547
backpointers: 30.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 92 launches for forward, 92 launches for backward
dengamma value 1.165069
backpointers: 33.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.063468
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 30 launches for forward, 30 launches for backward
dengamma value 1.117538
backpointers: 34.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 40 launches for forward, 40 launches for backward
dengamma value 1.068459
backpointers: 29.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.086082
backpointers: 27.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 100 launches for forward, 100 launches for backward
dengamma value 1.095840
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.014370
backpointers: 29.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 88 launches for forward, 88 launches for backward
dengamma value 1.139207
backpointers: 37.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.066824
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.079760
 Epoch[ 3 of 3]-Minibatch[ 101- 110 of 8192]: SamplesSeen = 5366; TrainLossPerSample =  0.08208266; EvalErr[0]PerSample = 0.30301901; TotalTime = 2.22295s; TotalTimePerSample = 0.41427ms; SamplesPerSecond = 2413
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.061395
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.047785
backpointers: 46.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.090933
backpointers: 34.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.070985
backpointers: 34.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.064221
backpointers: 33.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.015890
backpointers: 39.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 116 launches for forward, 116 launches for backward
dengamma value 1.125446
backpointers: 27.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 60 launches for forward, 60 launches for backward
dengamma value 1.071238
backpointers: 35.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.019477
backpointers: 35.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 87 launches for forward, 87 launches for backward
dengamma value 1.031274
backpointers: 42.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 14 launches for forward, 14 launches for backward
dengamma value 1.034352
backpointers: 32.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.063972
backpointers: 36.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.058113
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.040602
backpointers: 32.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.091374
backpointers: 33.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.086140
backpointers: 39.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.112651
backpointers: 31.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.103647
backpointers: 22.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.057306
backpointers: 27.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 0.988464
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.036586
backpointers: 31.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.127764
backpointers: 31.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 106 launches for forward, 106 launches for backward
dengamma value 1.148842
backpointers: 34.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 17 launches for forward, 17 launches for backward
dengamma value 1.096221
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.088360
 Epoch[ 3 of 3]-Minibatch[ 111- 120 of 8192]: SamplesSeen = 6220; TrainLossPerSample =  0.08114253; EvalErr[0]PerSample = 0.32250804; TotalTime = 2.27397s; TotalTimePerSample = 0.36559ms; SamplesPerSecond = 2735
backpointers: 34.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.082362
backpointers: 30.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.051853
backpointers: 39.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.113975
backpointers: 41.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.084998
backpointers: 37.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 89 launches for forward, 89 launches for backward
dengamma value 1.030133
backpointers: 34.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.070156
backpointers: 37.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.075748
backpointers: 23.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 102 launches for forward, 102 launches for backward
dengamma value 1.107124
backpointers: 30.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.086904
backpointers: 30.6% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.084355
backpointers: 33.1% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.043573
backpointers: 30.9% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 45 launches for forward, 45 launches for backward
dengamma value 1.068356
backpointers: 36.0% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 74 launches for forward, 74 launches for backward
dengamma value 1.032575
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.093060
backpointers: 40.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 12 launches for forward, 12 launches for backward
dengamma value 1.160387
backpointers: 36.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 1.106279
backpointers: 33.4% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.066269
backpointers: 37.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 72 launches for forward, 72 launches for backward
dengamma value 0.998378
backpointers: 29.7% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.082442
backpointers: 33.8% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.033566
backpointers: 31.5% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 1.083995
backpointers: 26.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.100964
backpointers: 29.3% edges have at least one /sil/ unit inside
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.142857
 Epoch[ 3 of 3]-Minibatch[ 121- 130 of 8192]: SamplesSeen = 6564; TrainLossPerSample =  0.08458945; EvalErr[0]PerSample = 0.30042657; TotalTime = 2.28939s; TotalTimePerSample = 0.34878ms; SamplesPerSecond = 2867
Finished Epoch[ 3 of 3]: [Training Set] TrainLossPerSample = 0.083290465; EvalErrPerSample = 0.31113231; AvgLearningRatePerSample = 1.999999995e-006; EpochTime=30.742683
CNTKCommandTrainEnd: sequenceTrain
__COMPLETED__

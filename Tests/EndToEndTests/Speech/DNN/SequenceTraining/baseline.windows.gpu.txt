=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData
05/03/2016 14:24:29: -------------------------------------------------------------------
05/03/2016 14:24:29: Build info: 

05/03/2016 14:24:29: 		Built time: May  3 2016 13:23:06
05/03/2016 14:24:29: 		Last modified date: Mon Apr 18 00:00:12 2016
05/03/2016 14:24:29: 		Build type: Release
05/03/2016 14:24:29: 		Build target: GPU
05/03/2016 14:24:29: 		With 1bit-SGD: no
05/03/2016 14:24:29: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
05/03/2016 14:24:29: 		CUB_PATH: C:\src\cub-1.4.1
05/03/2016 14:24:29: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
05/03/2016 14:24:29: 		Build Branch: HEAD
05/03/2016 14:24:29: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
05/03/2016 14:24:29: 		Built by svcphil on LIANA-09-w
05/03/2016 14:24:29: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
05/03/2016 14:24:29: -------------------------------------------------------------------

05/03/2016 14:24:29: Running on cntk-muc02 at 2016/05/03 14:24:29
05/03/2016 14:24:29: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu  DeviceId=0  timestamping=true



05/03/2016 14:24:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
05/03/2016 14:24:29: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "$RunDir$/models/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.sequence.0"
    editPath  = "$ConfigDir$/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "$DataDir$/model.overalltying"
            transpFile = "$DataDir$/model.transprob"
        ]
        lattices = [
            denlatTocFile = "$DataDir$/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

05/03/2016 14:24:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

05/03/2016 14:24:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
05/03/2016 14:24:29: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

05/03/2016 14:24:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

05/03/2016 14:24:29: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
05/03/2016 14:24:29: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
05/03/2016 14:24:29: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
05/03/2016 14:24:29: Precision = "float"
05/03/2016 14:24:29: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech
05/03/2016 14:24:29: CNTKCommandTrainInfo: dptPre1 : 2
05/03/2016 14:24:29: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech
05/03/2016 14:24:29: CNTKCommandTrainInfo: dptPre2 : 2
05/03/2016 14:24:29: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech
05/03/2016 14:24:29: CNTKCommandTrainInfo: speechTrain : 4
05/03/2016 14:24:29: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence
05/03/2016 14:24:29: CNTKCommandTrainInfo: sequenceTrain : 3
05/03/2016 14:24:29: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11

05/03/2016 14:24:29: ##############################################################################
05/03/2016 14:24:29: #                                                                            #
05/03/2016 14:24:29: # Action "train"                                                             #
05/03/2016 14:24:29: #                                                                            #
05/03/2016 14:24:29: ##############################################################################

05/03/2016 14:24:29: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

05/03/2016 14:24:29: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

05/03/2016 14:24:30: Created model with 19 nodes on GPU 0.

05/03/2016 14:24:30: Training criterion node(s):
05/03/2016 14:24:30: 	ce = CrossEntropyWithSoftmax

05/03/2016 14:24:30: Evaluation criterion node(s):

05/03/2016 14:24:30: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0000005B94F40980: {[logPrior Value[132 x 1]] }
0000005B94F40A20: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
0000005B94F40AC0: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
0000005B94F40E80: {[OL.W Value[132 x 512]] }
0000005B94F411A0: {[scaledLogLikelihood Value[132 x 1 x *]] }
0000005B94F41240: {[HL1.t Value[512 x *]] }
0000005B94F41380: {[HL1.W Value[512 x 363]] }
0000005B94F41880: {[ce Value[1]] }
0000005B94F41C40: {[featNorm Value[363 x *]] }
0000005B94F41D80: {[globalPrior Value[132 x 1]] }
0000005B94F41EC0: {[OL.b Value[132 x 1]] }
0000005B94F41F60: {[err Value[1]] }
0000005B94F42000: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
0000005B94F42140: {[HL1.b Value[512 x 1]] }
0000005B94F42320: {[ce Gradient[1]] }
0000005B94F42460: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
0000005B97758130: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
0000005B97758590: {[OL.t Gradient[132 x 1 x *]] }
0000005B977588B0: {[OL.b Gradient[132 x 1]] }
0000005BFCF6CD80: {[labels Value[132 x *]] }
0000005BFCF6CF60: {[globalMean Value[363 x 1]] }
0000005BFCF6D140: {[features Value[363 x *]] }
0000005BFCF6DAA0: {[globalInvStd Value[363 x 1]] }

05/03/2016 14:24:30: No PreCompute nodes found, skipping PreCompute step.

05/03/2016 14:24:30: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

05/03/2016 14:24:30: Starting minibatch loop.
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.89978256 * 2560; err = 0.84375000 * 2560; time = 0.2109s; samplesPerSecond = 12140.8
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.96755676 * 2560; err = 0.72031250 * 2560; time = 0.0284s; samplesPerSecond = 90188.5
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55723495 * 2560; err = 0.65859375 * 2560; time = 0.0285s; samplesPerSecond = 89859.2
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.29642715 * 2560; err = 0.61992187 * 2560; time = 0.0285s; samplesPerSecond = 89843.5
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.02396469 * 2560; err = 0.55117187 * 2560; time = 0.0285s; samplesPerSecond = 89780.5
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87309265 * 2560; err = 0.51484375 * 2560; time = 0.0287s; samplesPerSecond = 89152.0
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.78157196 * 2560; err = 0.50507813 * 2560; time = 0.0277s; samplesPerSecond = 92488.9
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.75391235 * 2560; err = 0.50781250 * 2560; time = 0.0276s; samplesPerSecond = 92699.9
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.66460266 * 2560; err = 0.45742187 * 2560; time = 0.0276s; samplesPerSecond = 92656.3
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62184296 * 2560; err = 0.47968750 * 2560; time = 0.0276s; samplesPerSecond = 92683.1
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.65328217 * 2560; err = 0.47265625 * 2560; time = 0.0276s; samplesPerSecond = 92878.1
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.50686951 * 2560; err = 0.44921875 * 2560; time = 0.0277s; samplesPerSecond = 92542.4
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.46723938 * 2560; err = 0.42304687 * 2560; time = 0.0276s; samplesPerSecond = 92740.2
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.49163513 * 2560; err = 0.44140625 * 2560; time = 0.0276s; samplesPerSecond = 92683.1
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46437683 * 2560; err = 0.43398437 * 2560; time = 0.0276s; samplesPerSecond = 92606.0
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.43047485 * 2560; err = 0.43867187 * 2560; time = 0.0276s; samplesPerSecond = 92757.0
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.42106018 * 2560; err = 0.41992188 * 2560; time = 0.0276s; samplesPerSecond = 92723.4
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.46538086 * 2560; err = 0.42421875 * 2560; time = 0.0276s; samplesPerSecond = 92592.6
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.47427673 * 2560; err = 0.44062500 * 2560; time = 0.0276s; samplesPerSecond = 92837.7
05/03/2016 14:24:30:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42847290 * 2560; err = 0.44023438 * 2560; time = 0.0276s; samplesPerSecond = 92777.2
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.34078369 * 2560; err = 0.41171875 * 2560; time = 0.0277s; samplesPerSecond = 92542.4
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.39474487 * 2560; err = 0.42734375 * 2560; time = 0.0276s; samplesPerSecond = 92783.9
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.40151062 * 2560; err = 0.41250000 * 2560; time = 0.0276s; samplesPerSecond = 92763.7
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.39345703 * 2560; err = 0.42734375 * 2560; time = 0.0277s; samplesPerSecond = 92542.4
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.32485046 * 2560; err = 0.40156250 * 2560; time = 0.0276s; samplesPerSecond = 92703.2
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27032471 * 2560; err = 0.39765625 * 2560; time = 0.0276s; samplesPerSecond = 92773.8
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.32375488 * 2560; err = 0.39257813 * 2560; time = 0.0276s; samplesPerSecond = 92646.2
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.25393982 * 2560; err = 0.38320312 * 2560; time = 0.0266s; samplesPerSecond = 96175.5
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23377075 * 2560; err = 0.36953125 * 2560; time = 0.0259s; samplesPerSecond = 98757.8
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20861511 * 2560; err = 0.35976562 * 2560; time = 0.0264s; samplesPerSecond = 96969.7
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23675232 * 2560; err = 0.36757812 * 2560; time = 0.0274s; samplesPerSecond = 93291.1
05/03/2016 14:24:31:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.22960205 * 2560; err = 0.37460938 * 2560; time = 0.0277s; samplesPerSecond = 92428.8
05/03/2016 14:24:31: Finished Epoch[ 1 of 2]: [Training] ce = 1.65172386 * 81920; err = 0.46774902 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.22961s
05/03/2016 14:24:31: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech.1'

05/03/2016 14:24:31: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

05/03/2016 14:24:31: Starting minibatch loop.
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.21869726 * 2560; err = 0.36992188 * 2560; time = 0.0269s; samplesPerSecond = 95223.9
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.18345690 * 2560; err = 0.36679688 * 2560; time = 0.0271s; samplesPerSecond = 94590.6
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.17220421 * 2560; err = 0.35898438 * 2560; time = 0.0273s; samplesPerSecond = 93745.4
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.20035286 * 2560; err = 0.35781250 * 2560; time = 0.0257s; samplesPerSecond = 99579.9
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.19499741 * 2560; err = 0.37460938 * 2560; time = 0.0259s; samplesPerSecond = 98803.6
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.16373482 * 2560; err = 0.34687500 * 2560; time = 0.0270s; samplesPerSecond = 94751.6
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.13869247 * 2560; err = 0.34804687 * 2560; time = 0.0265s; samplesPerSecond = 96571.0
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.19293823 * 2560; err = 0.36992188 * 2560; time = 0.0263s; samplesPerSecond = 97401.4
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.23978348 * 2560; err = 0.37539062 * 2560; time = 0.0276s; samplesPerSecond = 92733.5
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18622742 * 2560; err = 0.36406250 * 2560; time = 0.0279s; samplesPerSecond = 91657.7
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16710892 * 2560; err = 0.35703125 * 2560; time = 0.0271s; samplesPerSecond = 94513.8
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.24683685 * 2560; err = 0.38554688 * 2560; time = 0.0257s; samplesPerSecond = 99483.2
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.18601685 * 2560; err = 0.35273437 * 2560; time = 0.0261s; samplesPerSecond = 98246.2
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.21721497 * 2560; err = 0.37617187 * 2560; time = 0.0272s; samplesPerSecond = 94110.7
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.19934692 * 2560; err = 0.36953125 * 2560; time = 0.0276s; samplesPerSecond = 92831.0
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.15099945 * 2560; err = 0.34257813 * 2560; time = 0.0276s; samplesPerSecond = 92753.6
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.14984589 * 2560; err = 0.35703125 * 2560; time = 0.0277s; samplesPerSecond = 92562.5
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.19028320 * 2560; err = 0.35898438 * 2560; time = 0.0276s; samplesPerSecond = 92696.5
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.16434784 * 2560; err = 0.36406250 * 2560; time = 0.0276s; samplesPerSecond = 92686.5
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08853760 * 2560; err = 0.33359375 * 2560; time = 0.0279s; samplesPerSecond = 91687.3
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.15194244 * 2560; err = 0.35039063 * 2560; time = 0.0277s; samplesPerSecond = 92512.3
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16113434 * 2560; err = 0.35625000 * 2560; time = 0.0272s; samplesPerSecond = 94062.3
05/03/2016 14:24:31:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.18479004 * 2560; err = 0.36757812 * 2560; time = 0.0279s; samplesPerSecond = 91884.7
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.14554138 * 2560; err = 0.34843750 * 2560; time = 0.0281s; samplesPerSecond = 91132.4
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.15263367 * 2560; err = 0.35390625 * 2560; time = 0.0274s; samplesPerSecond = 93413.6
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.08563538 * 2560; err = 0.33437500 * 2560; time = 0.0257s; samplesPerSecond = 99595.4
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10797424 * 2560; err = 0.34882812 * 2560; time = 0.0271s; samplesPerSecond = 94562.6
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.07031860 * 2560; err = 0.33593750 * 2560; time = 0.0276s; samplesPerSecond = 92851.2
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.09429016 * 2560; err = 0.33476563 * 2560; time = 0.0270s; samplesPerSecond = 94952.0
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14634094 * 2560; err = 0.35351563 * 2560; time = 0.0257s; samplesPerSecond = 99711.8
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.10476990 * 2560; err = 0.34335938 * 2560; time = 0.0265s; samplesPerSecond = 96768.1
05/03/2016 14:24:32:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07355957 * 2560; err = 0.32695313 * 2560; time = 0.0268s; samplesPerSecond = 95351.6
05/03/2016 14:24:32: Finished Epoch[ 2 of 2]: [Training] ce = 1.16032982 * 81920; err = 0.35574951 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.867777s
05/03/2016 14:24:32: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech'
05/03/2016 14:24:32: CNTKCommandTrainEnd: dptPre1

05/03/2016 14:24:32: Action "train" complete.


05/03/2016 14:24:32: ##############################################################################
05/03/2016 14:24:32: #                                                                            #
05/03/2016 14:24:32: # Action "edit"                                                              #
05/03/2016 14:24:32: #                                                                            #
05/03/2016 14:24:32: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


05/03/2016 14:24:32: Action "edit" complete.


05/03/2016 14:24:32: ##############################################################################
05/03/2016 14:24:32: #                                                                            #
05/03/2016 14:24:32: # Action "train"                                                             #
05/03/2016 14:24:32: #                                                                            #
05/03/2016 14:24:32: ##############################################################################

05/03/2016 14:24:32: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

05/03/2016 14:24:32: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

05/03/2016 14:24:32: Loaded model with 24 nodes on GPU 0.

05/03/2016 14:24:32: Training criterion node(s):
05/03/2016 14:24:32: 	ce = CrossEntropyWithSoftmax

05/03/2016 14:24:32: Evaluation criterion node(s):

05/03/2016 14:24:32: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
0000005B94F40660: {[err Value[1]] }
0000005B94F40840: {[scaledLogLikelihood Value[132 x 1 x *3]] }
0000005B94F40980: {[ce Value[1]] }
0000005B94F40A20: {[logPrior Value[132 x 1]] }
0000005B94F40DE0: {[labels Value[132 x *3]] }
0000005B94F41380: {[HL2.W Value[512 x 512]] }
0000005B94F419C0: {[OL.b Value[132 x 1]] }
0000005B94F42320: {[OL.W Value[132 x 512]] }
0000005B97757CD0: {[globalMean Value[363 x 1]] }
0000005B97757E10: {[features Value[363 x *3]] }
0000005B97757EB0: {[globalPrior Value[132 x 1]] }
0000005B97758270: {[HL1.b Value[512 x 1]] }
0000005B97758450: {[globalInvStd Value[363 x 1]] }
0000005B97758590: {[HL2.b Value[512 x 1]] }
0000005B977586D0: {[HL1.W Value[512 x 363]] }
0000005B9C4BD890: {[ce Gradient[1]] }
0000005B9C4BDB10: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
0000005B9C4BDD90: {[OL.t Gradient[132 x 1 x *3]] }
0000005B9C4BE3D0: {[OL.b Gradient[132 x 1]] }
0000005BFCF6C920: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
0000005BFCF6CD80: {[HL1.t Value[512 x *3]] }
0000005BFCF6CF60: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
0000005BFCF6D140: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
0000005BFCF6D780: {[featNorm Value[363 x *3]] }
0000005BFCF6D8C0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
0000005BFCF6DA00: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
0000005BFCF6DAA0: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
0000005BFCF6DBE0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }

05/03/2016 14:24:32: No PreCompute nodes found, skipping PreCompute step.

05/03/2016 14:24:32: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

05/03/2016 14:24:33: Starting minibatch loop.
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 4.49739113 * 2560; err = 0.80429688 * 2560; time = 0.0469s; samplesPerSecond = 54609.8
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.83226433 * 2560; err = 0.68125000 * 2560; time = 0.0398s; samplesPerSecond = 64362.0
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.25921097 * 2560; err = 0.59921875 * 2560; time = 0.0392s; samplesPerSecond = 65256.2
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.91240921 * 2560; err = 0.51210937 * 2560; time = 0.0398s; samplesPerSecond = 64334.5
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.69259949 * 2560; err = 0.46679688 * 2560; time = 0.0398s; samplesPerSecond = 64344.2
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.59069672 * 2560; err = 0.45312500 * 2560; time = 0.0398s; samplesPerSecond = 64245.7
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.48813324 * 2560; err = 0.43789062 * 2560; time = 0.0398s; samplesPerSecond = 64355.6
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.48960571 * 2560; err = 0.43515625 * 2560; time = 0.0398s; samplesPerSecond = 64352.3
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.45628204 * 2560; err = 0.42187500 * 2560; time = 0.0397s; samplesPerSecond = 64488.5
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41567383 * 2560; err = 0.40820313 * 2560; time = 0.0400s; samplesPerSecond = 64070.5
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.42048950 * 2560; err = 0.41406250 * 2560; time = 0.0398s; samplesPerSecond = 64350.7
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.34279480 * 2560; err = 0.39726563 * 2560; time = 0.0398s; samplesPerSecond = 64328.1
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.31633148 * 2560; err = 0.38789062 * 2560; time = 0.0398s; samplesPerSecond = 64244.1
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.33296814 * 2560; err = 0.39804688 * 2560; time = 0.0398s; samplesPerSecond = 64347.5
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.32084351 * 2560; err = 0.39609375 * 2560; time = 0.0397s; samplesPerSecond = 64506.4
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27189636 * 2560; err = 0.38125000 * 2560; time = 0.0382s; samplesPerSecond = 67047.3
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29380188 * 2560; err = 0.38554688 * 2560; time = 0.0391s; samplesPerSecond = 65461.4
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.31463013 * 2560; err = 0.38984375 * 2560; time = 0.0391s; samplesPerSecond = 65550.3
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33578796 * 2560; err = 0.40664062 * 2560; time = 0.0394s; samplesPerSecond = 65047.3
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.32202454 * 2560; err = 0.41484375 * 2560; time = 0.0400s; samplesPerSecond = 64019.2
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23669434 * 2560; err = 0.37460938 * 2560; time = 0.0395s; samplesPerSecond = 64746.2
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.27109985 * 2560; err = 0.38906250 * 2560; time = 0.0400s; samplesPerSecond = 63977.6
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.26419678 * 2560; err = 0.37578125 * 2560; time = 0.0398s; samplesPerSecond = 64269.9
05/03/2016 14:24:33:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.23778992 * 2560; err = 0.37265625 * 2560; time = 0.0399s; samplesPerSecond = 64211.9
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.21040344 * 2560; err = 0.36757812 * 2560; time = 0.0398s; samplesPerSecond = 64347.5
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.18387146 * 2560; err = 0.36562500 * 2560; time = 0.0399s; samplesPerSecond = 64086.5
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23827515 * 2560; err = 0.37148437 * 2560; time = 0.0399s; samplesPerSecond = 64163.6
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.18418274 * 2560; err = 0.36328125 * 2560; time = 0.0401s; samplesPerSecond = 63764.1
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.16851501 * 2560; err = 0.35234375 * 2560; time = 0.0397s; samplesPerSecond = 64446.3
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14337463 * 2560; err = 0.34375000 * 2560; time = 0.0396s; samplesPerSecond = 64581.2
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.17227478 * 2560; err = 0.34882812 * 2560; time = 0.0398s; samplesPerSecond = 64305.5
05/03/2016 14:24:34:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.18431091 * 2560; err = 0.36835937 * 2560; time = 0.0403s; samplesPerSecond = 63572.5
05/03/2016 14:24:34: Finished Epoch[ 1 of 2]: [Training] ce = 1.51252575 * 81920; err = 0.42452393 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.43973s
05/03/2016 14:24:34: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.1'

05/03/2016 14:24:34: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

05/03/2016 14:24:34: Starting minibatch loop.
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.17448177 * 2560; err = 0.35195312 * 2560; time = 0.0421s; samplesPerSecond = 60754.2
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.14536762 * 2560; err = 0.35664062 * 2560; time = 0.0399s; samplesPerSecond = 64236.1
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.15722923 * 2560; err = 0.34531250 * 2560; time = 0.0382s; samplesPerSecond = 67087.7
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.14344521 * 2560; err = 0.34804687 * 2560; time = 0.0391s; samplesPerSecond = 65486.5
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.14842377 * 2560; err = 0.36562500 * 2560; time = 0.0392s; samplesPerSecond = 65259.5
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14489059 * 2560; err = 0.34218750 * 2560; time = 0.0402s; samplesPerSecond = 63740.3
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.09631271 * 2560; err = 0.33984375 * 2560; time = 0.0403s; samplesPerSecond = 63597.7
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16026917 * 2560; err = 0.35546875 * 2560; time = 0.0402s; samplesPerSecond = 63637.3
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.16528015 * 2560; err = 0.36015625 * 2560; time = 0.0401s; samplesPerSecond = 63850.0
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12257309 * 2560; err = 0.34492187 * 2560; time = 0.0401s; samplesPerSecond = 63810.2
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.12313080 * 2560; err = 0.34765625 * 2560; time = 0.0401s; samplesPerSecond = 63865.9
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18492126 * 2560; err = 0.36171875 * 2560; time = 0.0403s; samplesPerSecond = 63540.9
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.13058014 * 2560; err = 0.33476563 * 2560; time = 0.0403s; samplesPerSecond = 63580.4
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16725922 * 2560; err = 0.35781250 * 2560; time = 0.0403s; samplesPerSecond = 63563.0
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.12244720 * 2560; err = 0.34648438 * 2560; time = 0.0400s; samplesPerSecond = 63926.5
05/03/2016 14:24:34:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09480591 * 2560; err = 0.33671875 * 2560; time = 0.0402s; samplesPerSecond = 63621.5
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.11218109 * 2560; err = 0.34140625 * 2560; time = 0.0399s; samplesPerSecond = 64237.7
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11966095 * 2560; err = 0.33398438 * 2560; time = 0.0402s; samplesPerSecond = 63635.7
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10485687 * 2560; err = 0.33671875 * 2560; time = 0.0400s; samplesPerSecond = 63936.1
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.06019897 * 2560; err = 0.32617188 * 2560; time = 0.0401s; samplesPerSecond = 63800.6
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.10600891 * 2560; err = 0.34101562 * 2560; time = 0.0400s; samplesPerSecond = 63932.9
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.13724823 * 2560; err = 0.34101562 * 2560; time = 0.0401s; samplesPerSecond = 63889.8
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12464600 * 2560; err = 0.34609375 * 2560; time = 0.0402s; samplesPerSecond = 63699.0
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.10831604 * 2560; err = 0.33593750 * 2560; time = 0.0395s; samplesPerSecond = 64836.4
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09707031 * 2560; err = 0.34023437 * 2560; time = 0.0398s; samplesPerSecond = 64266.7
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.04812317 * 2560; err = 0.32773438 * 2560; time = 0.0402s; samplesPerSecond = 63748.2
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.04979248 * 2560; err = 0.33398438 * 2560; time = 0.0400s; samplesPerSecond = 64001.6
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.03223572 * 2560; err = 0.31835938 * 2560; time = 0.0385s; samplesPerSecond = 66484.9
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.05677490 * 2560; err = 0.32773438 * 2560; time = 0.0399s; samplesPerSecond = 64102.6
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10880737 * 2560; err = 0.34296875 * 2560; time = 0.0382s; samplesPerSecond = 67049.1
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08513489 * 2560; err = 0.33476563 * 2560; time = 0.0389s; samplesPerSecond = 65784.4
05/03/2016 14:24:35:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04244080 * 2560; err = 0.31757812 * 2560; time = 0.0387s; samplesPerSecond = 66208.0
05/03/2016 14:24:35: Finished Epoch[ 2 of 2]: [Training] ce = 1.11484108 * 81920; err = 0.34190674 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=1.27769s
05/03/2016 14:24:36: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech'
05/03/2016 14:24:36: CNTKCommandTrainEnd: dptPre2

05/03/2016 14:24:36: Action "train" complete.


05/03/2016 14:24:36: ##############################################################################
05/03/2016 14:24:36: #                                                                            #
05/03/2016 14:24:36: # Action "edit"                                                              #
05/03/2016 14:24:36: #                                                                            #
05/03/2016 14:24:36: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


05/03/2016 14:24:36: Action "edit" complete.


05/03/2016 14:24:36: ##############################################################################
05/03/2016 14:24:36: #                                                                            #
05/03/2016 14:24:36: # Action "train"                                                             #
05/03/2016 14:24:36: #                                                                            #
05/03/2016 14:24:36: ##############################################################################

05/03/2016 14:24:36: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

05/03/2016 14:24:36: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

05/03/2016 14:24:36: Loaded model with 29 nodes on GPU 0.

05/03/2016 14:24:36: Training criterion node(s):
05/03/2016 14:24:36: 	ce = CrossEntropyWithSoftmax

05/03/2016 14:24:36: Evaluation criterion node(s):

05/03/2016 14:24:36: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0000005B94F40840: {[globalInvStd Value[363 x 1]] }
0000005B94F41C40: {[features Value[363 x *6]] }
0000005B94F41E20: {[globalMean Value[363 x 1]] }
0000005B94F42140: {[globalPrior Value[132 x 1]] }
0000005B94F42500: {[HL1.b Value[512 x 1]] }
0000005B97757EB0: {[HL1.W Value[512 x 363]] }
0000005B97758270: {[HL2.b Value[512 x 1]] }
0000005B97758590: {[HL2.W Value[512 x 512]] }
0000005B9C4BCFD0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
0000005B9C4BD070: {[HL1.t Value[512 x *6]] }
0000005B9C4BD110: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
0000005B9C4BD1B0: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
0000005B9C4BD2F0: {[logPrior Value[132 x 1]] }
0000005B9C4BD390: {[ce Gradient[1]] }
0000005B9C4BD4D0: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
0000005B9C4BD570: {[OL.b Gradient[132 x 1]] }
0000005B9C4BD9D0: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
0000005B9C4BDBB0: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
0000005B9C4BDC50: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0000005B9C4BDF70: {[featNorm Value[363 x *6]] }
0000005B9C4BE010: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
0000005B9C4BE0B0: {[OL.t Gradient[132 x 1 x *6]] }
0000005B9C4BE290: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
0000005B9C4BE330: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
0000005B9C4BE5B0: {[err Value[1]] }
0000005B9C4BE650: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
0000005B9C4BE790: {[ce Value[1]] }
0000005B9C4BEAB0: {[OL.b Value[132 x 1]] }
0000005B9C4BEB50: {[scaledLogLikelihood Value[132 x 1 x *6]] }
0000005B9C4BED30: {[OL.W Value[132 x 512]] }
0000005BFCF66800: {[labels Value[132 x *6]] }
0000005BFCF6C920: {[HL3.b Value[512 x 1]] }
0000005BFCF6CF60: {[HL3.W Value[512 x 512]] }

05/03/2016 14:24:36: No PreCompute nodes found, skipping PreCompute step.

05/03/2016 14:24:36: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

05/03/2016 14:24:36: Starting minibatch loop.
05/03/2016 14:24:36:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.12455330 * 2560; err = 0.82734375 * 2560; time = 0.0617s; samplesPerSecond = 41505.9
05/03/2016 14:24:36:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.55599785 * 2560; err = 0.63007813 * 2560; time = 0.0523s; samplesPerSecond = 48925.9
05/03/2016 14:24:36:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.03516159 * 2560; err = 0.53945312 * 2560; time = 0.0525s; samplesPerSecond = 48741.5
05/03/2016 14:24:36:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.73739853 * 2560; err = 0.47500000 * 2560; time = 0.0530s; samplesPerSecond = 48320.1
05/03/2016 14:24:36:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.54207916 * 2560; err = 0.43515625 * 2560; time = 0.0529s; samplesPerSecond = 48375.8
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.44409790 * 2560; err = 0.41328125 * 2560; time = 0.0531s; samplesPerSecond = 48245.4
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.36059418 * 2560; err = 0.40898438 * 2560; time = 0.0527s; samplesPerSecond = 48537.2
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35930023 * 2560; err = 0.40117188 * 2560; time = 0.0526s; samplesPerSecond = 48670.1
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.34254303 * 2560; err = 0.38632813 * 2560; time = 0.0510s; samplesPerSecond = 50183.3
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.30505676 * 2560; err = 0.38320312 * 2560; time = 0.0521s; samplesPerSecond = 49140.0
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.30881348 * 2560; err = 0.38476563 * 2560; time = 0.0514s; samplesPerSecond = 49853.0
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.23755188 * 2560; err = 0.37304688 * 2560; time = 0.0527s; samplesPerSecond = 48566.7
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.21070251 * 2560; err = 0.35546875 * 2560; time = 0.0527s; samplesPerSecond = 48605.4
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.24008789 * 2560; err = 0.37109375 * 2560; time = 0.0522s; samplesPerSecond = 49000.8
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.23422089 * 2560; err = 0.36835937 * 2560; time = 0.0510s; samplesPerSecond = 50151.8
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.19425964 * 2560; err = 0.35195312 * 2560; time = 0.0524s; samplesPerSecond = 48851.2
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.21415710 * 2560; err = 0.36289063 * 2560; time = 0.0529s; samplesPerSecond = 48413.3
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.24289856 * 2560; err = 0.37031250 * 2560; time = 0.0522s; samplesPerSecond = 49044.0
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.26465759 * 2560; err = 0.38359375 * 2560; time = 0.0513s; samplesPerSecond = 49920.0
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.22050476 * 2560; err = 0.38085938 * 2560; time = 0.0527s; samplesPerSecond = 48538.2
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.17745056 * 2560; err = 0.35507813 * 2560; time = 0.0527s; samplesPerSecond = 48541.8
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.19851379 * 2560; err = 0.37109375 * 2560; time = 0.0527s; samplesPerSecond = 48571.3
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.21453857 * 2560; err = 0.35820313 * 2560; time = 0.0527s; samplesPerSecond = 48585.1
05/03/2016 14:24:37:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.18011475 * 2560; err = 0.35546875 * 2560; time = 0.0527s; samplesPerSecond = 48576.9
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.16693726 * 2560; err = 0.35195312 * 2560; time = 0.0527s; samplesPerSecond = 48599.9
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.12398987 * 2560; err = 0.35234375 * 2560; time = 0.0525s; samplesPerSecond = 48765.6
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.18822021 * 2560; err = 0.36328125 * 2560; time = 0.0518s; samplesPerSecond = 49461.9
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.13831482 * 2560; err = 0.35078125 * 2560; time = 0.0529s; samplesPerSecond = 48374.9
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12718811 * 2560; err = 0.33984375 * 2560; time = 0.0528s; samplesPerSecond = 48521.6
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.11155396 * 2560; err = 0.34179688 * 2560; time = 0.0529s; samplesPerSecond = 48413.3
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.13423157 * 2560; err = 0.34101562 * 2560; time = 0.0527s; samplesPerSecond = 48545.5
05/03/2016 14:24:38:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12716675 * 2560; err = 0.34414062 * 2560; time = 0.0528s; samplesPerSecond = 48511.5
05/03/2016 14:24:38: Finished Epoch[ 1 of 4]: [Training] ce = 1.40821428 * 81920; err = 0.40085449 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.84542s
05/03/2016 14:24:38: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.1'

05/03/2016 14:24:38: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

05/03/2016 14:24:38: Starting minibatch loop.
05/03/2016 14:24:38:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.20089607 * 5120; err = 0.36757812 * 5120; time = 0.0982s; samplesPerSecond = 52127.3
05/03/2016 14:24:38:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.15295639 * 5120; err = 0.34550781 * 5120; time = 0.0839s; samplesPerSecond = 61015.6
05/03/2016 14:24:38:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.09945831 * 5120; err = 0.33613281 * 5120; time = 0.0843s; samplesPerSecond = 60721.8
05/03/2016 14:24:38:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.09916496 * 5120; err = 0.33867188 * 5120; time = 0.0847s; samplesPerSecond = 60472.2
05/03/2016 14:24:38:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.17260475 * 5120; err = 0.36230469 * 5120; time = 0.0851s; samplesPerSecond = 60129.9
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.15717888 * 5120; err = 0.35820313 * 5120; time = 0.0848s; samplesPerSecond = 60407.3
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.14431229 * 5120; err = 0.34296875 * 5120; time = 0.0849s; samplesPerSecond = 60326.8
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.10515747 * 5120; err = 0.34394531 * 5120; time = 0.0843s; samplesPerSecond = 60703.8
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.15175400 * 5120; err = 0.35449219 * 5120; time = 0.0849s; samplesPerSecond = 60307.0
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.11654053 * 5120; err = 0.34101562 * 5120; time = 0.0847s; samplesPerSecond = 60465.8
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.11851807 * 5120; err = 0.34472656 * 5120; time = 0.0846s; samplesPerSecond = 60552.3
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.11374130 * 5120; err = 0.34492187 * 5120; time = 0.0850s; samplesPerSecond = 60241.7
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.04686737 * 5120; err = 0.32265625 * 5120; time = 0.0850s; samplesPerSecond = 60241.7
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02721252 * 5120; err = 0.32246094 * 5120; time = 0.0849s; samplesPerSecond = 60338.2
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.08386230 * 5120; err = 0.33144531 * 5120; time = 0.0847s; samplesPerSecond = 60462.9
05/03/2016 14:24:39:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06164856 * 5120; err = 0.32558594 * 5120; time = 0.0849s; samplesPerSecond = 60338.2
05/03/2016 14:24:39: Finished Epoch[ 2 of 4]: [Training] ce = 1.11574211 * 81920; err = 0.34266357 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=1.37169s
05/03/2016 14:24:39: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.2'

05/03/2016 14:24:39: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

05/03/2016 14:24:39: Starting minibatch loop.
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.12331724 * 5120; err = 0.34121094 * 5120; time = 0.0857s; samplesPerSecond = 59755.1
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.07871084 * 5120; err = 0.33652344 * 5120; time = 0.0846s; samplesPerSecond = 60529.4
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.06784954 * 5120; err = 0.33183594 * 5120; time = 0.0845s; samplesPerSecond = 60624.7
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.08440666 * 5120; err = 0.33398438 * 5120; time = 0.0844s; samplesPerSecond = 60644.1
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07466812 * 5120; err = 0.33320312 * 5120; time = 0.0837s; samplesPerSecond = 61205.2
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.05427513 * 5120; err = 0.33125000 * 5120; time = 0.0846s; samplesPerSecond = 60536.6
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06873093 * 5120; err = 0.32773438 * 5120; time = 0.0842s; samplesPerSecond = 60802.5
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08097610 * 5120; err = 0.33007813 * 5120; time = 0.0848s; samplesPerSecond = 60410.8
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.05431290 * 5120; err = 0.32792969 * 5120; time = 0.0849s; samplesPerSecond = 60301.3
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.06173096 * 5120; err = 0.32695313 * 5120; time = 0.0846s; samplesPerSecond = 60505.8
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.04505692 * 5120; err = 0.32792969 * 5120; time = 0.0846s; samplesPerSecond = 60540.1
05/03/2016 14:24:40:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.08151245 * 5120; err = 0.33574219 * 5120; time = 0.0853s; samplesPerSecond = 60024.9
05/03/2016 14:24:41:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.10628204 * 5120; err = 0.33437500 * 5120; time = 0.0846s; samplesPerSecond = 60511.5
05/03/2016 14:24:41:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.05827026 * 5120; err = 0.32636719 * 5120; time = 0.0847s; samplesPerSecond = 60453.6
05/03/2016 14:24:41:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05841064 * 5120; err = 0.33574219 * 5120; time = 0.0848s; samplesPerSecond = 60401.6
05/03/2016 14:24:41:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.04437714 * 5120; err = 0.32773438 * 5120; time = 0.0847s; samplesPerSecond = 60423.7
05/03/2016 14:24:41: Finished Epoch[ 3 of 4]: [Training] ce = 1.07143049 * 81920; err = 0.33178711 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=1.35744s
05/03/2016 14:24:41: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.3'

05/03/2016 14:24:41: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

05/03/2016 14:24:41: Starting minibatch loop.
05/03/2016 14:24:41:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.04450397 * 5120; err = 0.33125000 * 5120; time = 0.0853s; samplesPerSecond = 60048.8
05/03/2016 14:24:41:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.02895847 * 4926; err = 0.31567194 * 4926; time = 0.1426s; samplesPerSecond = 34551.4
05/03/2016 14:24:41:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.00198059 * 5120; err = 0.31601563 * 5120; time = 0.0849s; samplesPerSecond = 60299.9
05/03/2016 14:24:41:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.00561562 * 5120; err = 0.31777344 * 5120; time = 0.0849s; samplesPerSecond = 60288.5
05/03/2016 14:24:41:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.00148926 * 5120; err = 0.31601563 * 5120; time = 0.0846s; samplesPerSecond = 60523.0
05/03/2016 14:24:41:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00593376 * 5120; err = 0.31406250 * 5120; time = 0.0847s; samplesPerSecond = 60451.5
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.98752327 * 5120; err = 0.30722656 * 5120; time = 0.0846s; samplesPerSecond = 60487.9
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.01428757 * 5120; err = 0.31992188 * 5120; time = 0.0850s; samplesPerSecond = 60212.6
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99691544 * 5120; err = 0.31621094 * 5120; time = 0.0849s; samplesPerSecond = 60331.1
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96604996 * 5120; err = 0.30937500 * 5120; time = 0.0849s; samplesPerSecond = 60288.5
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 0.99062958 * 5120; err = 0.30527344 * 5120; time = 0.0850s; samplesPerSecond = 60268.6
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.99886856 * 5120; err = 0.30976562 * 5120; time = 0.0839s; samplesPerSecond = 61030.1
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.00958328 * 5120; err = 0.31523438 * 5120; time = 0.0845s; samplesPerSecond = 60566.6
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97942047 * 5120; err = 0.31171875 * 5120; time = 0.0847s; samplesPerSecond = 60424.4
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.94226837 * 5120; err = 0.30136719 * 5120; time = 0.0846s; samplesPerSecond = 60509.4
05/03/2016 14:24:42:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.96711578 * 5120; err = 0.30175781 * 5120; time = 0.0847s; samplesPerSecond = 60416.5
05/03/2016 14:24:42: Finished Epoch[ 4 of 4]: [Training] ce = 0.99611807 * 81920; err = 0.31303711 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=1.42207s
05/03/2016 14:24:42: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech'
05/03/2016 14:24:42: CNTKCommandTrainEnd: speechTrain

05/03/2016 14:24:42: Action "train" complete.


05/03/2016 14:24:42: ##############################################################################
05/03/2016 14:24:42: #                                                                            #
05/03/2016 14:24:42: # Action "edit"                                                              #
05/03/2016 14:24:42: #                                                                            #
05/03/2016 14:24:42: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


05/03/2016 14:24:43: Action "edit" complete.


05/03/2016 14:24:43: ##############################################################################
05/03/2016 14:24:43: #                                                                            #
05/03/2016 14:24:43: # Action "train"                                                             #
05/03/2016 14:24:43: #                                                                            #
05/03/2016 14:24:43: ##############################################################################

05/03/2016 14:24:43: CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu\TestData\CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames

05/03/2016 14:24:43: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0'.

Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *9]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *9]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *9], [363 x 1], [363 x 1] -> [363 x *9]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *9] -> [512 x *9]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *9] -> [132 x 1 x *9]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *9], [132 x 1 x *9], [132 x 1 x *9] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *9], [132 x 1 x *9] -> [1]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

05/03/2016 14:24:43: Loaded model with 29 nodes on GPU 0.

05/03/2016 14:24:43: Training criterion node(s):
05/03/2016 14:24:43: 	ce = SequenceWithSoftmax

05/03/2016 14:24:43: Evaluation criterion node(s):

05/03/2016 14:24:43: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *9]] [features Gradient[363 x *9]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *9]] [logPrior Gradient[132 x 1]] }
0000005B94F40660: {[features Value[363 x *9]] }
0000005B97758450: {[HL1.W Value[512 x 363]] }
0000005B9C32EB10: {[OL.t Gradient[132 x 1 x *9]] [scaledLogLikelihood Gradient[132 x 1 x *9]] }
0000005B9C32EED0: {[OL.b Gradient[132 x 1]] }
0000005B9C32FA10: {[ce Gradient[1]] }
0000005B9C32FB50: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *9]] [OL.z Gradient[132 x 1 x *9]] }
0000005BFCF66760: {[HL1.b Value[512 x 1]] }
0000005BFCF66D00: {[globalPrior Value[132 x 1]] }
0000005BFCF6CF60: {[globalMean Value[363 x 1]] }
0000005BFCF6D780: {[globalInvStd Value[363 x 1]] }
0000005BFED64600: {[HL2.W Value[512 x 512]] }
0000005BFED646A0: {[labels Value[132 x *9]] }
0000005BFED64880: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *9]] }
0000005BFED649C0: {[HL1.t Value[512 x *9]] }
0000005BFED64D80: {[OL.W Value[132 x 512]] }
0000005BFED64E20: {[err Value[1]] }
0000005BFED64F60: {[scaledLogLikelihood Value[132 x 1 x *9]] }
0000005BFED650A0: {[ce Value[1]] }
0000005BFED65140: {[HL1.z Gradient[512 x 1 x *9]] [HL2.t Value[512 x 1 x *9]] }
0000005BFED651E0: {[HL3.t Gradient[512 x 1 x *9]] [HL3.y Value[512 x 1 x *9]] }
0000005BFED65280: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *9]] [HL3.z Gradient[512 x 1 x *9]] [OL.t Value[132 x 1 x *9]] }
0000005BFED65320: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *9]] }
0000005BFED65460: {[HL2.b Value[512 x 1]] }
0000005BFED65500: {[HL3.W Value[512 x 512]] }
0000005BFED65780: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *9]] }
0000005BFED65820: {[HL3.b Value[512 x 1]] }
0000005BFED65B40: {[logPrior Value[132 x 1]] }
0000005BFED65C80: {[HL1.t Gradient[512 x *9]] [HL1.y Value[512 x 1 x *9]] }
0000005BFED65D20: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *9]] [HL2.z Gradient[512 x 1 x *9]] [HL3.t Value[512 x 1 x *9]] }
0000005BFED65DC0: {[HL2.t Gradient[512 x 1 x *9]] [HL2.y Value[512 x 1 x *9]] }
0000005BFED65E60: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *9]] }
0000005BFED65FA0: {[OL.b Value[132 x 1]] }
0000005BFED66220: {[featNorm Value[363 x *9]] }

05/03/2016 14:24:43: No PreCompute nodes found, skipping PreCompute step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-010
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

05/03/2016 14:24:43: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

05/03/2016 14:24:50: Starting minibatch loop.
dengamma value 1.015387
dengamma value 1.051343
dengamma value 1.072787
dengamma value 1.063094
dengamma value 1.076285
dengamma value 1.095859
dengamma value 0.976427
dengamma value 1.196782
dengamma value 1.015319
dengamma value 1.105372
dengamma value 1.034096
dengamma value 1.155336
dengamma value 1.050430
dengamma value 1.030251
dengamma value 1.107270
dengamma value 1.037082
dengamma value 1.125835
dengamma value 1.042351
dengamma value 1.060958
dengamma value 1.064541
dengamma value 1.000843
05/03/2016 14:24:52:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08358562 * 4628; err = 0.33059637 * 4628; time = 2.5684s; samplesPerSecond = 1801.9
dengamma value 1.099889
dengamma value 1.039597
dengamma value 1.115739
dengamma value 0.957857
dengamma value 1.122265
dengamma value 1.111263
dengamma value 1.065039
dengamma value 1.066569
dengamma value 0.987920
dengamma value 1.044044
dengamma value 1.004607
dengamma value 1.062701
dengamma value 1.035316
dengamma value 1.026637
dengamma value 1.144678
dengamma value 1.165378
dengamma value 1.132944
dengamma value 1.081424
dengamma value 1.108471
dengamma value 1.075180
dengamma value 1.088663
dengamma value 1.078510
05/03/2016 14:24:54:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08214612 * 5946; err = 0.31449714 * 5946; time = 1.2532s; samplesPerSecond = 4744.6
dengamma value 1.048358
dengamma value 1.103563
dengamma value 1.099066
dengamma value 1.136579
dengamma value 1.077474
dengamma value 1.060762
dengamma value 1.101553
dengamma value 1.107761
dengamma value 1.022390
dengamma value 1.081607
dengamma value 1.010834
dengamma value 1.077299
dengamma value 1.083294
dengamma value 1.126290
dengamma value 1.066091
dengamma value 1.078891
dengamma value 1.046375
dengamma value 0.986374
dengamma value 1.084432
dengamma value 1.116274
dengamma value 1.091088
dengamma value 1.074867
05/03/2016 14:24:55:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08367420 * 5916; err = 0.32437458 * 5916; time = 1.0670s; samplesPerSecond = 5544.5
dengamma value 1.030191
dengamma value 1.047688
dengamma value 1.041505
dengamma value 1.137154
dengamma value 1.101086
dengamma value 1.032440
dengamma value 1.077590
dengamma value 1.102024
dengamma value 1.104773
dengamma value 1.157433
dengamma value 1.050714
dengamma value 1.003643
dengamma value 1.107600
dengamma value 1.107183
dengamma value 0.954740
dengamma value 1.009407
dengamma value 1.066747
dengamma value 1.065564
dengamma value 1.048961
dengamma value 1.026667
dengamma value 1.096718
dengamma value 1.068000
05/03/2016 14:24:56:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08031854 * 6386; err = 0.32680864 * 6386; time = 1.1519s; samplesPerSecond = 5544.0
dengamma value 1.044291
dengamma value 1.082913
dengamma value 1.073895
dengamma value 1.133403
dengamma value 1.078159
dengamma value 1.102888
dengamma value 1.082325
dengamma value 1.099348
dengamma value 1.167385
dengamma value 1.058515
dengamma value 1.078929
dengamma value 1.050607
dengamma value 1.091514
dengamma value 1.052261
dengamma value 1.109094
dengamma value 1.121752
dengamma value 1.057149
dengamma value 1.042336
dengamma value 1.051527
dengamma value 0.977551
dengamma value 1.104787
dengamma value 1.159152
dengamma value 1.068097
05/03/2016 14:24:57:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08171638 * 6734; err = 0.29551530 * 6734; time = 1.2255s; samplesPerSecond = 5495.1
dengamma value 1.086780
dengamma value 1.130269
dengamma value 1.073455
dengamma value 1.091998
dengamma value 1.035754
dengamma value 0.985550
dengamma value 1.059557
dengamma value 1.123901
dengamma value 1.083224
dengamma value 1.059212
dengamma value 1.097843
dengamma value 1.036471
dengamma value 1.081966
dengamma value 0.984412
dengamma value 1.105340
dengamma value 1.047261
dengamma value 1.033234
dengamma value 1.141741
dengamma value 1.066171
dengamma value 1.077317
dengamma value 1.127820
dengamma value 1.077791
dengamma value 1.080292
dengamma value 0.981080
05/03/2016 14:24:58:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08549177 * 6202; err = 0.31360851 * 6202; time = 1.1406s; samplesPerSecond = 5437.4
dengamma value 1.130581
dengamma value 1.117521
dengamma value 1.055970
dengamma value 1.068845
dengamma value 1.151987
dengamma value 1.030249
dengamma value 1.106854
dengamma value 1.101331
dengamma value 1.060793
dengamma value 1.107020
dengamma value 1.105106
dengamma value 1.079484
dengamma value 1.108909
dengamma value 1.004101
dengamma value 0.950285
dengamma value 1.125628
dengamma value 1.085928
dengamma value 1.042281
dengamma value 1.065240
dengamma value 1.049562
dengamma value 1.073483
dengamma value 1.089740
dengamma value 1.053381
dengamma value 1.018081
05/03/2016 14:24:59:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07997486 * 6362; err = 0.32568375 * 6362; time = 1.1931s; samplesPerSecond = 5332.4
dengamma value 1.075601
dengamma value 1.083480
dengamma value 1.123550
dengamma value 1.028559
dengamma value 1.061858
dengamma value 1.100839
dengamma value 1.023886
dengamma value 1.043098
dengamma value 1.025118
dengamma value 1.136169
dengamma value 1.116578
dengamma value 1.054926
dengamma value 1.064200
dengamma value 1.068297
dengamma value 1.059706
dengamma value 1.095217
dengamma value 1.115496
dengamma value 1.135736
dengamma value 1.060187
dengamma value 1.092560
dengamma value 1.066627
05/03/2016 14:25:00:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.07502958 * 5608; err = 0.31847361 * 5608; time = 0.9897s; samplesPerSecond = 5666.6
dengamma value 1.063055
dengamma value 1.042724
dengamma value 1.059763
dengamma value 1.021321
dengamma value 1.097744
dengamma value 1.028584
dengamma value 1.103968
dengamma value 0.997650
dengamma value 1.112675
dengamma value 1.092727
dengamma value 1.043912
dengamma value 1.129032
dengamma value 1.116614
dengamma value 1.042492
dengamma value 1.061597
dengamma value 1.015698
dengamma value 1.053582
dengamma value 1.033995
dengamma value 1.093077
dengamma value 1.080365
dengamma value 1.104077
dengamma value 1.044763
dengamma value 1.020228
05/03/2016 14:25:02:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08151798 * 6594; err = 0.33651805 * 6594; time = 1.2714s; samplesPerSecond = 5186.5
dengamma value 1.028670
dengamma value 0.886836
dengamma value 1.157957
dengamma value 1.014026
dengamma value 1.116295
dengamma value 1.039071
dengamma value 1.099671
dengamma value 1.101143
dengamma value 1.113298
dengamma value 1.109584
dengamma value 1.079916
dengamma value 1.063747
dengamma value 1.095497
dengamma value 1.076887
dengamma value 1.141736
dengamma value 1.035007
dengamma value 1.117461
dengamma value 1.135798
dengamma value 1.057881
dengamma value 1.067153
dengamma value 1.072834
dengamma value 1.006985
dengamma value 1.110780
05/03/2016 14:25:03:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08032955 * 6364; err = 0.31615336 * 6364; time = 1.3043s; samplesPerSecond = 4879.4
dengamma value 1.061657
dengamma value 1.052374
dengamma value 0.987764
dengamma value 1.062911
dengamma value 1.061221
dengamma value 1.142910
dengamma value 1.130698
dengamma value 1.082887
dengamma value 1.044966
dengamma value 1.143583
dengamma value 1.073300
dengamma value 1.157146
dengamma value 1.095505
dengamma value 1.008495
dengamma value 1.116105
dengamma value 1.059661
dengamma value 1.161594
dengamma value 1.098735
dengamma value 1.146980
dengamma value 1.140211
dengamma value 1.096159
dengamma value 1.051981
dengamma value 1.046266
dengamma value 1.118023
dengamma value 1.042709
dengamma value 1.001388
dengamma value 1.012911
05/03/2016 14:25:04:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08613263 * 6536; err = 0.31548348 * 6536; time = 1.2198s; samplesPerSecond = 5358.4
dengamma value 1.087017
dengamma value 1.059792
dengamma value 1.046501
dengamma value 1.073579
dengamma value 1.095312
dengamma value 1.055991
dengamma value 1.085752
dengamma value 1.068742
dengamma value 1.091071
dengamma value 1.032232
dengamma value 1.084744
dengamma value 1.105438
dengamma value 1.041391
dengamma value 0.937120
dengamma value 1.102653
dengamma value 1.053025
dengamma value 1.061745
dengamma value 1.016491
dengamma value 1.044234
dengamma value 1.025076
dengamma value 1.032975
05/03/2016 14:25:05:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08295645 * 6208; err = 0.31765464 * 6208; time = 1.2395s; samplesPerSecond = 5008.3
dengamma value 1.085692
dengamma value 1.075794
dengamma value 1.118478
dengamma value 1.067378
dengamma value 0.945208
dengamma value 1.050391
dengamma value 1.078381
dengamma value 1.099215
dengamma value 1.086438
dengamma value 1.049932
dengamma value 1.063138
dengamma value 1.130924
dengamma value 1.028988
dengamma value 1.191101
dengamma value 1.027579
dengamma value 1.088230
dengamma value 1.089141
dengamma value 1.083345
dengamma value 1.072232
dengamma value 1.090651
dengamma value 1.135706
dengamma value 1.127723
05/03/2016 14:25:07:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08117240 * 6326; err = 0.29687006 * 6326; time = 1.2128s; samplesPerSecond = 5216.1
dengamma value 1.085506
dengamma value 1.071655
dengamma value 1.116561
dengamma value 1.135969
dengamma value 1.003626
dengamma value 1.027538
dengamma value 1.104102
05/03/2016 14:25:07: Finished Epoch[ 1 of 3]: [Training] ce = 0.08156705 * 81936; err = 0.31603935 * 81936; totalSamplesSeen = 81936; learningRatePerSample = 2e-006; epochTime=23.9178s
05/03/2016 14:25:07: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.1'

05/03/2016 14:25:07: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81936), data subset 0 of 1, with 1 datapasses

05/03/2016 14:25:07: Starting minibatch loop.
dengamma value 1.145242
dengamma value 1.085012
dengamma value 1.160179
dengamma value 1.053052
dengamma value 1.116928
dengamma value 1.056254
dengamma value 1.095588
dengamma value 1.143948
dengamma value 1.044307
dengamma value 1.043959
dengamma value 1.020891
dengamma value 1.092130
dengamma value 1.081158
dengamma value 1.091468
dengamma value 1.055750
dengamma value 1.017632
dengamma value 1.057541
dengamma value 1.110858
dengamma value 1.063133
dengamma value 1.071745
dengamma value 1.083208
dengamma value 1.134414
dengamma value 1.035728
dengamma value 1.045233
05/03/2016 14:25:08:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08368398 * 6182; err = 0.30071174 * 6182; time = 1.3083s; samplesPerSecond = 4725.1
dengamma value 1.046536
dengamma value 1.004586
dengamma value 1.288805
dengamma value 1.035090
dengamma value 1.095160
dengamma value 1.119750
dengamma value 1.107882
dengamma value 1.040208
dengamma value 1.045552
dengamma value 1.080889
dengamma value 1.044524
dengamma value 1.125896
dengamma value 1.135680
dengamma value 1.026725
dengamma value 1.036926
dengamma value 1.092974
dengamma value 1.024887
dengamma value 1.115769
dengamma value 1.044357
dengamma value 1.026637
dengamma value 1.048265
dengamma value 1.113920
05/03/2016 14:25:10:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08501332 * 5736; err = 0.28417015 * 5736; time = 1.1740s; samplesPerSecond = 4885.7
dengamma value 1.010748
dengamma value 1.110666
dengamma value 1.085324
dengamma value 1.081510
dengamma value 1.057740
dengamma value 1.163440
dengamma value 1.024172
dengamma value 1.126877
dengamma value 1.054233
dengamma value 1.054708
dengamma value 1.047825
dengamma value 1.056014
dengamma value 1.107507
dengamma value 1.188773
dengamma value 1.061890
dengamma value 1.168044
dengamma value 1.112633
dengamma value 1.068623
dengamma value 1.072696
dengamma value 1.098664
dengamma value 0.974260
dengamma value 0.964481
dengamma value 1.066706
05/03/2016 14:25:11:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08160519 * 6244; err = 0.30172966 * 6244; time = 1.2317s; samplesPerSecond = 5069.4
dengamma value 1.105943
dengamma value 1.116022
dengamma value 1.088509
dengamma value 1.058934
dengamma value 1.161254
dengamma value 1.097798
dengamma value 1.077121
dengamma value 1.044863
dengamma value 1.070741
dengamma value 1.016574
dengamma value 1.093447
dengamma value 1.126311
dengamma value 1.119959
dengamma value 1.129718
dengamma value 1.017900
dengamma value 1.132437
dengamma value 1.024505
dengamma value 1.020736
dengamma value 1.078649
dengamma value 1.095957
dengamma value 1.025882
dengamma value 1.065037
dengamma value 1.176504
dengamma value 1.076837
dengamma value 1.056695
05/03/2016 14:25:12:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08490154 * 6280; err = 0.31242038 * 6280; time = 1.1698s; samplesPerSecond = 5368.3
dengamma value 1.018347
dengamma value 1.082831
dengamma value 1.075497
dengamma value 1.085046
dengamma value 1.063649
dengamma value 1.074264
dengamma value 1.067420
dengamma value 1.053149
dengamma value 1.117731
dengamma value 1.072480
dengamma value 1.076274
dengamma value 1.046603
dengamma value 1.115489
dengamma value 1.067307
dengamma value 1.103777
dengamma value 1.085866
dengamma value 1.144046
dengamma value 1.088565
dengamma value 1.087098
dengamma value 1.063796
dengamma value 1.100174
dengamma value 1.084196
dengamma value 1.102490
dengamma value 1.084687
dengamma value 1.092789
dengamma value 1.103272
05/03/2016 14:25:14:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.07733350 * 7428; err = 0.29806139 * 7428; time = 1.5047s; samplesPerSecond = 4936.4
dengamma value 1.142144
dengamma value 1.081073
dengamma value 1.051410
dengamma value 1.134920
dengamma value 1.054180
dengamma value 1.033419
dengamma value 1.037870
dengamma value 1.091124
dengamma value 1.059701
dengamma value 1.061054
dengamma value 1.035654
dengamma value 1.057788
dengamma value 1.026473
dengamma value 1.151943
dengamma value 1.083800
dengamma value 1.082665
dengamma value 1.089776
dengamma value 0.975704
dengamma value 1.056808
dengamma value 1.071527
dengamma value 1.119046
dengamma value 1.022266
dengamma value 1.057294
05/03/2016 14:25:15:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08150286 * 6994; err = 0.31855876 * 6994; time = 1.2279s; samplesPerSecond = 5695.9
dengamma value 1.113186
dengamma value 1.079921
dengamma value 1.175955
dengamma value 1.024600
dengamma value 1.053710
dengamma value 1.130235
dengamma value 0.975800
dengamma value 1.141175
dengamma value 1.185706
dengamma value 1.065715
dengamma value 1.152167
dengamma value 1.111953
dengamma value 1.060055
dengamma value 1.027962
dengamma value 1.074742
dengamma value 1.093271
dengamma value 1.048562
dengamma value 1.041243
dengamma value 1.095344
dengamma value 1.088717
dengamma value 1.070026
dengamma value 1.152818
dengamma value 1.080982
dengamma value 1.101065
05/03/2016 14:25:16:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07725126 * 6572; err = 0.30340840 * 6572; time = 1.3081s; samplesPerSecond = 5023.9
dengamma value 1.097197
dengamma value 1.021022
dengamma value 1.078317
dengamma value 1.085192
dengamma value 1.072533
dengamma value 1.117073
dengamma value 1.200122
dengamma value 1.072118
dengamma value 1.074173
dengamma value 1.060047
dengamma value 1.006856
dengamma value 1.043448
dengamma value 1.018329
dengamma value 1.178683
dengamma value 1.084694
dengamma value 0.941980
dengamma value 1.006588
dengamma value 1.032836
dengamma value 1.074308
dengamma value 1.085413
dengamma value 1.216896
dengamma value 1.015795
05/03/2016 14:25:17:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08773964 * 5506; err = 0.32909553 * 5506; time = 1.0242s; samplesPerSecond = 5376.1
dengamma value 1.002740
dengamma value 1.074720
dengamma value 1.015641
dengamma value 1.082433
dengamma value 1.124583
dengamma value 1.121300
dengamma value 1.129818
dengamma value 1.078518
dengamma value 1.040683
dengamma value 1.013661
dengamma value 1.096474
dengamma value 0.958585
dengamma value 1.076417
dengamma value 1.027448
dengamma value 1.066752
dengamma value 1.115725
dengamma value 1.114337
dengamma value 1.090009
dengamma value 1.107314
dengamma value 1.006038
dengamma value 1.001244
05/03/2016 14:25:18:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08559576 * 5628; err = 0.33688699 * 5628; time = 1.0428s; samplesPerSecond = 5396.8
dengamma value 1.039761
dengamma value 1.123938
dengamma value 1.071136
dengamma value 1.098651
dengamma value 1.086918
dengamma value 0.986629
dengamma value 1.035707
dengamma value 1.000715
dengamma value 1.104032
dengamma value 1.055806
dengamma value 1.166729
dengamma value 1.092039
dengamma value 1.083523
dengamma value 1.073980
dengamma value 1.020984
dengamma value 1.043832
dengamma value 1.045460
dengamma value 1.060372
dengamma value 1.059040
dengamma value 1.135645
dengamma value 1.018242
dengamma value 1.030202
dengamma value 1.055369
dengamma value 0.988204
05/03/2016 14:25:19:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08217202 * 6032; err = 0.32625995 * 6032; time = 1.0739s; samplesPerSecond = 5616.9
dengamma value 1.040056
dengamma value 1.059124
dengamma value 1.058682
dengamma value 1.091049
dengamma value 1.067363
dengamma value 1.119150
dengamma value 1.081032
dengamma value 0.965229
dengamma value 1.122860
dengamma value 1.133967
dengamma value 1.037526
dengamma value 0.968076
dengamma value 1.059440
dengamma value 1.140410
dengamma value 1.086110
dengamma value 0.997839
dengamma value 1.165703
dengamma value 1.048398
dengamma value 1.104210
dengamma value 1.028737
05/03/2016 14:25:20:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08802488 * 4790; err = 0.33799582 * 4790; time = 0.9239s; samplesPerSecond = 5184.6
dengamma value 1.015772
dengamma value 0.967842
dengamma value 1.055026
dengamma value 1.060577
dengamma value 1.081279
dengamma value 1.012655
dengamma value 1.010069
dengamma value 1.089346
dengamma value 1.075223
dengamma value 0.957264
dengamma value 1.129906
dengamma value 1.057574
dengamma value 1.077032
dengamma value 1.016718
dengamma value 1.128840
dengamma value 1.068581
dengamma value 1.133209
dengamma value 1.167371
dengamma value 0.961193
dengamma value 1.143450
dengamma value 1.034745
dengamma value 1.034677
05/03/2016 14:25:21:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08562215 * 5986; err = 0.34513866 * 5986; time = 1.0750s; samplesPerSecond = 5568.3
dengamma value 1.027943
dengamma value 1.006940
dengamma value 1.054680
dengamma value 1.089151
dengamma value 1.029769
dengamma value 1.217526
dengamma value 1.101724
dengamma value 1.113607
dengamma value 1.074514
dengamma value 1.081673
dengamma value 1.094728
dengamma value 1.087523
dengamma value 1.003731
dengamma value 0.994219
dengamma value 1.028819
dengamma value 1.075156
dengamma value 1.049089
dengamma value 1.053536
dengamma value 1.120696
dengamma value 1.045499
dengamma value 1.108837
dengamma value 1.059376
dengamma value 1.091137
dengamma value 0.994257
05/03/2016 14:25:22:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08639331 * 7022; err = 0.31444033 * 7022; time = 1.2576s; samplesPerSecond = 5583.8
dengamma value 1.034703
dengamma value 1.093763
dengamma value 1.188881
dengamma value 1.072499
dengamma value 1.043478
dengamma value 1.085861
dengamma value 1.015125
dengamma value 1.085830
dengamma value 1.085830
05/03/2016 14:25:23: Finished Epoch[ 2 of 3]: [Training] ce = 0.08332526 * 82462; err = 0.31518760 * 82462; totalSamplesSeen = 164398; learningRatePerSample = 2e-006; epochTime=15.7118s
05/03/2016 14:25:23: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.2'

05/03/2016 14:25:23: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 164102), data subset 0 of 1, with 1 datapasses

05/03/2016 14:25:23: Starting minibatch loop.
dengamma value 1.088197
dengamma value 1.100273
dengamma value 1.084374
dengamma value 1.104666
dengamma value 1.080939
dengamma value 1.060389
dengamma value 1.150334
dengamma value 1.039812
dengamma value 1.112423
dengamma value 0.985965
dengamma value 1.117210
dengamma value 1.071346
dengamma value 1.071000
dengamma value 1.100134
dengamma value 1.077029
dengamma value 1.060612
dengamma value 1.105358
dengamma value 1.011032
dengamma value 1.095015
dengamma value 1.093480
dengamma value 1.050025
dengamma value 1.071393
dengamma value 1.042980
dengamma value 1.039123
05/03/2016 14:25:24:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08358906 * 6292; err = 0.30101716 * 6292; time = 1.1505s; samplesPerSecond = 5469.1
dengamma value 1.160757
dengamma value 1.035082
dengamma value 0.988546
dengamma value 1.101547
dengamma value 1.082563
dengamma value 1.025480
dengamma value 1.047410
dengamma value 1.136445
dengamma value 1.040094
dengamma value 0.976965
dengamma value 0.961430
dengamma value 1.100491
dengamma value 1.068102
dengamma value 1.070083
dengamma value 1.033161
dengamma value 1.068287
dengamma value 1.011897
dengamma value 1.077012
dengamma value 1.112298
dengamma value 1.034657
dengamma value 1.074293
dengamma value 1.025731
05/03/2016 14:25:25:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07963296 * 6596; err = 0.33110976 * 6596; time = 1.2415s; samplesPerSecond = 5312.8
dengamma value 1.085887
dengamma value 1.006123
dengamma value 1.100072
dengamma value 1.118208
dengamma value 1.071416
dengamma value 1.167362
dengamma value 1.031790
dengamma value 1.079441
dengamma value 1.055116
dengamma value 1.062575
dengamma value 1.106028
dengamma value 1.058904
dengamma value 1.022302
dengamma value 1.037796
dengamma value 1.120612
dengamma value 0.954849
dengamma value 1.048381
dengamma value 1.034218
dengamma value 1.031126
dengamma value 1.173674
dengamma value 1.070757
dengamma value 1.203380
05/03/2016 14:25:26:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08600030 * 5666; err = 0.29844688 * 5666; time = 1.1112s; samplesPerSecond = 5099.2
dengamma value 1.131507
dengamma value 1.027065
dengamma value 1.060685
dengamma value 1.093865
dengamma value 1.081543
dengamma value 1.078910
dengamma value 1.105020
dengamma value 1.070809
dengamma value 1.094553
dengamma value 1.076389
dengamma value 1.105419
dengamma value 1.050209
dengamma value 1.002027
dengamma value 1.128992
dengamma value 1.124045
dengamma value 1.039761
dengamma value 1.086744
dengamma value 1.040163
dengamma value 1.089048
dengamma value 1.037200
dengamma value 1.085009
dengamma value 1.031806
05/03/2016 14:25:28:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08130102 * 6626; err = 0.28327800 * 6626; time = 1.2686s; samplesPerSecond = 5223.0
dengamma value 1.094679
dengamma value 1.037562
dengamma value 1.033161
dengamma value 1.093461
dengamma value 1.125289
dengamma value 1.151998
dengamma value 1.093540
dengamma value 1.099577
dengamma value 1.029092
dengamma value 1.087039
dengamma value 1.145239
dengamma value 1.075084
dengamma value 1.043946
dengamma value 1.067017
dengamma value 1.024815
dengamma value 1.064146
dengamma value 1.083589
dengamma value 1.064345
dengamma value 1.111978
dengamma value 1.106094
dengamma value 1.074090
dengamma value 1.080812
dengamma value 1.068420
dengamma value 1.059111
05/03/2016 14:25:29:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08144237 * 5652; err = 0.30042463 * 5652; time = 1.2146s; samplesPerSecond = 4653.4
dengamma value 1.092134
dengamma value 1.047652
dengamma value 1.002004
dengamma value 1.048934
dengamma value 1.037261
dengamma value 1.150848
dengamma value 1.047778
dengamma value 1.048185
dengamma value 1.106731
dengamma value 1.022934
dengamma value 1.029545
dengamma value 1.055188
dengamma value 1.074436
dengamma value 1.073219
dengamma value 1.089149
dengamma value 1.040301
dengamma value 1.009003
dengamma value 1.036303
dengamma value 1.098994
dengamma value 1.043438
dengamma value 1.056252
05/03/2016 14:25:30:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08578500 * 6588; err = 0.33986035 * 6588; time = 1.2243s; samplesPerSecond = 5380.9
dengamma value 1.065623
dengamma value 1.096644
dengamma value 1.041698
dengamma value 0.983649
dengamma value 1.153678
dengamma value 1.073628
dengamma value 1.081920
dengamma value 1.037372
dengamma value 1.007501
dengamma value 1.093147
dengamma value 1.082585
dengamma value 1.026000
dengamma value 1.028268
dengamma value 1.123083
dengamma value 1.135424
dengamma value 1.056123
dengamma value 1.042214
dengamma value 1.079012
dengamma value 1.108924
dengamma value 1.058956
dengamma value 1.028283
05/03/2016 14:25:31:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08631517 * 6328; err = 0.30941846 * 6328; time = 1.2583s; samplesPerSecond = 5029.0
dengamma value 1.072575
dengamma value 1.097221
dengamma value 1.011787
dengamma value 1.103157
dengamma value 1.126538
dengamma value 1.070615
dengamma value 1.089780
dengamma value 1.109849
dengamma value 1.052070
dengamma value 0.991507
dengamma value 1.060412
dengamma value 1.032441
dengamma value 1.022123
dengamma value 1.104983
dengamma value 1.048826
dengamma value 1.083207
dengamma value 1.072539
dengamma value 1.192497
dengamma value 1.086666
dengamma value 1.062849
dengamma value 1.065173
dengamma value 1.108937
dengamma value 1.036068
dengamma value 1.100262
dengamma value 1.077704
05/03/2016 14:25:33:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08290494 * 6980; err = 0.28839542 * 6980; time = 1.3218s; samplesPerSecond = 5280.7
dengamma value 1.050160
dengamma value 0.978864
dengamma value 0.951252
dengamma value 1.137689
dengamma value 1.125302
dengamma value 1.023794
dengamma value 1.281458
dengamma value 1.102675
dengamma value 1.066576
dengamma value 1.116553
dengamma value 0.979398
dengamma value 1.075125
dengamma value 1.059821
dengamma value 1.015062
dengamma value 1.119501
dengamma value 1.061463
dengamma value 1.002527
dengamma value 1.284400
dengamma value 1.185396
dengamma value 1.049221
dengamma value 1.136681
dengamma value 0.981109
dengamma value 1.022417
05/03/2016 14:25:34:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.07985366 * 6774; err = 0.31251845 * 6774; time = 1.1218s; samplesPerSecond = 6038.4
dengamma value 0.991020
dengamma value 1.080265
dengamma value 1.125530
dengamma value 1.028282
dengamma value 0.931542
dengamma value 1.073510
dengamma value 1.040909
dengamma value 1.068603
dengamma value 0.930334
dengamma value 1.068783
dengamma value 1.012925
dengamma value 1.095008
dengamma value 1.032397
dengamma value 0.980707
dengamma value 1.071785
dengamma value 0.933892
dengamma value 1.054113
dengamma value 1.016794
dengamma value 1.071612
dengamma value 1.069752
dengamma value 1.103919
dengamma value 1.082665
05/03/2016 14:25:35:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08853478 * 6146; err = 0.35388871 * 6146; time = 1.2076s; samplesPerSecond = 5089.6
dengamma value 1.017763
dengamma value 1.131387
dengamma value 1.059095
dengamma value 1.147637
dengamma value 0.959775
dengamma value 1.082524
dengamma value 1.071139
dengamma value 1.150287
dengamma value 1.107908
dengamma value 1.082605
dengamma value 1.038306
dengamma value 1.014547
dengamma value 1.165069
dengamma value 1.063468
dengamma value 1.117538
dengamma value 1.068459
dengamma value 1.086082
dengamma value 1.095840
dengamma value 1.014370
dengamma value 1.139207
dengamma value 1.066824
dengamma value 1.079760
05/03/2016 14:25:36:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08208266 * 5366; err = 0.30301901 * 5366; time = 1.0536s; samplesPerSecond = 5092.8
dengamma value 1.061395
dengamma value 1.047785
dengamma value 1.090933
dengamma value 1.070985
dengamma value 1.064221
dengamma value 1.015890
dengamma value 1.125446
dengamma value 1.071238
dengamma value 1.019477
dengamma value 1.031274
dengamma value 1.034352
dengamma value 1.063972
dengamma value 1.058113
dengamma value 1.040602
dengamma value 1.091374
dengamma value 1.086139
dengamma value 1.112650
dengamma value 1.103647
dengamma value 1.057306
dengamma value 0.988464
dengamma value 1.036586
dengamma value 1.127764
dengamma value 1.148842
dengamma value 1.096221
dengamma value 1.088360
05/03/2016 14:25:37:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08114245 * 6220; err = 0.32250804 * 6220; time = 1.1265s; samplesPerSecond = 5521.7
dengamma value 1.082362
dengamma value 1.051853
dengamma value 1.113975
dengamma value 1.084998
dengamma value 1.030133
dengamma value 1.070156
dengamma value 1.075748
dengamma value 1.107123
dengamma value 1.086904
dengamma value 1.084355
dengamma value 1.043573
dengamma value 1.068356
dengamma value 1.032575
dengamma value 1.093060
dengamma value 1.160387
dengamma value 1.106279
dengamma value 1.066269
dengamma value 0.998378
dengamma value 1.082442
dengamma value 1.033566
dengamma value 1.083995
dengamma value 1.100963
dengamma value 1.142857
05/03/2016 14:25:38:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08458937 * 6564; err = 0.30042657 * 6564; time = 1.2004s; samplesPerSecond = 5468.2
05/03/2016 14:25:38: Finished Epoch[ 3 of 3]: [Training] ce = 0.08329045 * 81798; err = 0.31113230 * 81798; totalSamplesSeen = 246196; learningRatePerSample = 2e-006; epochTime=15.5029s
05/03/2016 14:25:38: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence'
05/03/2016 14:25:39: CNTKCommandTrainEnd: sequenceTrain

05/03/2016 14:25:39: Action "train" complete.

05/03/2016 14:25:39: __COMPLETED__
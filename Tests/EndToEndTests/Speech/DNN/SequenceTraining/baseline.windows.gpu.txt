CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3565 @ 3.20GHz
    Hardware threads: 8
    Total Memory: 12580436 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug 16 2016 02:54:53
		Last modified date: Fri Aug 12 05:31:21 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: c:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
		Built by svcphil on Philly-Pool3
		Build Path: c:\Jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData
08/16/2016 03:05:00: -------------------------------------------------------------------
08/16/2016 03:05:00: Build info: 

08/16/2016 03:05:00: 		Built time: Aug 16 2016 02:54:53
08/16/2016 03:05:00: 		Last modified date: Fri Aug 12 05:31:21 2016
08/16/2016 03:05:00: 		Build type: Release
08/16/2016 03:05:00: 		Build target: GPU
08/16/2016 03:05:00: 		With 1bit-SGD: no
08/16/2016 03:05:00: 		Math lib: mkl
08/16/2016 03:05:00: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
08/16/2016 03:05:00: 		CUB_PATH: c:\src\cub-1.4.1
08/16/2016 03:05:00: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
08/16/2016 03:05:00: 		Build Branch: HEAD
08/16/2016 03:05:00: 		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
08/16/2016 03:05:00: 		Built by svcphil on Philly-Pool3
08/16/2016 03:05:00: 		Build Path: c:\Jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
08/16/2016 03:05:00: -------------------------------------------------------------------
08/16/2016 03:05:00: -------------------------------------------------------------------
08/16/2016 03:05:00: GPU info:

08/16/2016 03:05:00: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
08/16/2016 03:05:00: -------------------------------------------------------------------

08/16/2016 03:05:00: Running on cntk-muc01 at 2016/08/16 03:05:00
08/16/2016 03:05:00: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu  DeviceId=0  timestamping=true



08/16/2016 03:05:00: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/16/2016 03:05:00: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "$RunDir$/models/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.sequence.0"
    editPath  = "$ConfigDir$/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "$DataDir$/model.overalltying"
            transpFile = "$DataDir$/model.transprob"
        ]
        lattices = [
            denlatTocFile = "$DataDir$/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 03:05:00: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/16/2016 03:05:00: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/16/2016 03:05:00: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 03:05:00: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/16/2016 03:05:00: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
08/16/2016 03:05:00: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/16/2016 03:05:00: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
08/16/2016 03:05:00: Precision = "float"
08/16/2016 03:05:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech
08/16/2016 03:05:00: CNTKCommandTrainInfo: dptPre1 : 2
08/16/2016 03:05:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech
08/16/2016 03:05:00: CNTKCommandTrainInfo: dptPre2 : 2
08/16/2016 03:05:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech
08/16/2016 03:05:00: CNTKCommandTrainInfo: speechTrain : 4
08/16/2016 03:05:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence
08/16/2016 03:05:00: CNTKCommandTrainInfo: sequenceTrain : 3
08/16/2016 03:05:00: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11

08/16/2016 03:05:00: ##############################################################################
08/16/2016 03:05:00: #                                                                            #
08/16/2016 03:05:00: # Action "train"                                                             #
08/16/2016 03:05:00: #                                                                            #
08/16/2016 03:05:00: ##############################################################################

08/16/2016 03:05:00: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 03:05:01: Creating virgin network.
Node 'globalMean' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalInvStd' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalPrior' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- 0.000000.
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- 0.000000.
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- uniform(seed=1, range=0.050000*1.000000, onCPU=false).
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=2, range=0.050000*1.000000, onCPU=false).
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- uniform(seed=3, range=0.050000*1.000000, onCPU=false).
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- uniform(seed=4, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 03:05:02: Created model with 19 nodes on GPU 0.

08/16/2016 03:05:02: Training criterion node(s):
08/16/2016 03:05:02: 	ce = CrossEntropyWithSoftmax

08/16/2016 03:05:02: Evaluation criterion node(s):
08/16/2016 03:05:02: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 29 matrices, 11 are shared as 5, and 18 are not shared.

	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *] }
	{ HL1.t : [512 x *] (gradient)
	  HL1.y : [512 x 1 x *] }
	{ HL1.z : [512 x 1 x *] (gradient)
	  OL.t : [132 x 1 x *] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] (gradient) }


08/16/2016 03:05:02: Training 254084 parameters in 4 out of 4 parameter tensors and 10 nodes with gradient:

08/16/2016 03:05:02: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 03:05:02: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:02: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 03:05:02: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 03:05:02: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 03:05:02: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 03:05:02: Starting minibatch loop.
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.89978218 * 2560; err = 0.84375000 * 2560; time = 0.1752s; samplesPerSecond = 14612.9
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.96755714 * 2560; err = 0.72031250 * 2560; time = 0.0206s; samplesPerSecond = 124091.1
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55723495 * 2560; err = 0.65859375 * 2560; time = 0.0206s; samplesPerSecond = 124326.2
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.29642715 * 2560; err = 0.61992187 * 2560; time = 0.0207s; samplesPerSecond = 123522.3
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.02396469 * 2560; err = 0.55117187 * 2560; time = 0.0213s; samplesPerSecond = 120216.0
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87309265 * 2560; err = 0.51484375 * 2560; time = 0.0205s; samplesPerSecond = 124598.5
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.78157196 * 2560; err = 0.50507813 * 2560; time = 0.0206s; samplesPerSecond = 124477.3
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.75391235 * 2560; err = 0.50781250 * 2560; time = 0.0206s; samplesPerSecond = 124519.7
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.66460266 * 2560; err = 0.45742187 * 2560; time = 0.0205s; samplesPerSecond = 124756.3
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62184296 * 2560; err = 0.47968750 * 2560; time = 0.0205s; samplesPerSecond = 124908.5
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.65328217 * 2560; err = 0.47265625 * 2560; time = 0.0205s; samplesPerSecond = 124908.5
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.50686951 * 2560; err = 0.44921875 * 2560; time = 0.0205s; samplesPerSecond = 124798.9
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.46723938 * 2560; err = 0.42304687 * 2560; time = 0.0205s; samplesPerSecond = 125012.2
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.49163513 * 2560; err = 0.44140625 * 2560; time = 0.0205s; samplesPerSecond = 124993.9
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46437683 * 2560; err = 0.43398437 * 2560; time = 0.0204s; samplesPerSecond = 125220.1
08/16/2016 03:05:02:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.43047485 * 2560; err = 0.43867187 * 2560; time = 0.0205s; samplesPerSecond = 124878.0
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.42106018 * 2560; err = 0.41992188 * 2560; time = 0.0205s; samplesPerSecond = 124945.1
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.46538086 * 2560; err = 0.42421875 * 2560; time = 0.0213s; samplesPerSecond = 120289.4
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.47427673 * 2560; err = 0.44062500 * 2560; time = 0.0255s; samplesPerSecond = 100569.6
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42847290 * 2560; err = 0.44023438 * 2560; time = 0.0262s; samplesPerSecond = 97646.6
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.34078369 * 2560; err = 0.41171875 * 2560; time = 0.0252s; samplesPerSecond = 101559.1
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.39474487 * 2560; err = 0.42734375 * 2560; time = 0.0259s; samplesPerSecond = 98754.0
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.40151062 * 2560; err = 0.41250000 * 2560; time = 0.0258s; samplesPerSecond = 99194.0
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.39345703 * 2560; err = 0.42734375 * 2560; time = 0.0260s; samplesPerSecond = 98518.4
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.32485046 * 2560; err = 0.40156250 * 2560; time = 0.0256s; samplesPerSecond = 100113.4
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27032471 * 2560; err = 0.39765625 * 2560; time = 0.0256s; samplesPerSecond = 100156.5
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.32375488 * 2560; err = 0.39257813 * 2560; time = 0.0239s; samplesPerSecond = 106907.2
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.25393982 * 2560; err = 0.38320312 * 2560; time = 0.0241s; samplesPerSecond = 106122.8
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23377075 * 2560; err = 0.36953125 * 2560; time = 0.0242s; samplesPerSecond = 105863.9
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20861511 * 2560; err = 0.35976562 * 2560; time = 0.0243s; samplesPerSecond = 105302.1
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23675232 * 2560; err = 0.36757812 * 2560; time = 0.0242s; samplesPerSecond = 105667.2
08/16/2016 03:05:03:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.22960205 * 2560; err = 0.37460938 * 2560; time = 0.0236s; samplesPerSecond = 108644.9
08/16/2016 03:05:03: Finished Epoch[ 1 of 2]: [Training] ce = 1.65172386 * 81920; err = 0.46774902 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.03546s
08/16/2016 03:05:03: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/16/2016 03:05:03: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 03:05:03: Starting minibatch loop.
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.21869726 * 2560; err = 0.36992188 * 2560; time = 0.0256s; samplesPerSecond = 100046.9
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.18345709 * 2560; err = 0.36679688 * 2560; time = 0.0249s; samplesPerSecond = 102642.2
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.17220440 * 2560; err = 0.35898438 * 2560; time = 0.0240s; samplesPerSecond = 106795.7
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.20035286 * 2560; err = 0.35781250 * 2560; time = 0.0240s; samplesPerSecond = 106835.8
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.19499741 * 2560; err = 0.37460938 * 2560; time = 0.0239s; samplesPerSecond = 106907.2
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.16373444 * 2560; err = 0.34687500 * 2560; time = 0.0240s; samplesPerSecond = 106604.5
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.13869247 * 2560; err = 0.34804687 * 2560; time = 0.0241s; samplesPerSecond = 106096.4
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.19293823 * 2560; err = 0.36992188 * 2560; time = 0.0240s; samplesPerSecond = 106622.2
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.23978348 * 2560; err = 0.37539062 * 2560; time = 0.0240s; samplesPerSecond = 106520.2
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18622742 * 2560; err = 0.36406250 * 2560; time = 0.0240s; samplesPerSecond = 106795.7
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16710892 * 2560; err = 0.35703125 * 2560; time = 0.0239s; samplesPerSecond = 106907.2
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.24683685 * 2560; err = 0.38554688 * 2560; time = 0.0240s; samplesPerSecond = 106849.2
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.18601685 * 2560; err = 0.35273437 * 2560; time = 0.0239s; samplesPerSecond = 107005.5
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.21721497 * 2560; err = 0.37617187 * 2560; time = 0.0240s; samplesPerSecond = 106653.3
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.19934692 * 2560; err = 0.36953125 * 2560; time = 0.0240s; samplesPerSecond = 106551.2
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.15099945 * 2560; err = 0.34257813 * 2560; time = 0.0239s; samplesPerSecond = 106920.6
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.14984589 * 2560; err = 0.35703125 * 2560; time = 0.0239s; samplesPerSecond = 107211.7
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.19028320 * 2560; err = 0.35898438 * 2560; time = 0.0240s; samplesPerSecond = 106724.5
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.16434784 * 2560; err = 0.36406250 * 2560; time = 0.0240s; samplesPerSecond = 106858.1
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08853760 * 2560; err = 0.33359375 * 2560; time = 0.0240s; samplesPerSecond = 106782.3
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.15194244 * 2560; err = 0.35039063 * 2560; time = 0.0239s; samplesPerSecond = 106992.1
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16113434 * 2560; err = 0.35625000 * 2560; time = 0.0238s; samplesPerSecond = 107612.8
08/16/2016 03:05:03:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.18479004 * 2560; err = 0.36757812 * 2560; time = 0.0239s; samplesPerSecond = 107113.0
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.14554138 * 2560; err = 0.34843750 * 2560; time = 0.0239s; samplesPerSecond = 107050.3
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.15263367 * 2560; err = 0.35390625 * 2560; time = 0.0239s; samplesPerSecond = 107198.2
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.08563538 * 2560; err = 0.33437500 * 2560; time = 0.0238s; samplesPerSecond = 107757.7
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10797424 * 2560; err = 0.34882812 * 2560; time = 0.0239s; samplesPerSecond = 107068.2
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.07031860 * 2560; err = 0.33593750 * 2560; time = 0.0240s; samplesPerSecond = 106822.4
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.09429016 * 2560; err = 0.33476563 * 2560; time = 0.0239s; samplesPerSecond = 107288.0
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14634094 * 2560; err = 0.35351563 * 2560; time = 0.0237s; samplesPerSecond = 107834.9
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.10476990 * 2560; err = 0.34335938 * 2560; time = 0.0240s; samplesPerSecond = 106835.8
08/16/2016 03:05:04:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07355957 * 2560; err = 0.32695313 * 2560; time = 0.0235s; samplesPerSecond = 108857.4
08/16/2016 03:05:04: Finished Epoch[ 2 of 2]: [Training] ce = 1.16032982 * 81920; err = 0.35574951 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.770816s
08/16/2016 03:05:04: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech'
08/16/2016 03:05:04: CNTKCommandTrainEnd: dptPre1

08/16/2016 03:05:04: Action "train" complete.


08/16/2016 03:05:04: ##############################################################################
08/16/2016 03:05:04: #                                                                            #
08/16/2016 03:05:04: # Action "edit"                                                              #
08/16/2016 03:05:04: #                                                                            #
08/16/2016 03:05:04: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=5, range=0.050000*1.000000, onCPU=false).
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=6, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 03:05:04: Action "edit" complete.


08/16/2016 03:05:04: ##############################################################################
08/16/2016 03:05:04: #                                                                            #
08/16/2016 03:05:04: # Action "train"                                                             #
08/16/2016 03:05:04: #                                                                            #
08/16/2016 03:05:04: ##############################################################################

08/16/2016 03:05:04: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 03:05:04: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 03:05:04: Loaded model with 24 nodes on GPU 0.

08/16/2016 03:05:04: Training criterion node(s):
08/16/2016 03:05:04: 	ce = CrossEntropyWithSoftmax

08/16/2016 03:05:04: Evaluation criterion node(s):
08/16/2016 03:05:04: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 39 matrices, 19 are shared as 8, and 20 are not shared.

	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *3] }
	{ HL1.t : [512 x *3] (gradient)
	  HL1.y : [512 x 1 x *3] }
	{ HL1.z : [512 x 1 x *3] (gradient)
	  HL2.t : [512 x 1 x *3] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *3] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *3] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *3] (gradient)
	  HL2.z : [512 x 1 x *3] (gradient)
	  OL.t : [132 x 1 x *3] }
	{ HL2.t : [512 x 1 x *3] (gradient)
	  HL2.y : [512 x 1 x *3] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *3] (gradient)
	  OL.z : [132 x 1 x *3] (gradient) }


08/16/2016 03:05:04: Training 516740 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

08/16/2016 03:05:04: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 03:05:04: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:04: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:05:04: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:04: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 03:05:04: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 03:05:04: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 03:05:04: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 03:05:04: Starting minibatch loop.
08/16/2016 03:05:04:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 4.49739113 * 2560; err = 0.80429688 * 2560; time = 0.0349s; samplesPerSecond = 73320.9
08/16/2016 03:05:04:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.83226433 * 2560; err = 0.68125000 * 2560; time = 0.0272s; samplesPerSecond = 94086.5
08/16/2016 03:05:04:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.25921097 * 2560; err = 0.59921875 * 2560; time = 0.0273s; samplesPerSecond = 93604.9
08/16/2016 03:05:04:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.91240921 * 2560; err = 0.51210937 * 2560; time = 0.0274s; samplesPerSecond = 93403.4
08/16/2016 03:05:04:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.69259949 * 2560; err = 0.46679688 * 2560; time = 0.0270s; samplesPerSecond = 94853.5
08/16/2016 03:05:04:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.59069672 * 2560; err = 0.45312500 * 2560; time = 0.0272s; samplesPerSecond = 94280.6
08/16/2016 03:05:04:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.48813324 * 2560; err = 0.43789062 * 2560; time = 0.0269s; samplesPerSecond = 95287.7
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.48960571 * 2560; err = 0.43515625 * 2560; time = 0.0274s; samplesPerSecond = 93263.9
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.45628204 * 2560; err = 0.42187500 * 2560; time = 0.0275s; samplesPerSecond = 93226.5
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41567383 * 2560; err = 0.40820313 * 2560; time = 0.0272s; samplesPerSecond = 94027.8
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.42048950 * 2560; err = 0.41406250 * 2560; time = 0.0275s; samplesPerSecond = 93162.1
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.34279480 * 2560; err = 0.39726563 * 2560; time = 0.0275s; samplesPerSecond = 93138.3
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.31633148 * 2560; err = 0.38789062 * 2560; time = 0.0276s; samplesPerSecond = 92861.3
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.33296814 * 2560; err = 0.39804688 * 2560; time = 0.0277s; samplesPerSecond = 92342.1
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.32084351 * 2560; err = 0.39609375 * 2560; time = 0.0272s; samplesPerSecond = 94048.5
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27189636 * 2560; err = 0.38125000 * 2560; time = 0.0278s; samplesPerSecond = 92099.6
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29380188 * 2560; err = 0.38554688 * 2560; time = 0.0274s; samplesPerSecond = 93376.1
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.31463013 * 2560; err = 0.38984375 * 2560; time = 0.0273s; samplesPerSecond = 93931.2
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33578796 * 2560; err = 0.40664062 * 2560; time = 0.0274s; samplesPerSecond = 93297.9
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.32202454 * 2560; err = 0.41484375 * 2560; time = 0.0275s; samplesPerSecond = 93009.7
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23669434 * 2560; err = 0.37460938 * 2560; time = 0.0275s; samplesPerSecond = 92925.3
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.27109985 * 2560; err = 0.38906250 * 2560; time = 0.0274s; samplesPerSecond = 93291.1
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.26419678 * 2560; err = 0.37578125 * 2560; time = 0.0275s; samplesPerSecond = 93097.7
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.23778992 * 2560; err = 0.37265625 * 2560; time = 0.0278s; samplesPerSecond = 92215.7
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.21040344 * 2560; err = 0.36757812 * 2560; time = 0.0271s; samplesPerSecond = 94384.8
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.18387146 * 2560; err = 0.36562500 * 2560; time = 0.0276s; samplesPerSecond = 92619.4
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23827515 * 2560; err = 0.37148437 * 2560; time = 0.0278s; samplesPerSecond = 92129.4
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.18418274 * 2560; err = 0.36328125 * 2560; time = 0.0271s; samplesPerSecond = 94419.7
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.16851501 * 2560; err = 0.35234375 * 2560; time = 0.0277s; samplesPerSecond = 92482.2
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14337463 * 2560; err = 0.34375000 * 2560; time = 0.0271s; samplesPerSecond = 94346.6
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.17227478 * 2560; err = 0.34882812 * 2560; time = 0.0272s; samplesPerSecond = 94193.8
08/16/2016 03:05:05:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.18431091 * 2560; err = 0.36835937 * 2560; time = 0.0273s; samplesPerSecond = 93704.2
08/16/2016 03:05:05: Finished Epoch[ 1 of 2]: [Training] ce = 1.51252575 * 81920; err = 0.42452393 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.02432s
08/16/2016 03:05:05: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/16/2016 03:05:05: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 03:05:05: Starting minibatch loop.
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.17448177 * 2560; err = 0.35195312 * 2560; time = 0.0290s; samplesPerSecond = 88333.7
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.14536781 * 2560; err = 0.35664062 * 2560; time = 0.0275s; samplesPerSecond = 93192.6
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.15722904 * 2560; err = 0.34531250 * 2560; time = 0.0273s; samplesPerSecond = 93824.4
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.14344521 * 2560; err = 0.34804687 * 2560; time = 0.0275s; samplesPerSecond = 93240.1
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.14842377 * 2560; err = 0.36562500 * 2560; time = 0.0280s; samplesPerSecond = 91395.9
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14489059 * 2560; err = 0.34218750 * 2560; time = 0.0269s; samplesPerSecond = 95110.7
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.09631195 * 2560; err = 0.33984375 * 2560; time = 0.0265s; samplesPerSecond = 96480.0
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16026917 * 2560; err = 0.35546875 * 2560; time = 0.0252s; samplesPerSecond = 101563.1
08/16/2016 03:05:05:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.16528091 * 2560; err = 0.36015625 * 2560; time = 0.0272s; samplesPerSecond = 93948.4
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12257309 * 2560; err = 0.34492187 * 2560; time = 0.0265s; samplesPerSecond = 96520.0
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.12313080 * 2560; err = 0.34765625 * 2560; time = 0.0277s; samplesPerSecond = 92255.6
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18492126 * 2560; err = 0.36171875 * 2560; time = 0.0273s; samplesPerSecond = 93615.2
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.13058014 * 2560; err = 0.33476563 * 2560; time = 0.0274s; samplesPerSecond = 93475.0
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16725922 * 2560; err = 0.35781250 * 2560; time = 0.0274s; samplesPerSecond = 93577.5
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.12244720 * 2560; err = 0.34648438 * 2560; time = 0.0270s; samplesPerSecond = 94807.8
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09480591 * 2560; err = 0.33671875 * 2560; time = 0.0263s; samplesPerSecond = 97520.1
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.11218109 * 2560; err = 0.34140625 * 2560; time = 0.0269s; samplesPerSecond = 95266.4
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11966095 * 2560; err = 0.33398438 * 2560; time = 0.0270s; samplesPerSecond = 94772.7
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10485687 * 2560; err = 0.33671875 * 2560; time = 0.0273s; samplesPerSecond = 93796.9
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.06019897 * 2560; err = 0.32617188 * 2560; time = 0.0277s; samplesPerSecond = 92375.4
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.10600891 * 2560; err = 0.34101562 * 2560; time = 0.0276s; samplesPerSecond = 92854.6
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.13724823 * 2560; err = 0.34101562 * 2560; time = 0.0275s; samplesPerSecond = 93077.4
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12464600 * 2560; err = 0.34609375 * 2560; time = 0.0272s; samplesPerSecond = 94017.4
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.10831604 * 2560; err = 0.33593750 * 2560; time = 0.0269s; samplesPerSecond = 95294.8
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09707031 * 2560; err = 0.34023437 * 2560; time = 0.0273s; samplesPerSecond = 93714.5
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.04812317 * 2560; err = 0.32773438 * 2560; time = 0.0277s; samplesPerSecond = 92535.7
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.04979248 * 2560; err = 0.33398438 * 2560; time = 0.0271s; samplesPerSecond = 94496.3
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.03223572 * 2560; err = 0.31835938 * 2560; time = 0.0270s; samplesPerSecond = 94955.5
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.05677490 * 2560; err = 0.32773438 * 2560; time = 0.0272s; samplesPerSecond = 93982.9
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10880737 * 2560; err = 0.34296875 * 2560; time = 0.0273s; samplesPerSecond = 93738.6
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08513489 * 2560; err = 0.33476563 * 2560; time = 0.0277s; samplesPerSecond = 92395.4
08/16/2016 03:05:06:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04244080 * 2560; err = 0.31757812 * 2560; time = 0.0276s; samplesPerSecond = 92915.2
08/16/2016 03:05:06: Finished Epoch[ 2 of 2]: [Training] ce = 1.11484108 * 81920; err = 0.34190674 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.874281s
08/16/2016 03:05:06: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech'
08/16/2016 03:05:06: CNTKCommandTrainEnd: dptPre2

08/16/2016 03:05:06: Action "train" complete.


08/16/2016 03:05:06: ##############################################################################
08/16/2016 03:05:06: #                                                                            #
08/16/2016 03:05:06: # Action "edit"                                                              #
08/16/2016 03:05:06: #                                                                            #
08/16/2016 03:05:06: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=7, range=0.050000*1.000000, onCPU=false).
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=8, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 03:05:06: Action "edit" complete.


08/16/2016 03:05:06: ##############################################################################
08/16/2016 03:05:06: #                                                                            #
08/16/2016 03:05:06: # Action "train"                                                             #
08/16/2016 03:05:06: #                                                                            #
08/16/2016 03:05:06: ##############################################################################

08/16/2016 03:05:06: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 03:05:07: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 03:05:07: Loaded model with 29 nodes on GPU 0.

08/16/2016 03:05:07: Training criterion node(s):
08/16/2016 03:05:07: 	ce = CrossEntropyWithSoftmax

08/16/2016 03:05:07: Evaluation criterion node(s):
08/16/2016 03:05:07: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 49 matrices, 27 are shared as 11, and 22 are not shared.

	{ HL1.t : [512 x *6] (gradient)
	  HL1.y : [512 x 1 x *6] }
	{ HL2.t : [512 x 1 x *6] (gradient)
	  HL2.y : [512 x 1 x *6] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *6] (gradient)
	  HL2.z : [512 x 1 x *6] (gradient)
	  HL3.t : [512 x 1 x *6] }
	{ HL3.t : [512 x 1 x *6] (gradient)
	  HL3.y : [512 x 1 x *6] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *6] }
	{ HL1.z : [512 x 1 x *6] (gradient)
	  HL2.t : [512 x 1 x *6] }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *6] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *6] }
	{ HL3.W : [512 x 512] (gradient)
	  HL3.z : [512 x 1 x *6] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *6] (gradient)
	  HL3.z : [512 x 1 x *6] (gradient)
	  OL.t : [132 x 1 x *6] }
	{ HL3.b : [512 x 1] (gradient)
	  HL3.y : [512 x 1 x *6] (gradient)
	  OL.z : [132 x 1 x *6] (gradient) }


08/16/2016 03:05:07: Training 779396 parameters in 8 out of 8 parameter tensors and 20 nodes with gradient:

08/16/2016 03:05:07: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 03:05:07: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:07: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:05:07: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:07: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:05:07: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:07: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 03:05:07: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 03:05:07: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 03:05:07: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 03:05:07: Starting minibatch loop.
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.12455368 * 2560; err = 0.82734375 * 2560; time = 0.0460s; samplesPerSecond = 55595.4
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.55599899 * 2560; err = 0.63007813 * 2560; time = 0.0361s; samplesPerSecond = 71004.6
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.03516159 * 2560; err = 0.53945312 * 2560; time = 0.0350s; samplesPerSecond = 73134.5
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.73739853 * 2560; err = 0.47500000 * 2560; time = 0.0343s; samplesPerSecond = 74568.2
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.54207916 * 2560; err = 0.43515625 * 2560; time = 0.0355s; samplesPerSecond = 72200.1
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.44409790 * 2560; err = 0.41328125 * 2560; time = 0.0353s; samplesPerSecond = 72580.9
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.36059418 * 2560; err = 0.40898438 * 2560; time = 0.0344s; samplesPerSecond = 74464.1
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35930023 * 2560; err = 0.40117188 * 2560; time = 0.0343s; samplesPerSecond = 74694.4
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.34254303 * 2560; err = 0.38632813 * 2560; time = 0.0341s; samplesPerSecond = 75051.3
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.30505676 * 2560; err = 0.38320312 * 2560; time = 0.0342s; samplesPerSecond = 74866.9
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.30881348 * 2560; err = 0.38476563 * 2560; time = 0.0350s; samplesPerSecond = 73136.6
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.23755188 * 2560; err = 0.37304688 * 2560; time = 0.0349s; samplesPerSecond = 73411.3
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.21070251 * 2560; err = 0.35546875 * 2560; time = 0.0343s; samplesPerSecond = 74622.5
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.24008789 * 2560; err = 0.37109375 * 2560; time = 0.0343s; samplesPerSecond = 74735.8
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.23422089 * 2560; err = 0.36835937 * 2560; time = 0.0343s; samplesPerSecond = 74631.2
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.19426117 * 2560; err = 0.35195312 * 2560; time = 0.0343s; samplesPerSecond = 74607.3
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.21415710 * 2560; err = 0.36289063 * 2560; time = 0.0342s; samplesPerSecond = 74755.4
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.24289551 * 2560; err = 0.37031250 * 2560; time = 0.0343s; samplesPerSecond = 74579.0
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.26465759 * 2560; err = 0.38359375 * 2560; time = 0.0343s; samplesPerSecond = 74674.8
08/16/2016 03:05:07:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.22050476 * 2560; err = 0.38085938 * 2560; time = 0.0343s; samplesPerSecond = 74602.9
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.17745056 * 2560; err = 0.35507813 * 2560; time = 0.0343s; samplesPerSecond = 74618.2
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.19851379 * 2560; err = 0.37109375 * 2560; time = 0.0346s; samplesPerSecond = 74065.5
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.21453857 * 2560; err = 0.35820313 * 2560; time = 0.0343s; samplesPerSecond = 74526.9
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.18011475 * 2560; err = 0.35546875 * 2560; time = 0.0343s; samplesPerSecond = 74529.1
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.16693726 * 2560; err = 0.35195312 * 2560; time = 0.0342s; samplesPerSecond = 74748.9
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.12398987 * 2560; err = 0.35234375 * 2560; time = 0.0343s; samplesPerSecond = 74674.8
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.18822021 * 2560; err = 0.36328125 * 2560; time = 0.0343s; samplesPerSecond = 74531.3
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.13831482 * 2560; err = 0.35078125 * 2560; time = 0.0343s; samplesPerSecond = 74700.9
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12718811 * 2560; err = 0.33984375 * 2560; time = 0.0343s; samplesPerSecond = 74740.2
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.11155396 * 2560; err = 0.34179688 * 2560; time = 0.0344s; samplesPerSecond = 74503.1
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.13423157 * 2560; err = 0.34101562 * 2560; time = 0.0343s; samplesPerSecond = 74727.1
08/16/2016 03:05:08:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12716675 * 2560; err = 0.34414062 * 2560; time = 0.0343s; samplesPerSecond = 74681.3
08/16/2016 03:05:08: Finished Epoch[ 1 of 4]: [Training] ce = 1.40821428 * 81920; err = 0.40085449 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.24549s
08/16/2016 03:05:08: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.1'

08/16/2016 03:05:08: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 03:05:08: Starting minibatch loop.
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.20089607 * 5120; err = 0.36757812 * 5120; time = 0.0574s; samplesPerSecond = 89217.3
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.15295639 * 5120; err = 0.34550781 * 5120; time = 0.0465s; samplesPerSecond = 110181.0
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.09945831 * 5120; err = 0.33613281 * 5120; time = 0.0466s; samplesPerSecond = 109828.8
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.09916496 * 5120; err = 0.33867188 * 5120; time = 0.0468s; samplesPerSecond = 109327.0
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.17260475 * 5120; err = 0.36230469 * 5120; time = 0.0472s; samplesPerSecond = 108578.1
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.15717888 * 5120; err = 0.35820313 * 5120; time = 0.0468s; samplesPerSecond = 109350.3
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.14431229 * 5120; err = 0.34296875 * 5120; time = 0.0470s; samplesPerSecond = 108889.8
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.10515747 * 5120; err = 0.34394531 * 5120; time = 0.0470s; samplesPerSecond = 109047.5
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.15175400 * 5120; err = 0.35449219 * 5120; time = 0.0467s; samplesPerSecond = 109579.7
08/16/2016 03:05:08:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.11654053 * 5120; err = 0.34101562 * 5120; time = 0.0465s; samplesPerSecond = 110008.2
08/16/2016 03:05:09:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.11851807 * 5120; err = 0.34472656 * 5120; time = 0.0464s; samplesPerSecond = 110256.9
08/16/2016 03:05:09:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.11374130 * 5120; err = 0.34492187 * 5120; time = 0.0465s; samplesPerSecond = 110190.5
08/16/2016 03:05:09:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.04686737 * 5120; err = 0.32265625 * 5120; time = 0.0464s; samplesPerSecond = 110290.2
08/16/2016 03:05:09:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02721252 * 5120; err = 0.32246094 * 5120; time = 0.0465s; samplesPerSecond = 110081.5
08/16/2016 03:05:09:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.08386230 * 5120; err = 0.33144531 * 5120; time = 0.0466s; samplesPerSecond = 109814.7
08/16/2016 03:05:09:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06164856 * 5120; err = 0.32558594 * 5120; time = 0.0465s; samplesPerSecond = 110154.9
08/16/2016 03:05:09: Finished Epoch[ 2 of 4]: [Training] ce = 1.11574211 * 81920; err = 0.34266357 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.760257s
08/16/2016 03:05:09: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.2'

08/16/2016 03:05:09: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/16/2016 03:05:09: Starting minibatch loop.
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.12331724 * 5120; err = 0.34121094 * 5120; time = 0.0473s; samplesPerSecond = 108309.4
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.07871103 * 5120; err = 0.33652344 * 5120; time = 0.0464s; samplesPerSecond = 110316.3
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.06784973 * 5120; err = 0.33183594 * 5120; time = 0.0465s; samplesPerSecond = 110133.6
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.08440666 * 5120; err = 0.33398438 * 5120; time = 0.0465s; samplesPerSecond = 110091.0
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07466774 * 5120; err = 0.33320312 * 5120; time = 0.0467s; samplesPerSecond = 109560.9
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.05427513 * 5120; err = 0.33125000 * 5120; time = 0.0466s; samplesPerSecond = 109831.2
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06873093 * 5120; err = 0.32773438 * 5120; time = 0.0466s; samplesPerSecond = 109762.9
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08097610 * 5120; err = 0.33007813 * 5120; time = 0.0465s; samplesPerSecond = 110119.4
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.05431290 * 5120; err = 0.32792969 * 5120; time = 0.0466s; samplesPerSecond = 109807.6
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.06173096 * 5120; err = 0.32695313 * 5120; time = 0.0468s; samplesPerSecond = 109457.8
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.04505692 * 5120; err = 0.32792969 * 5120; time = 0.0465s; samplesPerSecond = 110100.4
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.08151245 * 5120; err = 0.33574219 * 5120; time = 0.0464s; samplesPerSecond = 110318.7
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.10628204 * 5120; err = 0.33437500 * 5120; time = 0.0466s; samplesPerSecond = 109755.8
08/16/2016 03:05:09:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.05827026 * 5120; err = 0.32636719 * 5120; time = 0.0468s; samplesPerSecond = 109422.8
08/16/2016 03:05:10:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05841064 * 5120; err = 0.33574219 * 5120; time = 0.0467s; samplesPerSecond = 109664.2
08/16/2016 03:05:10:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.04437866 * 5120; err = 0.32773438 * 5120; time = 0.0471s; samplesPerSecond = 108778.8
08/16/2016 03:05:10: Finished Epoch[ 3 of 4]: [Training] ce = 1.07143059 * 81920; err = 0.33178711 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.749415s
08/16/2016 03:05:10: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.3'

08/16/2016 03:05:10: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/16/2016 03:05:10: Starting minibatch loop.
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.04450397 * 5120; err = 0.33125000 * 5120; time = 0.0473s; samplesPerSecond = 108268.1
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.02895867 * 4926; err = 0.31567194 * 4926; time = 0.0878s; samplesPerSecond = 56088.8
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.00198059 * 5120; err = 0.31601563 * 5120; time = 0.0465s; samplesPerSecond = 110053.1
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.00561543 * 5120; err = 0.31777344 * 5120; time = 0.0464s; samplesPerSecond = 110287.8
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.00148888 * 5120; err = 0.31601563 * 5120; time = 0.0463s; samplesPerSecond = 110523.5
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00593338 * 5120; err = 0.31406250 * 5120; time = 0.0466s; samplesPerSecond = 109833.5
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.98752327 * 5120; err = 0.30722656 * 5120; time = 0.0465s; samplesPerSecond = 110166.8
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.01428757 * 5120; err = 0.31992188 * 5120; time = 0.0464s; samplesPerSecond = 110394.8
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99691544 * 5120; err = 0.31621094 * 5120; time = 0.0465s; samplesPerSecond = 109998.7
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96604996 * 5120; err = 0.30937500 * 5120; time = 0.0464s; samplesPerSecond = 110447.2
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 0.99062958 * 5120; err = 0.30527344 * 5120; time = 0.0465s; samplesPerSecond = 110074.4
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.99886856 * 5120; err = 0.30976562 * 5120; time = 0.0465s; samplesPerSecond = 110112.3
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.00958405 * 5120; err = 0.31523438 * 5120; time = 0.0465s; samplesPerSecond = 110218.9
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97942047 * 5120; err = 0.31171875 * 5120; time = 0.0463s; samplesPerSecond = 110494.9
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.94226837 * 5120; err = 0.30136719 * 5120; time = 0.0463s; samplesPerSecond = 110561.7
08/16/2016 03:05:10:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.96711578 * 5120; err = 0.30175781 * 5120; time = 0.0461s; samplesPerSecond = 110959.4
08/16/2016 03:05:10: Finished Epoch[ 4 of 4]: [Training] ce = 0.99611807 * 81920; err = 0.31303711 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.79129s
08/16/2016 03:05:11: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech'
08/16/2016 03:05:11: CNTKCommandTrainEnd: speechTrain

08/16/2016 03:05:11: Action "train" complete.


08/16/2016 03:05:11: ##############################################################################
08/16/2016 03:05:11: #                                                                            #
08/16/2016 03:05:11: # Action "edit"                                                              #
08/16/2016 03:05:11: #                                                                            #
08/16/2016 03:05:11: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 03:05:11: Action "edit" complete.


08/16/2016 03:05:11: ##############################################################################
08/16/2016 03:05:11: #                                                                            #
08/16/2016 03:05:11: # Action "train"                                                             #
08/16/2016 03:05:11: #                                                                            #
08/16/2016 03:05:11: ##############################################################################

08/16/2016 03:05:11: CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu\TestData\CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames

08/16/2016 03:05:11: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0'.

Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *9]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *9]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *9], [363 x 1], [363 x 1] -> [363 x *9]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *9] -> [512 x *9]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *9] -> [132 x 1 x *9]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *9], [132 x 1 x *9], [132 x 1 x *9] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *9], [132 x 1 x *9] -> [1]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 03:05:11: Loaded model with 29 nodes on GPU 0.

08/16/2016 03:05:11: Training criterion node(s):
08/16/2016 03:05:11: 	ce = SequenceWithSoftmax

08/16/2016 03:05:11: Evaluation criterion node(s):
08/16/2016 03:05:11: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 50 matrices, 29 are shared as 12, and 21 are not shared.

	{ HL1.t : [512 x *9] (gradient)
	  HL1.y : [512 x 1 x *9] }
	{ HL3.W : [512 x 512] (gradient)
	  HL3.z : [512 x 1 x *9] }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *9] }
	{ HL1.z : [512 x 1 x *9] (gradient)
	  HL2.t : [512 x 1 x *9] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *9] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *9] (gradient)
	  HL2.z : [512 x 1 x *9] (gradient)
	  HL3.t : [512 x 1 x *9] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *9] }
	{ HL3.b : [512 x 1] (gradient)
	  HL3.y : [512 x 1 x *9] (gradient)
	  OL.z : [132 x 1 x *9] (gradient) }
	{ OL.t : [132 x 1 x *9] (gradient)
	  scaledLogLikelihood : [132 x 1 x *9] (gradient) }
	{ HL2.t : [512 x 1 x *9] (gradient)
	  HL2.y : [512 x 1 x *9] }
	{ HL3.t : [512 x 1 x *9] (gradient)
	  HL3.y : [512 x 1 x *9] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *9] (gradient)
	  HL3.z : [512 x 1 x *9] (gradient)
	  OL.t : [132 x 1 x *9] }


08/16/2016 03:05:11: Training 779396 parameters in 8 out of 8 parameter tensors and 21 nodes with gradient:

08/16/2016 03:05:11: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 03:05:11: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:11: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:05:11: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:11: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:05:11: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:05:11: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 03:05:11: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 03:05:11: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-010
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

08/16/2016 03:05:11: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 03:05:18: Starting minibatch loop.
dengamma value 1.015387
dengamma value 1.051343
dengamma value 1.072787
dengamma value 1.063094
dengamma value 1.076285
dengamma value 1.095859
dengamma value 0.976427
dengamma value 1.196782
dengamma value 1.015319
dengamma value 1.105372
dengamma value 1.034096
dengamma value 1.155336
dengamma value 1.050430
dengamma value 1.030251
dengamma value 1.107270
dengamma value 1.037082
dengamma value 1.125835
dengamma value 1.042350
dengamma value 1.060958
dengamma value 1.064541
dengamma value 1.000843
08/16/2016 03:05:20:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08358561 * 4628; err = 0.33059637 * 4628; time = 2.4688s; samplesPerSecond = 1874.6
dengamma value 1.099889
dengamma value 1.039597
dengamma value 1.115739
dengamma value 0.957857
dengamma value 1.122265
dengamma value 1.111263
dengamma value 1.065039
dengamma value 1.066569
dengamma value 0.987920
dengamma value 1.044044
dengamma value 1.004607
dengamma value 1.062701
dengamma value 1.035316
dengamma value 1.026637
dengamma value 1.144678
dengamma value 1.165378
dengamma value 1.132944
dengamma value 1.081424
dengamma value 1.108471
dengamma value 1.075180
dengamma value 1.088663
dengamma value 1.078510
08/16/2016 03:05:21:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08214610 * 5946; err = 0.31449714 * 5946; time = 1.1241s; samplesPerSecond = 5289.6
dengamma value 1.048358
dengamma value 1.103563
dengamma value 1.099066
dengamma value 1.136579
dengamma value 1.077474
dengamma value 1.060762
dengamma value 1.101553
dengamma value 1.107761
dengamma value 1.022390
dengamma value 1.081607
dengamma value 1.010834
dengamma value 1.077299
dengamma value 1.083294
dengamma value 1.126290
dengamma value 1.066091
dengamma value 1.078891
dengamma value 1.046375
dengamma value 0.986374
dengamma value 1.084432
dengamma value 1.116274
dengamma value 1.091088
dengamma value 1.074867
08/16/2016 03:05:22:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08367419 * 5916; err = 0.32437458 * 5916; time = 0.9925s; samplesPerSecond = 5960.9
dengamma value 1.030191
dengamma value 1.047688
dengamma value 1.041505
dengamma value 1.137154
dengamma value 1.101086
dengamma value 1.032440
dengamma value 1.077590
dengamma value 1.102024
dengamma value 1.104773
dengamma value 1.157433
dengamma value 1.050714
dengamma value 1.003643
dengamma value 1.107600
dengamma value 1.107183
dengamma value 0.954740
dengamma value 1.009407
dengamma value 1.066747
dengamma value 1.065564
dengamma value 1.048961
dengamma value 1.026667
dengamma value 1.096718
dengamma value 1.068000
08/16/2016 03:05:23:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08031854 * 6386; err = 0.32680864 * 6386; time = 1.0603s; samplesPerSecond = 6022.6
dengamma value 1.044291
dengamma value 1.082913
dengamma value 1.073895
dengamma value 1.133403
dengamma value 1.078159
dengamma value 1.102888
dengamma value 1.082325
dengamma value 1.099348
dengamma value 1.167385
dengamma value 1.058515
dengamma value 1.078929
dengamma value 1.050607
dengamma value 1.091514
dengamma value 1.052261
dengamma value 1.109094
dengamma value 1.121752
dengamma value 1.057149
dengamma value 1.042336
dengamma value 1.051527
dengamma value 0.977551
dengamma value 1.104787
dengamma value 1.159152
dengamma value 1.068097
08/16/2016 03:05:24:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08171634 * 6734; err = 0.29551530 * 6734; time = 1.1476s; samplesPerSecond = 5867.9
dengamma value 1.086780
dengamma value 1.130269
dengamma value 1.073455
dengamma value 1.091998
dengamma value 1.035754
dengamma value 0.985550
dengamma value 1.059557
dengamma value 1.123901
dengamma value 1.083224
dengamma value 1.059212
dengamma value 1.097844
dengamma value 1.036471
dengamma value 1.081966
dengamma value 0.984412
dengamma value 1.105340
dengamma value 1.047261
dengamma value 1.033234
dengamma value 1.141741
dengamma value 1.066171
dengamma value 1.077317
dengamma value 1.127820
dengamma value 1.077791
dengamma value 1.080292
dengamma value 0.981080
08/16/2016 03:05:25:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08549173 * 6202; err = 0.31360851 * 6202; time = 1.0497s; samplesPerSecond = 5908.1
dengamma value 1.130581
dengamma value 1.117521
dengamma value 1.055970
dengamma value 1.068845
dengamma value 1.151987
dengamma value 1.030249
dengamma value 1.106854
dengamma value 1.101331
dengamma value 1.060793
dengamma value 1.107020
dengamma value 1.105107
dengamma value 1.079484
dengamma value 1.108909
dengamma value 1.004101
dengamma value 0.950285
dengamma value 1.125628
dengamma value 1.085928
dengamma value 1.042281
dengamma value 1.065240
dengamma value 1.049562
dengamma value 1.073483
dengamma value 1.089741
dengamma value 1.053381
dengamma value 1.018081
08/16/2016 03:05:27:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07997490 * 6362; err = 0.32568375 * 6362; time = 1.1038s; samplesPerSecond = 5763.5
dengamma value 1.075601
dengamma value 1.083481
dengamma value 1.123551
dengamma value 1.028559
dengamma value 1.061858
dengamma value 1.100839
dengamma value 1.023886
dengamma value 1.043098
dengamma value 1.025118
dengamma value 1.136169
dengamma value 1.116578
dengamma value 1.054926
dengamma value 1.064200
dengamma value 1.068297
dengamma value 1.059706
dengamma value 1.095217
dengamma value 1.115496
dengamma value 1.135736
dengamma value 1.060187
dengamma value 1.092560
dengamma value 1.066627
08/16/2016 03:05:28:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.07502958 * 5608; err = 0.31847361 * 5608; time = 0.9343s; samplesPerSecond = 6002.4
dengamma value 1.063055
dengamma value 1.042724
dengamma value 1.059763
dengamma value 1.021321
dengamma value 1.097745
dengamma value 1.028584
dengamma value 1.103968
dengamma value 0.997650
dengamma value 1.112675
dengamma value 1.092727
dengamma value 1.043912
dengamma value 1.129032
dengamma value 1.116614
dengamma value 1.042492
dengamma value 1.061597
dengamma value 1.015698
dengamma value 1.053582
dengamma value 1.033995
dengamma value 1.093077
dengamma value 1.080366
dengamma value 1.104077
dengamma value 1.044763
dengamma value 1.020228
08/16/2016 03:05:29:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08151798 * 6594; err = 0.33651805 * 6594; time = 1.1653s; samplesPerSecond = 5658.6
dengamma value 1.028670
dengamma value 0.886836
dengamma value 1.157957
dengamma value 1.014026
dengamma value 1.116295
dengamma value 1.039071
dengamma value 1.099671
dengamma value 1.101143
dengamma value 1.113298
dengamma value 1.109584
dengamma value 1.079916
dengamma value 1.063747
dengamma value 1.095497
dengamma value 1.076887
dengamma value 1.141736
dengamma value 1.035007
dengamma value 1.117461
dengamma value 1.135798
dengamma value 1.057881
dengamma value 1.067153
dengamma value 1.072834
dengamma value 1.006985
dengamma value 1.110780
08/16/2016 03:05:30:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08032955 * 6364; err = 0.31615336 * 6364; time = 1.1660s; samplesPerSecond = 5458.0
dengamma value 1.061657
dengamma value 1.052374
dengamma value 0.987764
dengamma value 1.062911
dengamma value 1.061221
dengamma value 1.142910
dengamma value 1.130698
dengamma value 1.082887
dengamma value 1.044966
dengamma value 1.143583
dengamma value 1.073300
dengamma value 1.157146
dengamma value 1.095505
dengamma value 1.008495
dengamma value 1.116105
dengamma value 1.059661
dengamma value 1.161594
dengamma value 1.098735
dengamma value 1.146981
dengamma value 1.140211
dengamma value 1.096159
dengamma value 1.051981
dengamma value 1.046266
dengamma value 1.118023
dengamma value 1.042709
dengamma value 1.001388
dengamma value 1.012911
08/16/2016 03:05:31:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08613263 * 6536; err = 0.31548348 * 6536; time = 1.1124s; samplesPerSecond = 5875.5
dengamma value 1.087017
dengamma value 1.059792
dengamma value 1.046501
dengamma value 1.073579
dengamma value 1.095312
dengamma value 1.055991
dengamma value 1.085752
dengamma value 1.068742
dengamma value 1.091071
dengamma value 1.032232
dengamma value 1.084744
dengamma value 1.105438
dengamma value 1.041391
dengamma value 0.937120
dengamma value 1.102653
dengamma value 1.053025
dengamma value 1.061745
dengamma value 1.016491
dengamma value 1.044234
dengamma value 1.025076
dengamma value 1.032975
08/16/2016 03:05:32:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08295637 * 6208; err = 0.31765464 * 6208; time = 1.1054s; samplesPerSecond = 5616.1
dengamma value 1.085692
dengamma value 1.075794
dengamma value 1.118479
dengamma value 1.067378
dengamma value 0.945208
dengamma value 1.050391
dengamma value 1.078381
dengamma value 1.099215
dengamma value 1.086438
dengamma value 1.049932
dengamma value 1.063138
dengamma value 1.130924
dengamma value 1.028988
dengamma value 1.191101
dengamma value 1.027579
dengamma value 1.088230
dengamma value 1.089141
dengamma value 1.083345
dengamma value 1.072232
dengamma value 1.090651
dengamma value 1.135706
dengamma value 1.127724
08/16/2016 03:05:33:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08117240 * 6326; err = 0.29687006 * 6326; time = 1.1197s; samplesPerSecond = 5649.8
dengamma value 1.085506
dengamma value 1.071655
dengamma value 1.116561
dengamma value 1.135969
dengamma value 1.003626
dengamma value 1.027538
dengamma value 1.104102
08/16/2016 03:05:34: Finished Epoch[ 1 of 3]: [Training] ce = 0.08156704 * 81936; err = 0.31603935 * 81936; totalSamplesSeen = 81936; learningRatePerSample = 2e-006; epochTime=22.2988s
08/16/2016 03:05:34: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.1'

08/16/2016 03:05:34: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81936), data subset 0 of 1, with 1 datapasses

08/16/2016 03:05:34: Starting minibatch loop.
dengamma value 1.145242
dengamma value 1.085012
dengamma value 1.160179
dengamma value 1.053052
dengamma value 1.116928
dengamma value 1.056254
dengamma value 1.095588
dengamma value 1.143948
dengamma value 1.044307
dengamma value 1.043959
dengamma value 1.020891
dengamma value 1.092130
dengamma value 1.081158
dengamma value 1.091468
dengamma value 1.055750
dengamma value 1.017632
dengamma value 1.057541
dengamma value 1.110858
dengamma value 1.063133
dengamma value 1.071745
dengamma value 1.083208
dengamma value 1.134414
dengamma value 1.035728
dengamma value 1.045233
08/16/2016 03:05:35:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08368398 * 6182; err = 0.30071174 * 6182; time = 1.1767s; samplesPerSecond = 5253.9
dengamma value 1.046536
dengamma value 1.004586
dengamma value 1.288806
dengamma value 1.035090
dengamma value 1.095160
dengamma value 1.119750
dengamma value 1.107882
dengamma value 1.040208
dengamma value 1.045552
dengamma value 1.080889
dengamma value 1.044524
dengamma value 1.125897
dengamma value 1.135680
dengamma value 1.026725
dengamma value 1.036926
dengamma value 1.092974
dengamma value 1.024887
dengamma value 1.115769
dengamma value 1.044357
dengamma value 1.026637
dengamma value 1.048265
dengamma value 1.113920
08/16/2016 03:05:36:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08501333 * 5736; err = 0.28417015 * 5736; time = 1.0351s; samplesPerSecond = 5541.5
dengamma value 1.010748
dengamma value 1.110667
dengamma value 1.085324
dengamma value 1.081510
dengamma value 1.057740
dengamma value 1.163440
dengamma value 1.024172
dengamma value 1.126877
dengamma value 1.054233
dengamma value 1.054708
dengamma value 1.047825
dengamma value 1.056014
dengamma value 1.107508
dengamma value 1.188774
dengamma value 1.061890
dengamma value 1.168044
dengamma value 1.112633
dengamma value 1.068623
dengamma value 1.072696
dengamma value 1.098664
dengamma value 0.974260
dengamma value 0.964481
dengamma value 1.066706
08/16/2016 03:05:37:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08160518 * 6244; err = 0.30172966 * 6244; time = 1.0844s; samplesPerSecond = 5758.3
dengamma value 1.105943
dengamma value 1.116022
dengamma value 1.088509
dengamma value 1.058934
dengamma value 1.161254
dengamma value 1.097798
dengamma value 1.077121
dengamma value 1.044863
dengamma value 1.070741
dengamma value 1.016574
dengamma value 1.093447
dengamma value 1.126311
dengamma value 1.119959
dengamma value 1.129718
dengamma value 1.017900
dengamma value 1.132437
dengamma value 1.024505
dengamma value 1.020736
dengamma value 1.078649
dengamma value 1.095957
dengamma value 1.025882
dengamma value 1.065037
dengamma value 1.176504
dengamma value 1.076838
dengamma value 1.056696
08/16/2016 03:05:38:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08490154 * 6280; err = 0.31242038 * 6280; time = 1.0501s; samplesPerSecond = 5980.5
dengamma value 1.018347
dengamma value 1.082831
dengamma value 1.075497
dengamma value 1.085046
dengamma value 1.063649
dengamma value 1.074265
dengamma value 1.067420
dengamma value 1.053149
dengamma value 1.117731
dengamma value 1.072480
dengamma value 1.076274
dengamma value 1.046603
dengamma value 1.115489
dengamma value 1.067307
dengamma value 1.103777
dengamma value 1.085866
dengamma value 1.144047
dengamma value 1.088565
dengamma value 1.087098
dengamma value 1.063796
dengamma value 1.100174
dengamma value 1.084196
dengamma value 1.102490
dengamma value 1.084687
dengamma value 1.092789
dengamma value 1.103273
08/16/2016 03:05:39:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.07733347 * 7428; err = 0.29806139 * 7428; time = 1.3309s; samplesPerSecond = 5581.2
dengamma value 1.142144
dengamma value 1.081073
dengamma value 1.051410
dengamma value 1.134920
dengamma value 1.054180
dengamma value 1.033419
dengamma value 1.037870
dengamma value 1.091124
dengamma value 1.059701
dengamma value 1.061054
dengamma value 1.035654
dengamma value 1.057789
dengamma value 1.026473
dengamma value 1.151943
dengamma value 1.083800
dengamma value 1.082665
dengamma value 1.089776
dengamma value 0.975704
dengamma value 1.056808
dengamma value 1.071527
dengamma value 1.119046
dengamma value 1.022266
dengamma value 1.057294
08/16/2016 03:05:40:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08150286 * 6994; err = 0.31855876 * 6994; time = 1.1288s; samplesPerSecond = 6196.2
dengamma value 1.113186
dengamma value 1.079921
dengamma value 1.175955
dengamma value 1.024600
dengamma value 1.053710
dengamma value 1.130235
dengamma value 0.975800
dengamma value 1.141175
dengamma value 1.185706
dengamma value 1.065715
dengamma value 1.152167
dengamma value 1.111953
dengamma value 1.060055
dengamma value 1.027962
dengamma value 1.074742
dengamma value 1.093271
dengamma value 1.048562
dengamma value 1.041243
dengamma value 1.095344
dengamma value 1.088717
dengamma value 1.070026
dengamma value 1.152819
dengamma value 1.080982
dengamma value 1.101065
08/16/2016 03:05:42:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07725126 * 6572; err = 0.30340840 * 6572; time = 1.1671s; samplesPerSecond = 5631.1
dengamma value 1.097197
dengamma value 1.021022
dengamma value 1.078317
dengamma value 1.085192
dengamma value 1.072534
dengamma value 1.117073
dengamma value 1.200122
dengamma value 1.072119
dengamma value 1.074173
dengamma value 1.060047
dengamma value 1.006856
dengamma value 1.043448
dengamma value 1.018330
dengamma value 1.178684
dengamma value 1.084694
dengamma value 0.941980
dengamma value 1.006588
dengamma value 1.032836
dengamma value 1.074309
dengamma value 1.085413
dengamma value 1.216896
dengamma value 1.015795
08/16/2016 03:05:43:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08773969 * 5506; err = 0.32909553 * 5506; time = 0.9407s; samplesPerSecond = 5853.1
dengamma value 1.002740
dengamma value 1.074720
dengamma value 1.015641
dengamma value 1.082433
dengamma value 1.124583
dengamma value 1.121300
dengamma value 1.129819
dengamma value 1.078518
dengamma value 1.040683
dengamma value 1.013662
dengamma value 1.096474
dengamma value 0.958585
dengamma value 1.076417
dengamma value 1.027448
dengamma value 1.066752
dengamma value 1.115725
dengamma value 1.114337
dengamma value 1.090009
dengamma value 1.107314
dengamma value 1.006038
dengamma value 1.001244
08/16/2016 03:05:44:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08559584 * 5628; err = 0.33688699 * 5628; time = 0.9724s; samplesPerSecond = 5787.5
dengamma value 1.039761
dengamma value 1.123939
dengamma value 1.071136
dengamma value 1.098651
dengamma value 1.086919
dengamma value 0.986629
dengamma value 1.035707
dengamma value 1.000715
dengamma value 1.104032
dengamma value 1.055806
dengamma value 1.166729
dengamma value 1.092039
dengamma value 1.083523
dengamma value 1.073980
dengamma value 1.020984
dengamma value 1.043832
dengamma value 1.045460
dengamma value 1.060373
dengamma value 1.059040
dengamma value 1.135645
dengamma value 1.018242
dengamma value 1.030202
dengamma value 1.055369
dengamma value 0.988205
08/16/2016 03:05:45:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08217202 * 6032; err = 0.32625995 * 6032; time = 1.0083s; samplesPerSecond = 5982.6
dengamma value 1.040056
dengamma value 1.059124
dengamma value 1.058682
dengamma value 1.091049
dengamma value 1.067363
dengamma value 1.119150
dengamma value 1.081032
dengamma value 0.965229
dengamma value 1.122860
dengamma value 1.133967
dengamma value 1.037526
dengamma value 0.968076
dengamma value 1.059440
dengamma value 1.140410
dengamma value 1.086110
dengamma value 0.997839
dengamma value 1.165703
dengamma value 1.048398
dengamma value 1.104210
dengamma value 1.028737
08/16/2016 03:05:45:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08802488 * 4790; err = 0.33799582 * 4790; time = 0.8520s; samplesPerSecond = 5621.8
dengamma value 1.015772
dengamma value 0.967842
dengamma value 1.055026
dengamma value 1.060577
dengamma value 1.081279
dengamma value 1.012655
dengamma value 1.010070
dengamma value 1.089346
dengamma value 1.075223
dengamma value 0.957264
dengamma value 1.129906
dengamma value 1.057574
dengamma value 1.077032
dengamma value 1.016719
dengamma value 1.128840
dengamma value 1.068581
dengamma value 1.133210
dengamma value 1.167371
dengamma value 0.961193
dengamma value 1.143450
dengamma value 1.034745
dengamma value 1.034677
08/16/2016 03:05:46:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08562207 * 5986; err = 0.34513866 * 5986; time = 1.0006s; samplesPerSecond = 5982.5
dengamma value 1.027943
dengamma value 1.006940
dengamma value 1.054680
dengamma value 1.089151
dengamma value 1.029769
dengamma value 1.217527
dengamma value 1.101724
dengamma value 1.113608
dengamma value 1.074514
dengamma value 1.081673
dengamma value 1.094728
dengamma value 1.087523
dengamma value 1.003731
dengamma value 0.994219
dengamma value 1.028819
dengamma value 1.075157
dengamma value 1.049089
dengamma value 1.053537
dengamma value 1.120696
dengamma value 1.045499
dengamma value 1.108837
dengamma value 1.059376
dengamma value 1.091137
dengamma value 0.994257
08/16/2016 03:05:48:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08639331 * 7022; err = 0.31444033 * 7022; time = 1.1430s; samplesPerSecond = 6143.4
dengamma value 1.034703
dengamma value 1.093763
dengamma value 1.188881
dengamma value 1.072499
dengamma value 1.043478
dengamma value 1.085861
dengamma value 1.015125
dengamma value 1.085830
dengamma value 1.085830
08/16/2016 03:05:48: Finished Epoch[ 2 of 3]: [Training] ce = 0.08332526 * 82462; err = 0.31518760 * 82462; totalSamplesSeen = 164398; learningRatePerSample = 2e-006; epochTime=14.2534s
08/16/2016 03:05:48: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.2'

08/16/2016 03:05:48: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 164102), data subset 0 of 1, with 1 datapasses

08/16/2016 03:05:48: Starting minibatch loop.
dengamma value 1.088197
dengamma value 1.100274
dengamma value 1.084374
dengamma value 1.104666
dengamma value 1.080939
dengamma value 1.060389
dengamma value 1.150334
dengamma value 1.039812
dengamma value 1.112423
dengamma value 0.985965
dengamma value 1.117210
dengamma value 1.071346
dengamma value 1.071001
dengamma value 1.100134
dengamma value 1.077029
dengamma value 1.060612
dengamma value 1.105358
dengamma value 1.011032
dengamma value 1.095015
dengamma value 1.093480
dengamma value 1.050025
dengamma value 1.071393
dengamma value 1.042980
dengamma value 1.039124
08/16/2016 03:05:49:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08358905 * 6292; err = 0.30101716 * 6292; time = 1.0632s; samplesPerSecond = 5918.0
dengamma value 1.160758
dengamma value 1.035082
dengamma value 0.988546
dengamma value 1.101547
dengamma value 1.082563
dengamma value 1.025480
dengamma value 1.047410
dengamma value 1.136445
dengamma value 1.040095
dengamma value 0.976965
dengamma value 0.961430
dengamma value 1.100491
dengamma value 1.068102
dengamma value 1.070083
dengamma value 1.033162
dengamma value 1.068287
dengamma value 1.011897
dengamma value 1.077012
dengamma value 1.112298
dengamma value 1.034657
dengamma value 1.074293
dengamma value 1.025731
08/16/2016 03:05:50:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07963296 * 6596; err = 0.33110976 * 6596; time = 1.1251s; samplesPerSecond = 5862.6
dengamma value 1.085887
dengamma value 1.006123
dengamma value 1.100073
dengamma value 1.118208
dengamma value 1.071417
dengamma value 1.167362
dengamma value 1.031790
dengamma value 1.079441
dengamma value 1.055116
dengamma value 1.062575
dengamma value 1.106028
dengamma value 1.058904
dengamma value 1.022302
dengamma value 1.037796
dengamma value 1.120612
dengamma value 0.954849
dengamma value 1.048382
dengamma value 1.034218
dengamma value 1.031126
dengamma value 1.173674
dengamma value 1.070757
dengamma value 1.203380
08/16/2016 03:05:51:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08600026 * 5666; err = 0.29844688 * 5666; time = 1.0024s; samplesPerSecond = 5652.2
dengamma value 1.131507
dengamma value 1.027065
dengamma value 1.060685
dengamma value 1.093865
dengamma value 1.081544
dengamma value 1.078910
dengamma value 1.105020
dengamma value 1.070809
dengamma value 1.094553
dengamma value 1.076389
dengamma value 1.105420
dengamma value 1.050209
dengamma value 1.002027
dengamma value 1.128992
dengamma value 1.124045
dengamma value 1.039761
dengamma value 1.086744
dengamma value 1.040163
dengamma value 1.089048
dengamma value 1.037200
dengamma value 1.085009
dengamma value 1.031807
08/16/2016 03:05:52:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08130104 * 6626; err = 0.28327800 * 6626; time = 1.1615s; samplesPerSecond = 5704.6
dengamma value 1.094679
dengamma value 1.037563
dengamma value 1.033161
dengamma value 1.093461
dengamma value 1.125289
dengamma value 1.151998
dengamma value 1.093540
dengamma value 1.099577
dengamma value 1.029092
dengamma value 1.087039
dengamma value 1.145239
dengamma value 1.075084
dengamma value 1.043946
dengamma value 1.067017
dengamma value 1.024815
dengamma value 1.064146
dengamma value 1.083589
dengamma value 1.064345
dengamma value 1.111978
dengamma value 1.106094
dengamma value 1.074091
dengamma value 1.080812
dengamma value 1.068420
dengamma value 1.059111
08/16/2016 03:05:53:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08144237 * 5652; err = 0.30042463 * 5652; time = 1.0581s; samplesPerSecond = 5341.8
dengamma value 1.092134
dengamma value 1.047652
dengamma value 1.002004
dengamma value 1.048934
dengamma value 1.037261
dengamma value 1.150848
dengamma value 1.047778
dengamma value 1.048185
dengamma value 1.106732
dengamma value 1.022934
dengamma value 1.029545
dengamma value 1.055188
dengamma value 1.074436
dengamma value 1.073219
dengamma value 1.089149
dengamma value 1.040302
dengamma value 1.009003
dengamma value 1.036303
dengamma value 1.098994
dengamma value 1.043439
dengamma value 1.056252
08/16/2016 03:05:55:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08578500 * 6588; err = 0.33986035 * 6588; time = 1.1292s; samplesPerSecond = 5834.0
dengamma value 1.065623
dengamma value 1.096644
dengamma value 1.041698
dengamma value 0.983649
dengamma value 1.153678
dengamma value 1.073628
dengamma value 1.081920
dengamma value 1.037372
dengamma value 1.007501
dengamma value 1.093147
dengamma value 1.082585
dengamma value 1.026000
dengamma value 1.028268
dengamma value 1.123083
dengamma value 1.135424
dengamma value 1.056123
dengamma value 1.042214
dengamma value 1.079012
dengamma value 1.108924
dengamma value 1.058956
dengamma value 1.028283
08/16/2016 03:05:56:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08631525 * 6328; err = 0.30941846 * 6328; time = 1.1347s; samplesPerSecond = 5576.6
dengamma value 1.072576
dengamma value 1.097221
dengamma value 1.011787
dengamma value 1.103157
dengamma value 1.126538
dengamma value 1.070615
dengamma value 1.089780
dengamma value 1.109849
dengamma value 1.052071
dengamma value 0.991507
dengamma value 1.060412
dengamma value 1.032442
dengamma value 1.022123
dengamma value 1.104983
dengamma value 1.048826
dengamma value 1.083207
dengamma value 1.072539
dengamma value 1.192497
dengamma value 1.086666
dengamma value 1.062849
dengamma value 1.065174
dengamma value 1.108937
dengamma value 1.036068
dengamma value 1.100262
dengamma value 1.077704
08/16/2016 03:05:57:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08290498 * 6980; err = 0.28839542 * 6980; time = 1.1987s; samplesPerSecond = 5823.1
dengamma value 1.050160
dengamma value 0.978864
dengamma value 0.951252
dengamma value 1.137689
dengamma value 1.125302
dengamma value 1.023794
dengamma value 1.281458
dengamma value 1.102675
dengamma value 1.066576
dengamma value 1.116553
dengamma value 0.979398
dengamma value 1.075125
dengamma value 1.059821
dengamma value 1.015062
dengamma value 1.119501
dengamma value 1.061463
dengamma value 1.002527
dengamma value 1.284401
dengamma value 1.185396
dengamma value 1.049221
dengamma value 1.136681
dengamma value 0.981109
dengamma value 1.022418
08/16/2016 03:05:58:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.07985366 * 6774; err = 0.31251845 * 6774; time = 1.0624s; samplesPerSecond = 6376.3
dengamma value 0.991020
dengamma value 1.080265
dengamma value 1.125530
dengamma value 1.028283
dengamma value 0.931542
dengamma value 1.073510
dengamma value 1.040909
dengamma value 1.068603
dengamma value 0.930335
dengamma value 1.068783
dengamma value 1.012925
dengamma value 1.095008
dengamma value 1.032397
dengamma value 0.980707
dengamma value 1.071785
dengamma value 0.933892
dengamma value 1.054113
dengamma value 1.016794
dengamma value 1.071612
dengamma value 1.069752
dengamma value 1.103920
dengamma value 1.082666
08/16/2016 03:05:59:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08853470 * 6146; err = 0.35388871 * 6146; time = 1.0887s; samplesPerSecond = 5645.2
dengamma value 1.017763
dengamma value 1.131387
dengamma value 1.059096
dengamma value 1.147638
dengamma value 0.959775
dengamma value 1.082525
dengamma value 1.071139
dengamma value 1.150287
dengamma value 1.107909
dengamma value 1.082605
dengamma value 1.038306
dengamma value 1.014547
dengamma value 1.165069
dengamma value 1.063468
dengamma value 1.117538
dengamma value 1.068459
dengamma value 1.086083
dengamma value 1.095840
dengamma value 1.014370
dengamma value 1.139207
dengamma value 1.066824
dengamma value 1.079760
08/16/2016 03:06:00:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08208266 * 5366; err = 0.30301901 * 5366; time = 0.9817s; samplesPerSecond = 5466.2
dengamma value 1.061395
dengamma value 1.047785
dengamma value 1.090933
dengamma value 1.070985
dengamma value 1.064221
dengamma value 1.015890
dengamma value 1.125446
dengamma value 1.071238
dengamma value 1.019477
dengamma value 1.031274
dengamma value 1.034352
dengamma value 1.063972
dengamma value 1.058113
dengamma value 1.040602
dengamma value 1.091374
dengamma value 1.086140
dengamma value 1.112651
dengamma value 1.103647
dengamma value 1.057306
dengamma value 0.988464
dengamma value 1.036586
dengamma value 1.127764
dengamma value 1.148842
dengamma value 1.096221
dengamma value 1.088360
08/16/2016 03:06:01:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08114245 * 6220; err = 0.32250804 * 6220; time = 1.0576s; samplesPerSecond = 5881.1
dengamma value 1.082362
dengamma value 1.051853
dengamma value 1.113975
dengamma value 1.084998
dengamma value 1.030133
dengamma value 1.070156
dengamma value 1.075748
dengamma value 1.107124
dengamma value 1.086904
dengamma value 1.084355
dengamma value 1.043573
dengamma value 1.068356
dengamma value 1.032575
dengamma value 1.093060
dengamma value 1.160388
dengamma value 1.106279
dengamma value 1.066269
dengamma value 0.998378
dengamma value 1.082442
dengamma value 1.033566
dengamma value 1.083996
dengamma value 1.100964
dengamma value 1.142857
08/16/2016 03:06:02:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08458937 * 6564; err = 0.30042657 * 6564; time = 1.1086s; samplesPerSecond = 5920.8
08/16/2016 03:06:02: Finished Epoch[ 3 of 3]: [Training] ce = 0.08329045 * 81798; err = 0.31113230 * 81798; totalSamplesSeen = 246196; learningRatePerSample = 2e-006; epochTime=14.1742s
08/16/2016 03:06:02: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence'
08/16/2016 03:06:02: CNTKCommandTrainEnd: sequenceTrain

08/16/2016 03:06:02: Action "train" complete.

08/16/2016 03:06:02: __COMPLETED__
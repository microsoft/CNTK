precision = "float"
command = speechTrain
deviceId = $DeviceId$

parallelTrain = true

speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.dnn"
    deviceId = $DeviceId$
    traceLevel = 1

    SimpleNetworkBuilder = [
        layerSizes = 363:512:512:132
        trainingCriterion = "CrossEntropyWithSoftmax"
        evalCriterion = "ClassificationError"
        layerTypes = "Sigmoid"
        initValueScale = 1.0
        applyMeanVarNorm = true
        uniformInit = true
        needPrior = true
    ]

    ExperimentalNetworkBuilder = [    // the same as above but with BS. Not active; activate by commenting out the SimpleNetworkBuilder entry above
        layerSizes = 363:512:512:132
        trainingCriterion = 'CE'
        evalCriterion = 'Err'

        applyMeanVarNorm = true

        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = if trainingCriterion == 'CE'
             then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
             else Fail('unknown trainingCriterion ' + trainingCriterion)
        Err = if evalCriterion == 'Err' then
              ClassificationError(labels, outZ, tag='evaluation')
              else Fail('unknown evalCriterion ' + evalCriterion)
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]

    SGD = [
        epochSize = 20480
        minibatchSize = 64:256:1024
        learningRatesPerMB = 1.0:0.5:0.1
        numMBsToShowResult = 3
        momentumPerMB = 0.9:0.656119
        dropoutRate = 0.0
        maxEpochs = 5
        keepCheckPointFiles = true
        clippingThresholdPerSample = 1#INF

        ParallelTrain = [
            parallelizationMethod = "BlockMomentumSGD"
            distributedMBReading = true
            syncPerfStats=1
            BlockMomentumSGD = [
                blockSizePerWorker=2048
                resetSGDMomentum=true
                useNesterovMomentum=true
            ]
        ]

        AutoAdjust = [
            reduceLearnRateIfImproveLessThan = 0
            loadBestModel = true
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            autoAdjustLR = "adjustAfterEpoch"
        ]
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        useMersenneTwisterRand=true

        features = [
            dim = 363
            type = "real"
            scpFile = "glob_0000.scp"
        ]
    
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
    ]
]

dataDir: ../../Data
tags:
     # running on every BVT job in 'P' (Speech) leg in Debug-GPU and Release-CPU configurations:
     - bvt-p  ((build_sku == 'gpu') or (build_sku == '1bitsgd')) and ((flavor=='debug') ^ (device=='cpu'))
     # running unconditionally on every Nightly job in 'P' leg
     - nightly-p ((build_sku == 'gpu') or (build_sku == '1bitsgd'))

testCases:
  Must train epochs in exactly same order and parameters for each MPI Rank:
    patterns:
      - ^MPI Rank {{integer}}
      - Starting Epoch {{integer}}
      - learning rate per sample = {{float}}
      - momentum = {{float}}

  Epochs must be finished with expected results for each MPI Rank for training:
    patterns:
      - ^MPI Rank {{integer}}
      - Finished Epoch[{{integer}} of {{integer}}]
      - Training
      - CrossEntropyWithSoftmax = {{float,tolerance=0%}}
      - EvalClassificationError = {{float,tolerance=0%}}
      - learningRatePerSample = {{float,tolerance=0.001%}}

  Epochs must be finished with expected results for each MPI Rank for CV:
    patterns:
      - ^MPI Rank {{integer}}
      - Finished Epoch[{{integer}} of {{integer}}]
      - Validate
      - CrossEntropyWithSoftmax = {{float,tolerance=0%}}
      - EvalClassificationError = {{float,tolerance=0%}}

  Per-minibatch training results must match for each MPI Rank:
    patterns:
      - ^MPI Rank {{integer}}
      - Epoch[{{integer}} of {{integer}}]-Minibatch[{{integer}}-{{integer}}
      - " * {{integer}}; "
      - CrossEntropyWithSoftmax = {{float,tolerance=0%}}
      - EvalClassificationError = {{float,tolerance=0%}}

  DataParallelSGD training parameters must match for each MPI Rank:
    patterns:
      - ^MPI Rank {{integer}}
      - Starting minibatch loop
      - DataParallelSGD training
      - myRank = {{integer}}
      - numNodes = 2
      - numGradientBits = 64
      - distributed reading is ENABLED

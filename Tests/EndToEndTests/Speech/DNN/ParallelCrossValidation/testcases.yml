dataDir: ../../Data
tags:
     # running on every BVT job in 'P' (Speech) leg in Debug-GPU and Release-CPU configurations:
     - bvt-p  ((build_sku == 'gpu') or (build_sku == '1bitsgd')) and ((flavor=='debug') ^ (device=='cpu'))
     # running unconditionally on every Nightly job in 'P' leg
     - nightly-p ((build_sku == 'gpu') or (build_sku == '1bitsgd'))

testCases:
  Must train epochs in exactly same order and parameters for each MPI Rank:
    patterns:
      - ^MPI Rank {{integer}}
      - Starting Epoch {{integer}}
      - learning rate per sample = {{float}}
      - momentum = {{float}}

  Epochs must be finished with expected results for each MPI Rank for training:
    patterns:
      - ^MPI Rank {{integer}}
      - Finished Epoch[{{integer}} of {{integer}}]
      - Training Set
      - TrainLossPerSample = {{float,tolerance=0%}}
      - EvalErrPerSample = {{float,tolerance=0%}}
      - AvgLearningRatePerSample = {{float,tolerance=0.001%}}

  Epochs must be finished with expected results for each MPI Rank for CV:
    patterns:
      - ^MPI Rank {{integer}}
      - Finished Epoch[{{integer}} of {{integer}}]
      - Validation Set
      - TrainLossPerSample = {{float,tolerance=0%}}
      - EvalErrPerSample = {{float,tolerance=0%}}

  Per-minibatch training results must match for each MPI Rank:
    patterns:
      - ^MPI Rank {{integer}}
      - Epoch[{{integer}} of {{integer}}]-Minibatch[{{integer}}-{{integer}}
      - SamplesSeen = {{integer}}
      - TrainLossPerSample = {{float,tolerance=0%}}
      - EvalErr[0]PerSample = {{float,tolerance=0%}}

  DataParallelSGD training parameters must match for each MPI Rank:
    patterns:
      - ^MPI Rank {{integer}}
      - Starting minibatch loop
      - DataParallelSGD training
      - MyRank = {{integer}}
      - NumNodes = 2
      - NumGradientBits = 64
      - distributed reading is ENABLED

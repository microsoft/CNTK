# end-to-end test for recurrent LSTM for speech

precision = "float"
deviceId = $DeviceId$

command = speechTrain

// Note: These options are overridden from the command line in some test cases.
frameMode = false
truncated = true
parallelTrain = false

speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.dnn"
    #deviceId = $DeviceId$
    traceLevel = 1
    
    SGD = [
        epochSize = 20480
        minibatchSize = 20
        learningRatesPerMB = 0.5
        numMBsToShowResult = 10
        momentumPerMB = 0:0.9
        maxEpochs = 4
        keepCheckPointFiles = true       
    ]

    reader = [
        verbosity = 0
        randomize = true

        # Description of the corpus – a file will contains all logical sequence keys that identify the same sequence across different deserializers.
        # Possibly support other types of parameters in the future (such as regex, etc.). For now just file with id per line.
        corpus = [
        ]

        # A list of deserializers to use.
        # TODO: change this to readers/dataProviders(?)
        deserializers = [
            [   
                type = "HTKDataDeserializer"        
                module = "ExperimentalHTKMLFReader"

                # Description of input streams
                inputs = [
                    # Description of a particular input stream
                    features = [
                        dim = 363
                        scpFile = "$DataDir$/glob_0000.scp"
                    ]
                ]
            ]:
            [
                type = "MLFDataDeserializer"                
                module = "ExperimentalHTKMLFReader"
                inputs = [
                    labels = [
                        mlfFile = "$DataDir$/glob_0000.mlf"
                        labelMappingFile = "$DataDir$/state.list"
                        scpFile = "$DataDir$/glob_0000.scp"
                        dim = 132
                    ]
                ]
            ]
        ]
    ]

    # define network using BrainScript
    BrainScriptNetworkBuilder = [

        # import some namespaces
        # TODO: allow to say import BS.RNNs LSTMP or import BS.RNNs to import all (literally creates new dict members mirroring those)
        RecurrentLSTMP2 = BS.RNNs.RecurrentLSTMP2
        Parameters = BS.Parameters

        useSelfStabilization = true

        // define basic I/O
        baseFeatDim = 33
        featDim = 11 * baseFeatDim
        labelDim = 132

        // hidden dimensions
        innerCellDim  = 1024
        hiddenDim     = 256
        numLSTMLayers = 3        // number of hidden LSTM model layers

        // features
        features = Input((1 : featDim),  tag='feature') // TEST: Artificially reading data transposed
        realFeatures = Transpose (features)             //       and swapping them back to (featDim:1), for testing Transpose()
        labels   = Input(labelDim, tag='label')
        feashift = RowSlice(featDim - baseFeatDim, baseFeatDim, realFeatures);

        featNorm = MeanVarNorm(feashift)

        // define the stack of hidden LSTM layers
        LSTMoutput[k:1..numLSTMLayers] =
            if k == 1
            then RecurrentLSTMP2 (hiddenDim, cellDim=innerCellDim, featNorm,        inputDim=baseFeatDim, enableSelfStabilization=useSelfStabilization).h
            else RecurrentLSTMP2 (hiddenDim, cellDim=innerCellDim, LSTMoutput[k-1], inputDim=hiddenDim,   enableSelfStabilization=useSelfStabilization).h

        // and add a softmax layer on top
        W(x) = Parameters.WeightParam (labelDim, hiddenDim) * Parameters.Stabilize (x, enabled=useSelfStabilization)
        B = Parameters.BiasParam (labelDim)

        z = W(LSTMoutput[numLSTMLayers]) + B; // top-level input to Softmax

        // training
        useExplicitCriterion = true
        crNode = CrossEntropyWithSoftmax(labels, z)                 // this is the objective, as a node
        crExplicit = -(ReducePlus (labels .* LogSoftmax (z)))        // manually-defined per-sample objective
        cr = Pass (if useExplicitCriterion then crExplicit else crNode, tag='criterion')
        Err = ErrorPrediction(labels, z, tag='evaluation')          // this also gets tracked

        // decoding
        logPrior = LogPrior(labels)	 
        ScaledLogLikelihood = Minus(z, logPrior, tag='output')      // sadly we can't say x - y since we want to assign a tag
    ]
]

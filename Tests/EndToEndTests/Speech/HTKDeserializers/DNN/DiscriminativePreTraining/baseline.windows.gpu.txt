CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3565 @ 3.20GHz
    Hardware threads: 8
    Total Memory: 12580436 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug 16 2016 02:54:53
		Last modified date: Fri Aug 12 05:31:21 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: c:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
		Built by svcphil on Philly-Pool3
		Build Path: c:\Jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
08/16/2016 03:02:01: -------------------------------------------------------------------
08/16/2016 03:02:01: Build info: 

08/16/2016 03:02:01: 		Built time: Aug 16 2016 02:54:53
08/16/2016 03:02:01: 		Last modified date: Fri Aug 12 05:31:21 2016
08/16/2016 03:02:01: 		Build type: Release
08/16/2016 03:02:01: 		Build target: GPU
08/16/2016 03:02:01: 		With 1bit-SGD: no
08/16/2016 03:02:01: 		Math lib: mkl
08/16/2016 03:02:01: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
08/16/2016 03:02:01: 		CUB_PATH: c:\src\cub-1.4.1
08/16/2016 03:02:01: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
08/16/2016 03:02:01: 		Build Branch: HEAD
08/16/2016 03:02:01: 		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
08/16/2016 03:02:01: 		Built by svcphil on Philly-Pool3
08/16/2016 03:02:01: 		Build Path: c:\Jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
08/16/2016 03:02:01: -------------------------------------------------------------------
08/16/2016 03:02:02: -------------------------------------------------------------------
08/16/2016 03:02:02: GPU info:

08/16/2016 03:02:02: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
08/16/2016 03:02:02: -------------------------------------------------------------------

08/16/2016 03:02:02: Running on cntk-muc01 at 2016/08/16 03:02:02
08/16/2016 03:02:02: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu  DeviceId=0  timestamping=true



08/16/2016 03:02:02: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/16/2016 03:02:02: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    useMersenneTwisterRand=true
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 03:02:02: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/16/2016 03:02:02: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/16/2016 03:02:02: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    useMersenneTwisterRand=true
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 03:02:02: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/16/2016 03:02:02: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    useMersenneTwisterRand=true
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
08/16/2016 03:02:02: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/16/2016 03:02:02: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
08/16/2016 03:02:02: Precision = "float"
08/16/2016 03:02:02: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
08/16/2016 03:02:02: CNTKCommandTrainInfo: dptPre1 : 2
08/16/2016 03:02:02: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
08/16/2016 03:02:02: CNTKCommandTrainInfo: dptPre2 : 2
08/16/2016 03:02:02: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
08/16/2016 03:02:02: CNTKCommandTrainInfo: speechTrain : 4
08/16/2016 03:02:02: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

08/16/2016 03:02:02: ##############################################################################
08/16/2016 03:02:02: #                                                                            #
08/16/2016 03:02:02: # Action "train"                                                             #
08/16/2016 03:02:02: #                                                                            #
08/16/2016 03:02:02: ##############################################################################

08/16/2016 03:02:02: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 03:02:02: Creating virgin network.
Node 'globalMean' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalInvStd' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalPrior' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- 0.000000.
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- 0.000000.
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- uniform(seed=1, range=0.050000*1.000000, onCPU=false).
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=2, range=0.050000*1.000000, onCPU=false).
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- uniform(seed=3, range=0.050000*1.000000, onCPU=false).
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- uniform(seed=4, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 03:02:03: Created model with 19 nodes on GPU 0.

08/16/2016 03:02:03: Training criterion node(s):
08/16/2016 03:02:03: 	ce = CrossEntropyWithSoftmax

08/16/2016 03:02:03: Evaluation criterion node(s):
08/16/2016 03:02:03: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 29 matrices, 11 are shared as 5, and 18 are not shared.

	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *] }
	{ HL1.z : [512 x 1 x *] (gradient)
	  OL.t : [132 x 1 x *] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *] }
	{ HL1.t : [512 x *] (gradient)
	  HL1.y : [512 x 1 x *] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] (gradient) }


08/16/2016 03:02:03: Training 254084 parameters in 4 out of 4 parameter tensors and 10 nodes with gradient:

08/16/2016 03:02:03: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 03:02:03: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:02:03: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 03:02:03: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 03:02:03: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 03:02:03: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 03:02:03: Starting minibatch loop.
08/16/2016 03:02:03:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.77545395 * 2560; err = 0.83984375 * 2560; time = 0.2561s; samplesPerSecond = 9994.5
08/16/2016 03:02:03:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.92129135 * 2560; err = 0.69921875 * 2560; time = 0.0199s; samplesPerSecond = 128656.1
08/16/2016 03:02:03:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.54243622 * 2560; err = 0.64882812 * 2560; time = 0.0198s; samplesPerSecond = 129195.1
08/16/2016 03:02:03:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.20117416 * 2560; err = 0.60156250 * 2560; time = 0.0200s; samplesPerSecond = 128166.6
08/16/2016 03:02:03:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.98474197 * 2560; err = 0.55273438 * 2560; time = 0.0203s; samplesPerSecond = 125885.1
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87129364 * 2560; err = 0.51562500 * 2560; time = 0.0203s; samplesPerSecond = 125897.5
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.83400879 * 2560; err = 0.52812500 * 2560; time = 0.0225s; samplesPerSecond = 113919.5
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.71646271 * 2560; err = 0.49335937 * 2560; time = 0.0232s; samplesPerSecond = 110525.9
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.66541901 * 2560; err = 0.46328125 * 2560; time = 0.0233s; samplesPerSecond = 109876.0
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.57725983 * 2560; err = 0.46054688 * 2560; time = 0.0231s; samplesPerSecond = 110659.6
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.61621094 * 2560; err = 0.45390625 * 2560; time = 0.0233s; samplesPerSecond = 110074.4
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.56063995 * 2560; err = 0.44140625 * 2560; time = 0.0233s; samplesPerSecond = 110060.2
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.52853241 * 2560; err = 0.44492188 * 2560; time = 0.0232s; samplesPerSecond = 110473.4
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.53460999 * 2560; err = 0.46210937 * 2560; time = 0.0231s; samplesPerSecond = 110741.0
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46377869 * 2560; err = 0.44140625 * 2560; time = 0.0233s; samplesPerSecond = 110102.8
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.43344116 * 2560; err = 0.42617187 * 2560; time = 0.0232s; samplesPerSecond = 110392.4
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.43216858 * 2560; err = 0.42148438 * 2560; time = 0.0232s; samplesPerSecond = 110268.8
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.37991028 * 2560; err = 0.41250000 * 2560; time = 0.0232s; samplesPerSecond = 110511.5
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.35844727 * 2560; err = 0.40039063 * 2560; time = 0.0232s; samplesPerSecond = 110192.8
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.44847107 * 2560; err = 0.42656250 * 2560; time = 0.0230s; samplesPerSecond = 111077.4
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.43933411 * 2560; err = 0.42539063 * 2560; time = 0.0232s; samplesPerSecond = 110145.4
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.41740417 * 2560; err = 0.42539063 * 2560; time = 0.0232s; samplesPerSecond = 110169.1
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.33209229 * 2560; err = 0.40468750 * 2560; time = 0.0233s; samplesPerSecond = 109734.7
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.36106567 * 2560; err = 0.40429688 * 2560; time = 0.0232s; samplesPerSecond = 110549.7
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.30946045 * 2560; err = 0.39804688 * 2560; time = 0.0232s; samplesPerSecond = 110392.4
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.25348206 * 2560; err = 0.37031250 * 2560; time = 0.0231s; samplesPerSecond = 110626.2
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.30354004 * 2560; err = 0.39687500 * 2560; time = 0.0233s; samplesPerSecond = 110041.3
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.36096191 * 2560; err = 0.40937500 * 2560; time = 0.0230s; samplesPerSecond = 111072.5
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.29523315 * 2560; err = 0.39375000 * 2560; time = 0.0233s; samplesPerSecond = 109682.9
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.35650330 * 2560; err = 0.41054687 * 2560; time = 0.0231s; samplesPerSecond = 110894.5
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.32445068 * 2560; err = 0.39843750 * 2560; time = 0.0230s; samplesPerSecond = 111212.5
08/16/2016 03:02:04:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.26812134 * 2560; err = 0.38203125 * 2560; time = 0.0228s; samplesPerSecond = 112463.2
08/16/2016 03:02:04: Finished Epoch[ 1 of 2]: [Training] ce = 1.65210629 * 81920; err = 0.46728516 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.11631s
08/16/2016 03:02:04: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/16/2016 03:02:04: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 03:02:04: Starting minibatch loop.
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.24494066 * 2560; err = 0.39062500 * 2560; time = 0.0244s; samplesPerSecond = 105077.4
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.23623838 * 2560; err = 0.36718750 * 2560; time = 0.0230s; samplesPerSecond = 111377.0
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.26030045 * 2560; err = 0.39296875 * 2560; time = 0.0231s; samplesPerSecond = 110942.6
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.26366539 * 2560; err = 0.38320312 * 2560; time = 0.0232s; samplesPerSecond = 110387.7
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.27961769 * 2560; err = 0.36914063 * 2560; time = 0.0232s; samplesPerSecond = 110306.8
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.17966232 * 2560; err = 0.35312500 * 2560; time = 0.0230s; samplesPerSecond = 111270.5
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.19451904 * 2560; err = 0.36640625 * 2560; time = 0.0240s; samplesPerSecond = 106453.8
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.22797241 * 2560; err = 0.37109375 * 2560; time = 0.0232s; samplesPerSecond = 110373.4
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.26700439 * 2560; err = 0.38750000 * 2560; time = 0.0234s; samplesPerSecond = 109415.7
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.24487915 * 2560; err = 0.37109375 * 2560; time = 0.0238s; samplesPerSecond = 107400.6
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.20520401 * 2560; err = 0.36796875 * 2560; time = 0.0236s; samplesPerSecond = 108520.6
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18870239 * 2560; err = 0.36289063 * 2560; time = 0.0240s; samplesPerSecond = 106617.8
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.16525116 * 2560; err = 0.36367187 * 2560; time = 0.0238s; samplesPerSecond = 107671.6
08/16/2016 03:02:04:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16569672 * 2560; err = 0.36523438 * 2560; time = 0.0238s; samplesPerSecond = 107766.8
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.14518127 * 2560; err = 0.34218750 * 2560; time = 0.0235s; samplesPerSecond = 109042.9
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.11732483 * 2560; err = 0.35195312 * 2560; time = 0.0237s; samplesPerSecond = 107944.0
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.17393036 * 2560; err = 0.35390625 * 2560; time = 0.0236s; samplesPerSecond = 108562.0
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.13165283 * 2560; err = 0.35703125 * 2560; time = 0.0230s; samplesPerSecond = 111130.4
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.14869232 * 2560; err = 0.35312500 * 2560; time = 0.0234s; samplesPerSecond = 109495.3
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11426697 * 2560; err = 0.33710937 * 2560; time = 0.0231s; samplesPerSecond = 110774.6
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.17790833 * 2560; err = 0.35585937 * 2560; time = 0.0241s; samplesPerSecond = 106043.7
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.15280457 * 2560; err = 0.35625000 * 2560; time = 0.0230s; samplesPerSecond = 111289.8
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.09865417 * 2560; err = 0.34687500 * 2560; time = 0.0231s; samplesPerSecond = 110760.2
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.13216553 * 2560; err = 0.35312500 * 2560; time = 0.0231s; samplesPerSecond = 110630.9
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09484863 * 2560; err = 0.34101562 * 2560; time = 0.0230s; samplesPerSecond = 111222.1
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.09128418 * 2560; err = 0.32539062 * 2560; time = 0.0230s; samplesPerSecond = 111202.8
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.12996826 * 2560; err = 0.35195312 * 2560; time = 0.0231s; samplesPerSecond = 111029.2
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.13035278 * 2560; err = 0.33945313 * 2560; time = 0.0231s; samplesPerSecond = 110976.2
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.11663208 * 2560; err = 0.34687500 * 2560; time = 0.0231s; samplesPerSecond = 110856.1
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.08121948 * 2560; err = 0.33750000 * 2560; time = 0.0231s; samplesPerSecond = 110904.1
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08856812 * 2560; err = 0.34531250 * 2560; time = 0.0230s; samplesPerSecond = 111135.2
08/16/2016 03:02:05:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07200928 * 2560; err = 0.32500000 * 2560; time = 0.0228s; samplesPerSecond = 112256.1
08/16/2016 03:02:05: Finished Epoch[ 2 of 2]: [Training] ce = 1.16628494 * 81920; err = 0.35725098 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.750257s
08/16/2016 03:02:05: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech'
08/16/2016 03:02:05: CNTKCommandTrainEnd: dptPre1

08/16/2016 03:02:05: Action "train" complete.


08/16/2016 03:02:05: ##############################################################################
08/16/2016 03:02:05: #                                                                            #
08/16/2016 03:02:05: # Action "edit"                                                              #
08/16/2016 03:02:05: #                                                                            #
08/16/2016 03:02:05: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=5, range=0.050000*1.000000, onCPU=false).
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=6, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 03:02:05: Action "edit" complete.


08/16/2016 03:02:05: ##############################################################################
08/16/2016 03:02:05: #                                                                            #
08/16/2016 03:02:05: # Action "train"                                                             #
08/16/2016 03:02:05: #                                                                            #
08/16/2016 03:02:05: ##############################################################################

08/16/2016 03:02:05: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 03:02:05: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 03:02:05: Loaded model with 24 nodes on GPU 0.

08/16/2016 03:02:05: Training criterion node(s):
08/16/2016 03:02:05: 	ce = CrossEntropyWithSoftmax

08/16/2016 03:02:05: Evaluation criterion node(s):
08/16/2016 03:02:05: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 39 matrices, 19 are shared as 8, and 20 are not shared.

	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *3] (gradient)
	  OL.z : [132 x 1 x *3] (gradient) }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *3] }
	{ HL2.t : [512 x 1 x *3] (gradient)
	  HL2.y : [512 x 1 x *3] }
	{ HL1.t : [512 x *3] (gradient)
	  HL1.y : [512 x 1 x *3] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *3] }
	{ HL1.z : [512 x 1 x *3] (gradient)
	  HL2.t : [512 x 1 x *3] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *3] (gradient)
	  HL2.z : [512 x 1 x *3] (gradient)
	  OL.t : [132 x 1 x *3] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *3] }


08/16/2016 03:02:05: Training 516740 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

08/16/2016 03:02:05: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 03:02:05: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:02:05: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:02:05: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:02:05: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 03:02:05: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 03:02:05: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 03:02:05: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 03:02:06: Starting minibatch loop.
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.79625931 * 2560; err = 0.81367188 * 2560; time = 0.0345s; samplesPerSecond = 74187.8
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.49683189 * 2560; err = 0.60937500 * 2560; time = 0.0280s; samplesPerSecond = 91493.9
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.11241722 * 2560; err = 0.56250000 * 2560; time = 0.0273s; samplesPerSecond = 93790.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.76884384 * 2560; err = 0.48750000 * 2560; time = 0.0269s; samplesPerSecond = 95316.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.60076065 * 2560; err = 0.46328125 * 2560; time = 0.0273s; samplesPerSecond = 93704.2
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.55093689 * 2560; err = 0.43671875 * 2560; time = 0.0269s; samplesPerSecond = 95153.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.54472351 * 2560; err = 0.45742187 * 2560; time = 0.0265s; samplesPerSecond = 96563.7
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.46992188 * 2560; err = 0.42929688 * 2560; time = 0.0255s; samplesPerSecond = 100518.3
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.43440094 * 2560; err = 0.41953125 * 2560; time = 0.0263s; samplesPerSecond = 97371.7
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.37695770 * 2560; err = 0.40585938 * 2560; time = 0.0282s; samplesPerSecond = 90670.8
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.40858002 * 2560; err = 0.40390625 * 2560; time = 0.0289s; samplesPerSecond = 88630.4
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.36172943 * 2560; err = 0.39414063 * 2560; time = 0.0290s; samplesPerSecond = 88318.5
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.32661438 * 2560; err = 0.38515625 * 2560; time = 0.0295s; samplesPerSecond = 86862.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.32606049 * 2560; err = 0.40312500 * 2560; time = 0.0288s; samplesPerSecond = 88944.5
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.27767487 * 2560; err = 0.37851563 * 2560; time = 0.0281s; samplesPerSecond = 91210.3
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27971802 * 2560; err = 0.39023438 * 2560; time = 0.0285s; samplesPerSecond = 89878.2
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.28320313 * 2560; err = 0.38203125 * 2560; time = 0.0283s; samplesPerSecond = 90475.3
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.24906006 * 2560; err = 0.38203125 * 2560; time = 0.0276s; samplesPerSecond = 92841.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.26756287 * 2560; err = 0.38281250 * 2560; time = 0.0275s; samplesPerSecond = 92962.5
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.34237671 * 2560; err = 0.40781250 * 2560; time = 0.0284s; samplesPerSecond = 90252.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.33161621 * 2560; err = 0.39687500 * 2560; time = 0.0281s; samplesPerSecond = 91145.4
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.32575378 * 2560; err = 0.40117188 * 2560; time = 0.0282s; samplesPerSecond = 90625.9
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.24573364 * 2560; err = 0.37187500 * 2560; time = 0.0282s; samplesPerSecond = 90780.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27067566 * 2560; err = 0.38320312 * 2560; time = 0.0280s; samplesPerSecond = 91268.9
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.23005066 * 2560; err = 0.37304688 * 2560; time = 0.0282s; samplesPerSecond = 90860.7
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.19244995 * 2560; err = 0.35546875 * 2560; time = 0.0283s; samplesPerSecond = 90504.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.19972534 * 2560; err = 0.37343750 * 2560; time = 0.0282s; samplesPerSecond = 90844.6
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.25191956 * 2560; err = 0.37304688 * 2560; time = 0.0282s; samplesPerSecond = 90899.4
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.18147278 * 2560; err = 0.34257813 * 2560; time = 0.0282s; samplesPerSecond = 90822.0
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.18853455 * 2560; err = 0.35742188 * 2560; time = 0.0282s; samplesPerSecond = 90625.9
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.19779663 * 2560; err = 0.35859375 * 2560; time = 0.0282s; samplesPerSecond = 90867.1
08/16/2016 03:02:06:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.16066589 * 2560; err = 0.34218750 * 2560; time = 0.0280s; samplesPerSecond = 91435.1
08/16/2016 03:02:06: Finished Epoch[ 1 of 2]: [Training] ce = 1.47034464 * 81920; err = 0.41949463 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.05183s
08/16/2016 03:02:06: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/16/2016 03:02:06: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 03:02:06: Starting minibatch loop.
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.17178698 * 2560; err = 0.36601563 * 2560; time = 0.0297s; samplesPerSecond = 86131.5
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.18522463 * 2560; err = 0.36132813 * 2560; time = 0.0281s; samplesPerSecond = 91138.9
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.15346012 * 2560; err = 0.35546875 * 2560; time = 0.0284s; samplesPerSecond = 90271.2
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.16406136 * 2560; err = 0.35468750 * 2560; time = 0.0282s; samplesPerSecond = 90867.1
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.19939270 * 2560; err = 0.35468750 * 2560; time = 0.0279s; samplesPerSecond = 91743.1
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14232559 * 2560; err = 0.35390625 * 2560; time = 0.0280s; samplesPerSecond = 91474.3
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14752731 * 2560; err = 0.35703125 * 2560; time = 0.0281s; samplesPerSecond = 91002.8
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.17431107 * 2560; err = 0.35859375 * 2560; time = 0.0278s; samplesPerSecond = 92202.4
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.18282318 * 2560; err = 0.36757812 * 2560; time = 0.0279s; samplesPerSecond = 91822.1
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.19736481 * 2560; err = 0.36914063 * 2560; time = 0.0280s; samplesPerSecond = 91533.2
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.14922562 * 2560; err = 0.35000000 * 2560; time = 0.0280s; samplesPerSecond = 91382.9
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.13426971 * 2560; err = 0.34062500 * 2560; time = 0.0282s; samplesPerSecond = 90867.1
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.10167999 * 2560; err = 0.34453125 * 2560; time = 0.0280s; samplesPerSecond = 91549.5
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.12279205 * 2560; err = 0.35000000 * 2560; time = 0.0279s; samplesPerSecond = 91726.7
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10566101 * 2560; err = 0.33320312 * 2560; time = 0.0280s; samplesPerSecond = 91425.3
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.07870789 * 2560; err = 0.33867188 * 2560; time = 0.0282s; samplesPerSecond = 90709.4
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.12519989 * 2560; err = 0.33710937 * 2560; time = 0.0279s; samplesPerSecond = 91864.9
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.07521820 * 2560; err = 0.32500000 * 2560; time = 0.0283s; samplesPerSecond = 90555.4
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.11555634 * 2560; err = 0.35000000 * 2560; time = 0.0281s; samplesPerSecond = 91184.3
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.07946167 * 2560; err = 0.32851562 * 2560; time = 0.0279s; samplesPerSecond = 91602.0
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.12444916 * 2560; err = 0.33203125 * 2560; time = 0.0280s; samplesPerSecond = 91458.0
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.10074615 * 2560; err = 0.34101562 * 2560; time = 0.0282s; samplesPerSecond = 90632.3
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.05298920 * 2560; err = 0.33085938 * 2560; time = 0.0294s; samplesPerSecond = 86968.3
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.10481262 * 2560; err = 0.34296875 * 2560; time = 0.0296s; samplesPerSecond = 86492.3
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09785767 * 2560; err = 0.33945313 * 2560; time = 0.0295s; samplesPerSecond = 86665.1
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07632446 * 2560; err = 0.32929687 * 2560; time = 0.0285s; samplesPerSecond = 89837.2
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.11539917 * 2560; err = 0.34648438 * 2560; time = 0.0282s; samplesPerSecond = 90776.9
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09566040 * 2560; err = 0.33593750 * 2560; time = 0.0285s; samplesPerSecond = 89780.5
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.10765076 * 2560; err = 0.34492187 * 2560; time = 0.0294s; samplesPerSecond = 87057.1
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.09392090 * 2560; err = 0.32890625 * 2560; time = 0.0281s; samplesPerSecond = 91087.0
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08085632 * 2560; err = 0.33593750 * 2560; time = 0.0286s; samplesPerSecond = 89357.4
08/16/2016 03:02:07:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06326294 * 2560; err = 0.32851562 * 2560; time = 0.0277s; samplesPerSecond = 92315.5
08/16/2016 03:02:07: Finished Epoch[ 2 of 2]: [Training] ce = 1.12249937 * 81920; err = 0.34476318 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.909404s
08/16/2016 03:02:08: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech'
08/16/2016 03:02:08: CNTKCommandTrainEnd: dptPre2

08/16/2016 03:02:08: Action "train" complete.


08/16/2016 03:02:08: ##############################################################################
08/16/2016 03:02:08: #                                                                            #
08/16/2016 03:02:08: # Action "edit"                                                              #
08/16/2016 03:02:08: #                                                                            #
08/16/2016 03:02:08: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=7, range=0.050000*1.000000, onCPU=false).
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=8, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 03:02:08: Action "edit" complete.


08/16/2016 03:02:08: ##############################################################################
08/16/2016 03:02:08: #                                                                            #
08/16/2016 03:02:08: # Action "train"                                                             #
08/16/2016 03:02:08: #                                                                            #
08/16/2016 03:02:08: ##############################################################################

08/16/2016 03:02:08: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 03:02:08: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 03:02:08: Loaded model with 29 nodes on GPU 0.

08/16/2016 03:02:08: Training criterion node(s):
08/16/2016 03:02:08: 	ce = CrossEntropyWithSoftmax

08/16/2016 03:02:08: Evaluation criterion node(s):
08/16/2016 03:02:08: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 49 matrices, 27 are shared as 11, and 22 are not shared.

	{ HL3.b : [512 x 1] (gradient)
	  HL3.y : [512 x 1 x *6] (gradient)
	  OL.z : [132 x 1 x *6] (gradient) }
	{ HL3.W : [512 x 512] (gradient)
	  HL3.z : [512 x 1 x *6] }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *6] }
	{ HL1.t : [512 x *6] (gradient)
	  HL1.y : [512 x 1 x *6] }
	{ HL1.z : [512 x 1 x *6] (gradient)
	  HL2.t : [512 x 1 x *6] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *6] (gradient)
	  HL2.z : [512 x 1 x *6] (gradient)
	  HL3.t : [512 x 1 x *6] }
	{ HL2.t : [512 x 1 x *6] (gradient)
	  HL2.y : [512 x 1 x *6] }
	{ HL3.t : [512 x 1 x *6] (gradient)
	  HL3.y : [512 x 1 x *6] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *6] (gradient)
	  HL3.z : [512 x 1 x *6] (gradient)
	  OL.t : [132 x 1 x *6] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *6] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *6] }


08/16/2016 03:02:08: Training 779396 parameters in 8 out of 8 parameter tensors and 20 nodes with gradient:

08/16/2016 03:02:08: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 03:02:08: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:02:08: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:02:08: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:02:08: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 03:02:08: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 03:02:08: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 03:02:08: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 03:02:08: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 03:02:08: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 03:02:08: Starting minibatch loop.
08/16/2016 03:02:08:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.06556931 * 2560; err = 0.84296875 * 2560; time = 0.0438s; samplesPerSecond = 58502.3
08/16/2016 03:02:08:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.55621643 * 2560; err = 0.61093750 * 2560; time = 0.0338s; samplesPerSecond = 75847.4
08/16/2016 03:02:08:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.04087830 * 2560; err = 0.55546875 * 2560; time = 0.0339s; samplesPerSecond = 75496.2
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.68804855 * 2560; err = 0.46992187 * 2560; time = 0.0339s; samplesPerSecond = 75500.6
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.51386871 * 2560; err = 0.43632813 * 2560; time = 0.0339s; samplesPerSecond = 75509.5
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.46156158 * 2560; err = 0.42148438 * 2560; time = 0.0340s; samplesPerSecond = 75400.6
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.45794983 * 2560; err = 0.42539063 * 2560; time = 0.0339s; samplesPerSecond = 75534.0
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.37771149 * 2560; err = 0.40000000 * 2560; time = 0.0340s; samplesPerSecond = 75311.8
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.33111267 * 2560; err = 0.39218750 * 2560; time = 0.0342s; samplesPerSecond = 74755.4
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.28664703 * 2560; err = 0.38007812 * 2560; time = 0.0341s; samplesPerSecond = 74970.0
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.31763306 * 2560; err = 0.38398437 * 2560; time = 0.0343s; samplesPerSecond = 74735.8
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.28329010 * 2560; err = 0.37578125 * 2560; time = 0.0347s; samplesPerSecond = 73868.9
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.24721680 * 2560; err = 0.37148437 * 2560; time = 0.0337s; samplesPerSecond = 75858.6
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.25346680 * 2560; err = 0.38593750 * 2560; time = 0.0341s; samplesPerSecond = 75068.9
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.19950714 * 2560; err = 0.36054687 * 2560; time = 0.0339s; samplesPerSecond = 75427.2
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.22199860 * 2560; err = 0.36875000 * 2560; time = 0.0340s; samplesPerSecond = 75338.4
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.21289673 * 2560; err = 0.36093750 * 2560; time = 0.0339s; samplesPerSecond = 75607.7
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.19815369 * 2560; err = 0.37343750 * 2560; time = 0.0339s; samplesPerSecond = 75625.5
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.22143250 * 2560; err = 0.37734375 * 2560; time = 0.0338s; samplesPerSecond = 75636.7
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.25475159 * 2560; err = 0.38046875 * 2560; time = 0.0339s; samplesPerSecond = 75531.8
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.24834290 * 2560; err = 0.37226562 * 2560; time = 0.0339s; samplesPerSecond = 75576.4
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.25514221 * 2560; err = 0.38515625 * 2560; time = 0.0339s; samplesPerSecond = 75456.1
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.18282166 * 2560; err = 0.35117188 * 2560; time = 0.0340s; samplesPerSecond = 75347.3
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.20910645 * 2560; err = 0.36875000 * 2560; time = 0.0339s; samplesPerSecond = 75416.1
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.17322388 * 2560; err = 0.35390625 * 2560; time = 0.0339s; samplesPerSecond = 75440.6
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.14150085 * 2560; err = 0.34179688 * 2560; time = 0.0339s; samplesPerSecond = 75536.3
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.13449402 * 2560; err = 0.35000000 * 2560; time = 0.0340s; samplesPerSecond = 75291.9
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.19717407 * 2560; err = 0.35898438 * 2560; time = 0.0339s; samplesPerSecond = 75425.0
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12801819 * 2560; err = 0.33320312 * 2560; time = 0.0339s; samplesPerSecond = 75514.0
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.14295654 * 2560; err = 0.35234375 * 2560; time = 0.0340s; samplesPerSecond = 75296.3
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.15014038 * 2560; err = 0.34648438 * 2560; time = 0.0340s; samplesPerSecond = 75362.8
08/16/2016 03:02:09:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.10776672 * 2560; err = 0.33320312 * 2560; time = 0.0339s; samplesPerSecond = 75556.3
08/16/2016 03:02:09: Finished Epoch[ 1 of 4]: [Training] ce = 1.41439371 * 81920; err = 0.40377197 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.24434s
08/16/2016 03:02:10: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.1'

08/16/2016 03:02:10: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 03:02:10: Starting minibatch loop.
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.19489040 * 5120; err = 0.36777344 * 5120; time = 0.0547s; samplesPerSecond = 93551.9
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.37802391 * 5120; err = 0.39589844 * 5120; time = 0.0459s; samplesPerSecond = 111614.9
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.29806042 * 5120; err = 0.38203125 * 5120; time = 0.0462s; samplesPerSecond = 110721.9
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.17699699 * 5120; err = 0.35507813 * 5120; time = 0.0461s; samplesPerSecond = 111123.2
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.15650597 * 5120; err = 0.34921875 * 5120; time = 0.0460s; samplesPerSecond = 111420.6
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.12829361 * 5120; err = 0.34414062 * 5120; time = 0.0460s; samplesPerSecond = 111335.8
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.11826630 * 5120; err = 0.34980469 * 5120; time = 0.0459s; samplesPerSecond = 111534.7
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08189774 * 5120; err = 0.33867188 * 5120; time = 0.0458s; samplesPerSecond = 111905.2
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.08464584 * 5120; err = 0.32734375 * 5120; time = 0.0459s; samplesPerSecond = 111486.1
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.09985352 * 5120; err = 0.34472656 * 5120; time = 0.0463s; samplesPerSecond = 110490.1
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.09615936 * 5120; err = 0.33046875 * 5120; time = 0.0461s; samplesPerSecond = 110990.7
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06952820 * 5120; err = 0.33007813 * 5120; time = 0.0464s; samplesPerSecond = 110361.5
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08608704 * 5120; err = 0.33691406 * 5120; time = 0.0464s; samplesPerSecond = 110344.8
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.15416412 * 5120; err = 0.35214844 * 5120; time = 0.0473s; samplesPerSecond = 108263.6
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.08932800 * 5120; err = 0.33007813 * 5120; time = 0.0467s; samplesPerSecond = 109643.0
08/16/2016 03:02:10:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06142273 * 5120; err = 0.32578125 * 5120; time = 0.0465s; samplesPerSecond = 110183.4
08/16/2016 03:02:10: Finished Epoch[ 2 of 4]: [Training] ce = 1.14213276 * 81920; err = 0.34750977 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.751127s
08/16/2016 03:02:10: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.2'

08/16/2016 03:02:10: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/16/2016 03:02:10: Starting minibatch loop.
08/16/2016 03:02:10:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.08546238 * 5120; err = 0.33808594 * 5120; time = 0.0474s; samplesPerSecond = 108124.1
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.09619579 * 5120; err = 0.33183594 * 5120; time = 0.0466s; samplesPerSecond = 109772.3
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.08299332 * 5120; err = 0.33398438 * 5120; time = 0.0463s; samplesPerSecond = 110635.7
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.04968491 * 5120; err = 0.32714844 * 5120; time = 0.0464s; samplesPerSecond = 110325.8
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07498894 * 5120; err = 0.32382813 * 5120; time = 0.0465s; samplesPerSecond = 110223.7
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.09102592 * 5120; err = 0.33574219 * 5120; time = 0.0464s; samplesPerSecond = 110373.4
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06637192 * 5120; err = 0.32968750 * 5120; time = 0.0461s; samplesPerSecond = 111115.9
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08646622 * 5120; err = 0.33984375 * 5120; time = 0.0466s; samplesPerSecond = 109972.7
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.10471420 * 5120; err = 0.35234375 * 5120; time = 0.0467s; samplesPerSecond = 109708.8
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.02982941 * 5120; err = 0.31757812 * 5120; time = 0.0465s; samplesPerSecond = 110128.8
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.03783646 * 5120; err = 0.32304688 * 5120; time = 0.0465s; samplesPerSecond = 110166.8
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.05894775 * 5120; err = 0.33164063 * 5120; time = 0.0462s; samplesPerSecond = 110743.4
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.07611847 * 5120; err = 0.34296875 * 5120; time = 0.0466s; samplesPerSecond = 109934.9
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.11403198 * 5120; err = 0.35859375 * 5120; time = 0.0462s; samplesPerSecond = 110911.3
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.02603912 * 5120; err = 0.31640625 * 5120; time = 0.0464s; samplesPerSecond = 110273.5
08/16/2016 03:02:11:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.03398590 * 5120; err = 0.32558594 * 5120; time = 0.0464s; samplesPerSecond = 110268.8
08/16/2016 03:02:11: Finished Epoch[ 3 of 4]: [Training] ce = 1.06966829 * 81920; err = 0.33302002 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.746573s
08/16/2016 03:02:11: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.3'

08/16/2016 03:02:12: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/16/2016 03:02:12: Starting minibatch loop.
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.02380686 * 5120; err = 0.32851562 * 5120; time = 0.0473s; samplesPerSecond = 108352.9
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.00228529 * 4926; err = 0.31526594 * 4926; time = 0.0999s; samplesPerSecond = 49306.3
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01724129 * 5120; err = 0.31816406 * 5120; time = 0.0464s; samplesPerSecond = 110390.0
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.03762894 * 5120; err = 0.32285156 * 5120; time = 0.0465s; samplesPerSecond = 110145.4
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.03465233 * 5120; err = 0.31855469 * 5120; time = 0.0464s; samplesPerSecond = 110318.7
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00077057 * 5120; err = 0.31484375 * 5120; time = 0.0464s; samplesPerSecond = 110368.6
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.01924973 * 5120; err = 0.31464844 * 5120; time = 0.0465s; samplesPerSecond = 110117.0
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.01371307 * 5120; err = 0.31191406 * 5120; time = 0.0464s; samplesPerSecond = 110325.8
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.98328781 * 5120; err = 0.32089844 * 5120; time = 0.0463s; samplesPerSecond = 110554.5
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.98929749 * 5120; err = 0.30664063 * 5120; time = 0.0464s; samplesPerSecond = 110411.5
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.03435593 * 5120; err = 0.32070312 * 5120; time = 0.0463s; samplesPerSecond = 110564.0
08/16/2016 03:02:12:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.98207321 * 5120; err = 0.30214844 * 5120; time = 0.0464s; samplesPerSecond = 110330.6
08/16/2016 03:02:13:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.00333176 * 5120; err = 0.30937500 * 5120; time = 0.0463s; samplesPerSecond = 110645.3
08/16/2016 03:02:13:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.96886292 * 5120; err = 0.30527344 * 5120; time = 0.0462s; samplesPerSecond = 110829.7
08/16/2016 03:02:13:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.97402802 * 5120; err = 0.29765625 * 5120; time = 0.0463s; samplesPerSecond = 110523.5
08/16/2016 03:02:13:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.97356567 * 5120; err = 0.30253906 * 5120; time = 0.0461s; samplesPerSecond = 110983.5
08/16/2016 03:02:13: Finished Epoch[ 4 of 4]: [Training] ce = 1.00385303 * 81920; err = 0.31324463 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.802624s
08/16/2016 03:02:13: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160816030157.855216\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech'
08/16/2016 03:02:13: CNTKCommandTrainEnd: speechTrain

08/16/2016 03:02:13: Action "train" complete.

08/16/2016 03:02:13: __COMPLETED__
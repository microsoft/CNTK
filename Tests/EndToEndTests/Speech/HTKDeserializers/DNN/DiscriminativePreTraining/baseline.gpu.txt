CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug 16 2016 09:41:56
		Last modified date: Fri Aug 12 07:32:43 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
		Built by philly on f67b30a647de
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
08/16/2016 09:57:15: -------------------------------------------------------------------
08/16/2016 09:57:15: Build info: 

08/16/2016 09:57:15: 		Built time: Aug 16 2016 09:41:56
08/16/2016 09:57:15: 		Last modified date: Fri Aug 12 07:32:43 2016
08/16/2016 09:57:15: 		Build type: release
08/16/2016 09:57:15: 		Build target: GPU
08/16/2016 09:57:15: 		With 1bit-SGD: no
08/16/2016 09:57:15: 		Math lib: mkl
08/16/2016 09:57:15: 		CUDA_PATH: /usr/local/cuda-7.5
08/16/2016 09:57:15: 		CUB_PATH: /usr/local/cub-1.4.1
08/16/2016 09:57:15: 		CUDNN_PATH: /usr/local/cudnn-4.0
08/16/2016 09:57:15: 		Build Branch: HEAD
08/16/2016 09:57:15: 		Build SHA1: 026b1e772b963461e189f8f00aa7ed6951298f84
08/16/2016 09:57:15: 		Built by philly on f67b30a647de
08/16/2016 09:57:15: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
08/16/2016 09:57:15: -------------------------------------------------------------------
08/16/2016 09:57:16: -------------------------------------------------------------------
08/16/2016 09:57:16: GPU info:

08/16/2016 09:57:16: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:57:16: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:57:16: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:57:16: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/16/2016 09:57:16: -------------------------------------------------------------------

08/16/2016 09:57:16: Running on localhost at 2016/08/16 09:57:16
08/16/2016 09:57:16: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining  OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu  DeviceId=0  timestamping=true



08/16/2016 09:57:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/16/2016 09:57:16: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    useMersenneTwisterRand=true
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 09:57:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/16/2016 09:57:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/16/2016 09:57:16: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    useMersenneTwisterRand=true
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/16/2016 09:57:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/16/2016 09:57:16: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    useMersenneTwisterRand=true
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
08/16/2016 09:57:16: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/16/2016 09:57:16: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
08/16/2016 09:57:16: Precision = "float"
08/16/2016 09:57:16: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
08/16/2016 09:57:16: CNTKCommandTrainInfo: dptPre1 : 2
08/16/2016 09:57:16: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
08/16/2016 09:57:16: CNTKCommandTrainInfo: dptPre2 : 2
08/16/2016 09:57:16: CNTKModelPath: /tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
08/16/2016 09:57:16: CNTKCommandTrainInfo: speechTrain : 4
08/16/2016 09:57:16: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

08/16/2016 09:57:16: ##############################################################################
08/16/2016 09:57:16: #                                                                            #
08/16/2016 09:57:16: # Action "train"                                                             #
08/16/2016 09:57:16: #                                                                            #
08/16/2016 09:57:16: ##############################################################################

08/16/2016 09:57:16: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 09:57:16: Creating virgin network.
Node 'globalMean' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalInvStd' (LearnableParameter operation): Initializing Parameter[363 x 1] <- 0.000000.
Node 'globalPrior' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- 0.000000.
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- 0.000000.
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- 0.000000.
Node 'HL1.W' (LearnableParameter operation): Initializing Parameter[512 x 363] <- uniform(seed=1, range=0.050000*1.000000, onCPU=false).
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
Node 'HL1.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=2, range=0.050000*1.000000, onCPU=false).
Node 'OL.W' (LearnableParameter operation): Initializing Parameter[132 x 512] <- uniform(seed=3, range=0.050000*1.000000, onCPU=false).
Node 'OL.b' (LearnableParameter operation): Initializing Parameter[132 x 1] <- uniform(seed=4, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 09:57:16: Created model with 19 nodes on GPU 0.

08/16/2016 09:57:16: Training criterion node(s):
08/16/2016 09:57:16: 	ce = CrossEntropyWithSoftmax

08/16/2016 09:57:16: Evaluation criterion node(s):
08/16/2016 09:57:16: 	err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 29 matrices, 11 are shared as 5, and 18 are not shared.

	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *] }
	{ HL1.t : [512 x *] (gradient)
	  HL1.y : [512 x 1 x *] }
	{ HL1.z : [512 x 1 x *] (gradient)
	  OL.t : [132 x 1 x *] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] (gradient) }


08/16/2016 09:57:16: Training 254084 parameters in 4 out of 4 parameter tensors and 10 nodes with gradient:

08/16/2016 09:57:16: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 09:57:16: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:57:16: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 09:57:16: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 09:57:16: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 09:57:16: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 09:57:16: Starting minibatch loop.
08/16/2016 09:57:16:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.77545433 * 2560; err = 0.83984375 * 2560; time = 0.1196s; samplesPerSecond = 21402.2
08/16/2016 09:57:16:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.92129173 * 2560; err = 0.69921875 * 2560; time = 0.0086s; samplesPerSecond = 297225.1
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.54243622 * 2560; err = 0.64882812 * 2560; time = 0.0086s; samplesPerSecond = 298786.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.20117416 * 2560; err = 0.60156250 * 2560; time = 0.0086s; samplesPerSecond = 297847.6
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.98474121 * 2560; err = 0.55273438 * 2560; time = 0.0086s; samplesPerSecond = 298925.7
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87129364 * 2560; err = 0.51562500 * 2560; time = 0.0086s; samplesPerSecond = 299240.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.83400879 * 2560; err = 0.52812500 * 2560; time = 0.0086s; samplesPerSecond = 298855.9
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.71646271 * 2560; err = 0.49335937 * 2560; time = 0.0086s; samplesPerSecond = 299135.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.66541901 * 2560; err = 0.46328125 * 2560; time = 0.0086s; samplesPerSecond = 297639.8
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.57725983 * 2560; err = 0.46054688 * 2560; time = 0.0085s; samplesPerSecond = 299660.5
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.61621094 * 2560; err = 0.45390625 * 2560; time = 0.0085s; samplesPerSecond = 300610.6
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.56063995 * 2560; err = 0.44140625 * 2560; time = 0.0085s; samplesPerSecond = 300011.7
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.52853241 * 2560; err = 0.44492188 * 2560; time = 0.0085s; samplesPerSecond = 301637.8
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.53460999 * 2560; err = 0.46210937 * 2560; time = 0.0086s; samplesPerSecond = 299345.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46377869 * 2560; err = 0.44140625 * 2560; time = 0.0085s; samplesPerSecond = 300681.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.43344116 * 2560; err = 0.42617187 * 2560; time = 0.0085s; samplesPerSecond = 300787.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.43216858 * 2560; err = 0.42148438 * 2560; time = 0.0085s; samplesPerSecond = 300328.5
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.37991028 * 2560; err = 0.41250000 * 2560; time = 0.0085s; samplesPerSecond = 300082.1
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.35844727 * 2560; err = 0.40039062 * 2560; time = 0.0085s; samplesPerSecond = 300645.9
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.44847107 * 2560; err = 0.42656250 * 2560; time = 0.0086s; samplesPerSecond = 298472.7
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.43933411 * 2560; err = 0.42539063 * 2560; time = 0.0085s; samplesPerSecond = 301070.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.41740417 * 2560; err = 0.42539063 * 2560; time = 0.0085s; samplesPerSecond = 300187.6
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.33209229 * 2560; err = 0.40468750 * 2560; time = 0.0085s; samplesPerSecond = 299555.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.36106567 * 2560; err = 0.40429688 * 2560; time = 0.0085s; samplesPerSecond = 299485.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.30946045 * 2560; err = 0.39804688 * 2560; time = 0.0086s; samplesPerSecond = 298298.8
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.25348206 * 2560; err = 0.37031250 * 2560; time = 0.0086s; samplesPerSecond = 299240.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.30354004 * 2560; err = 0.39687500 * 2560; time = 0.0085s; samplesPerSecond = 300999.4
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.36096191 * 2560; err = 0.40937500 * 2560; time = 0.0085s; samplesPerSecond = 299941.4
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.29523315 * 2560; err = 0.39375000 * 2560; time = 0.0092s; samplesPerSecond = 277086.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.35650330 * 2560; err = 0.41054687 * 2560; time = 0.0080s; samplesPerSecond = 319560.6
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.32445068 * 2560; err = 0.39843750 * 2560; time = 0.0079s; samplesPerSecond = 322987.6
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.26812134 * 2560; err = 0.38203125 * 2560; time = 0.0080s; samplesPerSecond = 320200.1
08/16/2016 09:57:17: Finished Epoch[ 1 of 2]: [Training] ce = 1.65210629 * 81920; err = 0.46728516 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.554957s
08/16/2016 09:57:17: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/16/2016 09:57:17: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 09:57:17: Starting minibatch loop.
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.24494066 * 2560; err = 0.39062500 * 2560; time = 0.0094s; samplesPerSecond = 273533.5
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.23623838 * 2560; err = 0.36718750 * 2560; time = 0.0081s; samplesPerSecond = 317421.0
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.26030045 * 2560; err = 0.39296875 * 2560; time = 0.0081s; samplesPerSecond = 316753.3
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.26366539 * 2560; err = 0.38320312 * 2560; time = 0.0081s; samplesPerSecond = 317617.9
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.27961769 * 2560; err = 0.36914062 * 2560; time = 0.0080s; samplesPerSecond = 321769.7
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.17966309 * 2560; err = 0.35312500 * 2560; time = 0.0079s; samplesPerSecond = 322012.6
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.19451904 * 2560; err = 0.36640625 * 2560; time = 0.0079s; samplesPerSecond = 324214.8
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.22797241 * 2560; err = 0.37109375 * 2560; time = 0.0079s; samplesPerSecond = 322865.4
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.26700439 * 2560; err = 0.38750000 * 2560; time = 0.0079s; samplesPerSecond = 322987.6
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.24487915 * 2560; err = 0.37109375 * 2560; time = 0.0079s; samplesPerSecond = 323722.8
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.20520325 * 2560; err = 0.36796875 * 2560; time = 0.0079s; samplesPerSecond = 322662.0
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18870239 * 2560; err = 0.36289063 * 2560; time = 0.0079s; samplesPerSecond = 325327.2
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.16525116 * 2560; err = 0.36367187 * 2560; time = 0.0080s; samplesPerSecond = 320802.0
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16569672 * 2560; err = 0.36523438 * 2560; time = 0.0079s; samplesPerSecond = 323354.8
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.14518127 * 2560; err = 0.34218750 * 2560; time = 0.0088s; samplesPerSecond = 292170.7
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.11732483 * 2560; err = 0.35195312 * 2560; time = 0.0079s; samplesPerSecond = 322865.4
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.17393036 * 2560; err = 0.35390625 * 2560; time = 0.0079s; samplesPerSecond = 324790.7
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.13165283 * 2560; err = 0.35703125 * 2560; time = 0.0079s; samplesPerSecond = 325865.6
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.14869232 * 2560; err = 0.35312500 * 2560; time = 0.0078s; samplesPerSecond = 326697.3
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11426697 * 2560; err = 0.33710937 * 2560; time = 0.0079s; samplesPerSecond = 325244.6
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.17790833 * 2560; err = 0.35585937 * 2560; time = 0.0078s; samplesPerSecond = 326364.1
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.15280762 * 2560; err = 0.35625000 * 2560; time = 0.0078s; samplesPerSecond = 326655.6
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.09865112 * 2560; err = 0.34687500 * 2560; time = 0.0078s; samplesPerSecond = 328036.9
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.13216553 * 2560; err = 0.35312500 * 2560; time = 0.0079s; samplesPerSecond = 324379.1
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.09484863 * 2560; err = 0.34101562 * 2560; time = 0.0078s; samplesPerSecond = 326364.1
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.09128418 * 2560; err = 0.32539062 * 2560; time = 0.0079s; samplesPerSecond = 326073.1
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.12996826 * 2560; err = 0.35195312 * 2560; time = 0.0078s; samplesPerSecond = 328879.8
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.13035278 * 2560; err = 0.33945313 * 2560; time = 0.0079s; samplesPerSecond = 322987.6
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.11663208 * 2560; err = 0.34687500 * 2560; time = 0.0078s; samplesPerSecond = 329048.8
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.08121948 * 2560; err = 0.33750000 * 2560; time = 0.0078s; samplesPerSecond = 327742.9
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08856812 * 2560; err = 0.34531250 * 2560; time = 0.0078s; samplesPerSecond = 328922.0
08/16/2016 09:57:17:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07200928 * 2560; err = 0.32500000 * 2560; time = 0.0078s; samplesPerSecond = 328795.3
08/16/2016 09:57:17: Finished Epoch[ 2 of 2]: [Training] ce = 1.16628494 * 81920; err = 0.35725098 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.258187s
08/16/2016 09:57:17: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech'
08/16/2016 09:57:17: CNTKCommandTrainEnd: dptPre1

08/16/2016 09:57:17: Action "train" complete.


08/16/2016 09:57:17: ##############################################################################
08/16/2016 09:57:17: #                                                                            #
08/16/2016 09:57:17: # Action "edit"                                                              #
08/16/2016 09:57:17: #                                                                            #
08/16/2016 09:57:17: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL2.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=5, range=0.050000*1.000000, onCPU=false).
Node 'HL2.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=6, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 09:57:17: Action "edit" complete.


08/16/2016 09:57:17: ##############################################################################
08/16/2016 09:57:17: #                                                                            #
08/16/2016 09:57:17: # Action "train"                                                             #
08/16/2016 09:57:17: #                                                                            #
08/16/2016 09:57:17: ##############################################################################

08/16/2016 09:57:17: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 09:57:17: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 09:57:17: Loaded model with 24 nodes on GPU 0.

08/16/2016 09:57:17: Training criterion node(s):
08/16/2016 09:57:17: 	ce = CrossEntropyWithSoftmax

08/16/2016 09:57:17: Evaluation criterion node(s):
08/16/2016 09:57:17: 	err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 39 matrices, 19 are shared as 8, and 20 are not shared.

	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *3] }
	{ HL2.t : [512 x 1 x *3] (gradient)
	  HL2.y : [512 x 1 x *3] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *3] (gradient)
	  HL2.z : [512 x 1 x *3] (gradient)
	  OL.t : [132 x 1 x *3] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *3] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *3] (gradient)
	  OL.z : [132 x 1 x *3] (gradient) }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *3] }
	{ HL1.t : [512 x *3] (gradient)
	  HL1.y : [512 x 1 x *3] }
	{ HL1.z : [512 x 1 x *3] (gradient)
	  HL2.t : [512 x 1 x *3] }


08/16/2016 09:57:17: Training 516740 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

08/16/2016 09:57:17: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 09:57:17: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:57:17: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:57:17: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:57:17: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 09:57:17: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 09:57:17: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 09:57:17: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 09:57:17: Starting minibatch loop.
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.89158669 * 2560; err = 0.82148438 * 2560; time = 0.0149s; samplesPerSecond = 172286.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.56412315 * 2560; err = 0.63710937 * 2560; time = 0.0123s; samplesPerSecond = 208792.1
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.14925079 * 2560; err = 0.58242187 * 2560; time = 0.0123s; samplesPerSecond = 208707.0
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.80655594 * 2560; err = 0.50156250 * 2560; time = 0.0123s; samplesPerSecond = 208962.5
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.62402344 * 2560; err = 0.46601562 * 2560; time = 0.0123s; samplesPerSecond = 208979.6
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.57313461 * 2560; err = 0.44804688 * 2560; time = 0.0123s; samplesPerSecond = 208877.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.57385864 * 2560; err = 0.46484375 * 2560; time = 0.0123s; samplesPerSecond = 208809.1
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.49221191 * 2560; err = 0.44687500 * 2560; time = 0.0123s; samplesPerSecond = 208860.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.43610687 * 2560; err = 0.41992188 * 2560; time = 0.0123s; samplesPerSecond = 208282.5
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.39413300 * 2560; err = 0.40703125 * 2560; time = 0.0123s; samplesPerSecond = 208520.0
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.41445618 * 2560; err = 0.40937500 * 2560; time = 0.0123s; samplesPerSecond = 208894.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.36034698 * 2560; err = 0.39726563 * 2560; time = 0.0123s; samplesPerSecond = 208877.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.32356415 * 2560; err = 0.39101562 * 2560; time = 0.0123s; samplesPerSecond = 208962.5
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.32742920 * 2560; err = 0.40703125 * 2560; time = 0.0122s; samplesPerSecond = 209133.2
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.27870331 * 2560; err = 0.38085938 * 2560; time = 0.0123s; samplesPerSecond = 208622.0
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.28920898 * 2560; err = 0.39335938 * 2560; time = 0.0153s; samplesPerSecond = 167517.3
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.29697266 * 2560; err = 0.37968750 * 2560; time = 0.0124s; samplesPerSecond = 206801.8
08/16/2016 09:57:17:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.27047729 * 2560; err = 0.38554688 * 2560; time = 0.0123s; samplesPerSecond = 207556.3
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.29291077 * 2560; err = 0.38984375 * 2560; time = 0.0124s; samplesPerSecond = 206735.0
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.32550964 * 2560; err = 0.40273437 * 2560; time = 0.0123s; samplesPerSecond = 207792.2
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.31292419 * 2560; err = 0.39179687 * 2560; time = 0.0123s; samplesPerSecond = 207489.1
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.32257080 * 2560; err = 0.40195313 * 2560; time = 0.0124s; samplesPerSecond = 207136.5
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.23712158 * 2560; err = 0.36914062 * 2560; time = 0.0123s; samplesPerSecond = 207657.4
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.26952209 * 2560; err = 0.38242188 * 2560; time = 0.0123s; samplesPerSecond = 207825.9
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.23038940 * 2560; err = 0.36914062 * 2560; time = 0.0123s; samplesPerSecond = 207573.2
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.19490356 * 2560; err = 0.35898438 * 2560; time = 0.0123s; samplesPerSecond = 207792.2
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.20699768 * 2560; err = 0.36953125 * 2560; time = 0.0123s; samplesPerSecond = 207927.2
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.25049133 * 2560; err = 0.37265625 * 2560; time = 0.0124s; samplesPerSecond = 207069.5
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.17547913 * 2560; err = 0.34804687 * 2560; time = 0.0123s; samplesPerSecond = 207961.0
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.18261414 * 2560; err = 0.35898438 * 2560; time = 0.0123s; samplesPerSecond = 208350.3
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.19815369 * 2560; err = 0.36093750 * 2560; time = 0.0124s; samplesPerSecond = 207019.2
08/16/2016 09:57:18:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.15399170 * 2560; err = 0.34531250 * 2560; time = 0.0123s; samplesPerSecond = 207337.8
08/16/2016 09:57:18: Finished Epoch[ 1 of 2]: [Training] ce = 1.48186636 * 81920; err = 0.42377930 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.4797s
08/16/2016 09:57:18: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/16/2016 09:57:18: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 09:57:18: Starting minibatch loop.
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.16762066 * 2560; err = 0.36328125 * 2560; time = 0.0139s; samplesPerSecond = 184597.6
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.18762207 * 2560; err = 0.35937500 * 2560; time = 0.0123s; samplesPerSecond = 207775.3
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.15600147 * 2560; err = 0.35625000 * 2560; time = 0.0123s; samplesPerSecond = 207287.4
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.15653267 * 2560; err = 0.35117188 * 2560; time = 0.0124s; samplesPerSecond = 206785.1
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.19814186 * 2560; err = 0.34843750 * 2560; time = 0.0123s; samplesPerSecond = 207321.0
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.13408813 * 2560; err = 0.35039063 * 2560; time = 0.0124s; samplesPerSecond = 206985.8
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14792786 * 2560; err = 0.35429688 * 2560; time = 0.0123s; samplesPerSecond = 207405.0
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.17106400 * 2560; err = 0.36093750 * 2560; time = 0.0124s; samplesPerSecond = 206634.9
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.18514709 * 2560; err = 0.36718750 * 2560; time = 0.0123s; samplesPerSecond = 207893.5
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18752975 * 2560; err = 0.35664062 * 2560; time = 0.0123s; samplesPerSecond = 207825.9
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.14243393 * 2560; err = 0.34843750 * 2560; time = 0.0123s; samplesPerSecond = 207859.7
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.14062653 * 2560; err = 0.34570312 * 2560; time = 0.0123s; samplesPerSecond = 207421.8
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10054779 * 2560; err = 0.34375000 * 2560; time = 0.0123s; samplesPerSecond = 207674.2
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.12264099 * 2560; err = 0.34960938 * 2560; time = 0.0124s; samplesPerSecond = 206751.7
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.10169220 * 2560; err = 0.33554688 * 2560; time = 0.0124s; samplesPerSecond = 206852.0
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.07068787 * 2560; err = 0.33906250 * 2560; time = 0.0124s; samplesPerSecond = 207119.7
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.11545105 * 2560; err = 0.33437500 * 2560; time = 0.0123s; samplesPerSecond = 207691.1
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.07761688 * 2560; err = 0.32929687 * 2560; time = 0.0124s; samplesPerSecond = 207036.0
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.11424866 * 2560; err = 0.34531250 * 2560; time = 0.0123s; samplesPerSecond = 207472.2
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08009796 * 2560; err = 0.32578125 * 2560; time = 0.0124s; samplesPerSecond = 206568.2
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.13003235 * 2560; err = 0.33398438 * 2560; time = 0.0123s; samplesPerSecond = 207405.0
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.09874115 * 2560; err = 0.33671875 * 2560; time = 0.0124s; samplesPerSecond = 206818.5
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.04590759 * 2560; err = 0.32578125 * 2560; time = 0.0123s; samplesPerSecond = 207724.8
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.10446167 * 2560; err = 0.33984375 * 2560; time = 0.0124s; samplesPerSecond = 206751.7
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.10089111 * 2560; err = 0.34335938 * 2560; time = 0.0123s; samplesPerSecond = 207606.8
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.07502136 * 2560; err = 0.32617188 * 2560; time = 0.0123s; samplesPerSecond = 208265.5
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.11576538 * 2560; err = 0.34687500 * 2560; time = 0.0123s; samplesPerSecond = 207724.8
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09406433 * 2560; err = 0.33671875 * 2560; time = 0.0123s; samplesPerSecond = 207556.3
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.10549927 * 2560; err = 0.34296875 * 2560; time = 0.0123s; samplesPerSecond = 207758.5
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.09874878 * 2560; err = 0.32734375 * 2560; time = 0.0124s; samplesPerSecond = 206885.4
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.07799988 * 2560; err = 0.33203125 * 2560; time = 0.0124s; samplesPerSecond = 207270.7
08/16/2016 09:57:18:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06160278 * 2560; err = 0.32929687 * 2560; time = 0.0123s; samplesPerSecond = 207674.2
08/16/2016 09:57:18: Finished Epoch[ 2 of 2]: [Training] ce = 1.12082672 * 81920; err = 0.34331055 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.400409s
08/16/2016 09:57:18: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech'
08/16/2016 09:57:18: CNTKCommandTrainEnd: dptPre2

08/16/2016 09:57:18: Action "train" complete.


08/16/2016 09:57:18: ##############################################################################
08/16/2016 09:57:18: #                                                                            #
08/16/2016 09:57:18: # Action "edit"                                                              #
08/16/2016 09:57:18: #                                                                            #
08/16/2016 09:57:18: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- 0.000000.
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- 0.000000.
Node 'HL3.W' (LearnableParameter operation): Initializing Parameter[512 x 512] <- uniform(seed=7, range=0.050000*1.000000, onCPU=false).
Node 'HL3.b' (LearnableParameter operation): Initializing Parameter[512 x 1] <- uniform(seed=8, range=0.050000*1.000000, onCPU=false).

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/16/2016 09:57:18: Action "edit" complete.


08/16/2016 09:57:18: ##############################################################################
08/16/2016 09:57:18: #                                                                            #
08/16/2016 09:57:18: # Action "train"                                                             #
08/16/2016 09:57:18: #                                                                            #
08/16/2016 09:57:18: ##############################################################################

08/16/2016 09:57:18: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/16/2016 09:57:18: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/16/2016 09:57:18: Loaded model with 29 nodes on GPU 0.

08/16/2016 09:57:18: Training criterion node(s):
08/16/2016 09:57:18: 	ce = CrossEntropyWithSoftmax

08/16/2016 09:57:18: Evaluation criterion node(s):
08/16/2016 09:57:18: 	err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 49 matrices, 27 are shared as 11, and 22 are not shared.

	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *6] }
	{ HL1.t : [512 x *6] (gradient)
	  HL1.y : [512 x 1 x *6] }
	{ HL3.t : [512 x 1 x *6] (gradient)
	  HL3.y : [512 x 1 x *6] }
	{ HL2.b : [512 x 1] (gradient)
	  HL2.y : [512 x 1 x *6] (gradient)
	  HL3.z : [512 x 1 x *6] (gradient)
	  OL.t : [132 x 1 x *6] }
	{ OL.W : [132 x 512] (gradient)
	  OL.z : [132 x 1 x *6] }
	{ HL3.b : [512 x 1] (gradient)
	  HL3.y : [512 x 1 x *6] (gradient)
	  OL.z : [132 x 1 x *6] (gradient) }
	{ HL1.z : [512 x 1 x *6] (gradient)
	  HL2.t : [512 x 1 x *6] }
	{ HL2.W : [512 x 512] (gradient)
	  HL2.z : [512 x 1 x *6] }
	{ HL2.t : [512 x 1 x *6] (gradient)
	  HL2.y : [512 x 1 x *6] }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *6] (gradient)
	  HL2.z : [512 x 1 x *6] (gradient)
	  HL3.t : [512 x 1 x *6] }
	{ HL3.W : [512 x 512] (gradient)
	  HL3.z : [512 x 1 x *6] }


08/16/2016 09:57:18: Training 779396 parameters in 8 out of 8 parameter tensors and 20 nodes with gradient:

08/16/2016 09:57:18: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
08/16/2016 09:57:18: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:57:18: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:57:18: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:57:18: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
08/16/2016 09:57:18: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
08/16/2016 09:57:18: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
08/16/2016 09:57:18: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

08/16/2016 09:57:18: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/16/2016 09:57:18: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/16/2016 09:57:18: Starting minibatch loop.
08/16/2016 09:57:18:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 4.06310844 * 2560; err = 0.84296875 * 2560; time = 0.0196s; samplesPerSecond = 130719.0
08/16/2016 09:57:18:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.55720177 * 2560; err = 0.61328125 * 2560; time = 0.0168s; samplesPerSecond = 152544.4
08/16/2016 09:57:18:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.04571991 * 2560; err = 0.56132812 * 2560; time = 0.0167s; samplesPerSecond = 153018.5
08/16/2016 09:57:18:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.68830490 * 2560; err = 0.47031250 * 2560; time = 0.0167s; samplesPerSecond = 153073.4
08/16/2016 09:57:18:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.51176605 * 2560; err = 0.43710938 * 2560; time = 0.0167s; samplesPerSecond = 152954.5
08/16/2016 09:57:18:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.46000977 * 2560; err = 0.42304687 * 2560; time = 0.0168s; samplesPerSecond = 152290.3
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.45907898 * 2560; err = 0.42539063 * 2560; time = 0.0168s; samplesPerSecond = 152553.5
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.37488098 * 2560; err = 0.40507813 * 2560; time = 0.0168s; samplesPerSecond = 152190.7
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.32691956 * 2560; err = 0.38359375 * 2560; time = 0.0168s; samplesPerSecond = 152172.6
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.28918762 * 2560; err = 0.37890625 * 2560; time = 0.0168s; samplesPerSecond = 152553.5
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.31724243 * 2560; err = 0.38242188 * 2560; time = 0.0167s; samplesPerSecond = 152854.1
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27811432 * 2560; err = 0.37460938 * 2560; time = 0.0168s; samplesPerSecond = 152172.6
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.24795837 * 2560; err = 0.37500000 * 2560; time = 0.0168s; samplesPerSecond = 152064.2
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.25469818 * 2560; err = 0.38203125 * 2560; time = 0.0168s; samplesPerSecond = 152799.3
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.20223083 * 2560; err = 0.36093750 * 2560; time = 0.0168s; samplesPerSecond = 152435.4
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.21909790 * 2560; err = 0.37070313 * 2560; time = 0.0168s; samplesPerSecond = 152508.0
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.21332092 * 2560; err = 0.36445312 * 2560; time = 0.0168s; samplesPerSecond = 152571.7
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.19662476 * 2560; err = 0.37265625 * 2560; time = 0.0167s; samplesPerSecond = 153046.0
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.21360779 * 2560; err = 0.36640625 * 2560; time = 0.0168s; samplesPerSecond = 152535.3
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.25376587 * 2560; err = 0.37929687 * 2560; time = 0.0167s; samplesPerSecond = 152918.0
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.24345703 * 2560; err = 0.36835937 * 2560; time = 0.0168s; samplesPerSecond = 152671.8
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.25221252 * 2560; err = 0.38867188 * 2560; time = 0.0167s; samplesPerSecond = 152927.1
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.18008728 * 2560; err = 0.35351562 * 2560; time = 0.0168s; samplesPerSecond = 152589.9
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.21273804 * 2560; err = 0.37890625 * 2560; time = 0.0168s; samplesPerSecond = 152826.7
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.16963806 * 2560; err = 0.35546875 * 2560; time = 0.0168s; samplesPerSecond = 152790.2
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.13971252 * 2560; err = 0.33867188 * 2560; time = 0.0168s; samplesPerSecond = 152562.6
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.13331909 * 2560; err = 0.34882812 * 2560; time = 0.0168s; samplesPerSecond = 152517.1
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.19458008 * 2560; err = 0.35859375 * 2560; time = 0.0168s; samplesPerSecond = 152735.5
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.12374268 * 2560; err = 0.33125000 * 2560; time = 0.0168s; samplesPerSecond = 152381.0
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.14518433 * 2560; err = 0.35390625 * 2560; time = 0.0168s; samplesPerSecond = 152426.3
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.14932251 * 2560; err = 0.34726563 * 2560; time = 0.0168s; samplesPerSecond = 152781.1
08/16/2016 09:57:19:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.10969543 * 2560; err = 0.33085938 * 2560; time = 0.0168s; samplesPerSecond = 152826.7
08/16/2016 09:57:19: Finished Epoch[ 1 of 4]: [Training] ce = 1.41332903 * 81920; err = 0.40386963 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.621108s
08/16/2016 09:57:19: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.1'

08/16/2016 09:57:19: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/16/2016 09:57:19: Starting minibatch loop.
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.23170967 * 5120; err = 0.37734375 * 5120; time = 0.0289s; samplesPerSecond = 176978.9
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.32993221 * 5120; err = 0.39023438 * 5120; time = 0.0261s; samplesPerSecond = 196492.3
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.31869526 * 5120; err = 0.39687500 * 5120; time = 0.0261s; samplesPerSecond = 196304.0
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.16838951 * 5120; err = 0.35839844 * 5120; time = 0.0261s; samplesPerSecond = 196349.1
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.16817627 * 5120; err = 0.35644531 * 5120; time = 0.0261s; samplesPerSecond = 196364.2
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.12597427 * 5120; err = 0.34238281 * 5120; time = 0.0261s; samplesPerSecond = 196055.9
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.09737778 * 5120; err = 0.34160156 * 5120; time = 0.0262s; samplesPerSecond = 195741.1
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07114639 * 5120; err = 0.33300781 * 5120; time = 0.0261s; samplesPerSecond = 195943.4
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.07779846 * 5120; err = 0.32910156 * 5120; time = 0.0261s; samplesPerSecond = 196221.2
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.09942703 * 5120; err = 0.34511719 * 5120; time = 0.0261s; samplesPerSecond = 196432.0
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.08886490 * 5120; err = 0.32460937 * 5120; time = 0.0261s; samplesPerSecond = 196033.4
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06541290 * 5120; err = 0.33046875 * 5120; time = 0.0261s; samplesPerSecond = 196055.9
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.08977051 * 5120; err = 0.33378906 * 5120; time = 0.0261s; samplesPerSecond = 196492.3
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.18295441 * 5120; err = 0.36210938 * 5120; time = 0.0261s; samplesPerSecond = 195913.4
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.09081879 * 5120; err = 0.32792969 * 5120; time = 0.0262s; samplesPerSecond = 195741.1
08/16/2016 09:57:19:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.05504150 * 5120; err = 0.32402344 * 5120; time = 0.0260s; samplesPerSecond = 196741.5
08/16/2016 09:57:19: Finished Epoch[ 2 of 4]: [Training] ce = 1.14134312 * 81920; err = 0.34833984 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.423751s
08/16/2016 09:57:19: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.2'

08/16/2016 09:57:19: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/16/2016 09:57:19: Starting minibatch loop.
08/16/2016 09:57:19:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.07509031 * 5120; err = 0.33964844 * 5120; time = 0.0268s; samplesPerSecond = 191187.5
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.09945135 * 5120; err = 0.33574219 * 5120; time = 0.0260s; samplesPerSecond = 197029.2
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.08966846 * 5120; err = 0.33730469 * 5120; time = 0.0261s; samplesPerSecond = 196507.4
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.05308990 * 5120; err = 0.32851562 * 5120; time = 0.0262s; samplesPerSecond = 195778.5
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07635727 * 5120; err = 0.32597656 * 5120; time = 0.0261s; samplesPerSecond = 195920.9
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07994118 * 5120; err = 0.33046875 * 5120; time = 0.0261s; samplesPerSecond = 196078.4
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06702271 * 5120; err = 0.33203125 * 5120; time = 0.0261s; samplesPerSecond = 195905.9
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08101730 * 5120; err = 0.34355469 * 5120; time = 0.0260s; samplesPerSecond = 196938.2
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09447479 * 5120; err = 0.34980469 * 5120; time = 0.0260s; samplesPerSecond = 196749.0
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.01812363 * 5120; err = 0.31425781 * 5120; time = 0.0261s; samplesPerSecond = 196514.9
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.03670349 * 5120; err = 0.32480469 * 5120; time = 0.0261s; samplesPerSecond = 195920.9
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.05544510 * 5120; err = 0.33085938 * 5120; time = 0.0261s; samplesPerSecond = 196326.5
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.06688385 * 5120; err = 0.33437500 * 5120; time = 0.0261s; samplesPerSecond = 196198.7
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.12102509 * 5120; err = 0.35625000 * 5120; time = 0.0261s; samplesPerSecond = 196514.9
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.03197174 * 5120; err = 0.31796875 * 5120; time = 0.0261s; samplesPerSecond = 196078.4
08/16/2016 09:57:20:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.03749237 * 5120; err = 0.33417969 * 5120; time = 0.0261s; samplesPerSecond = 196296.4
08/16/2016 09:57:20: Finished Epoch[ 3 of 4]: [Training] ce = 1.06773491 * 81920; err = 0.33348389 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.421033s
08/16/2016 09:57:20: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.3'

08/16/2016 09:57:20: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/16/2016 09:57:20: Starting minibatch loop.
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.02262182 * 5120; err = 0.32519531 * 5120; time = 0.0269s; samplesPerSecond = 190462.0
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.00228231 * 4926; err = 0.31607795 * 4926; time = 0.0494s; samplesPerSecond = 99750.9
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.02140751 * 5120; err = 0.31992188 * 5120; time = 0.0261s; samplesPerSecond = 196507.4
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.04078465 * 5120; err = 0.32089844 * 5120; time = 0.0261s; samplesPerSecond = 196334.1
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.03124695 * 5120; err = 0.32343750 * 5120; time = 0.0261s; samplesPerSecond = 195965.9
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 0.99630127 * 5120; err = 0.31679687 * 5120; time = 0.0261s; samplesPerSecond = 196296.4
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.01578407 * 5120; err = 0.31738281 * 5120; time = 0.0261s; samplesPerSecond = 195913.4
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00295029 * 5120; err = 0.30859375 * 5120; time = 0.0261s; samplesPerSecond = 195890.9
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.97806015 * 5120; err = 0.31933594 * 5120; time = 0.0261s; samplesPerSecond = 195860.9
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.98690643 * 5120; err = 0.30585937 * 5120; time = 0.0261s; samplesPerSecond = 196183.6
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.03510742 * 5120; err = 0.32187500 * 5120; time = 0.0261s; samplesPerSecond = 196146.0
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.98232269 * 5120; err = 0.30117187 * 5120; time = 0.0261s; samplesPerSecond = 195860.9
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.99689026 * 5120; err = 0.30976562 * 5120; time = 0.0262s; samplesPerSecond = 195763.6
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.96024933 * 5120; err = 0.29726562 * 5120; time = 0.0261s; samplesPerSecond = 196326.5
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.96784058 * 5120; err = 0.30312500 * 5120; time = 0.0261s; samplesPerSecond = 195928.4
08/16/2016 09:57:20:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.96324615 * 5120; err = 0.30195312 * 5120; time = 0.0261s; samplesPerSecond = 196545.1
08/16/2016 09:57:20: Finished Epoch[ 4 of 4]: [Training] ce = 1.00040827 * 81920; err = 0.31309814 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.446489s
08/16/2016 09:57:20: SGD: Saving checkpoint model '/tmp/cntk-test-20160816095713.701165/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech'
08/16/2016 09:57:20: CNTKCommandTrainEnd: speechTrain

08/16/2016 09:57:20: Action "train" complete.

08/16/2016 09:57:20: __COMPLETED__
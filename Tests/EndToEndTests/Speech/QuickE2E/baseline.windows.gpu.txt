=== Running /cygdrive/e/NetScale/CNTK/git_repos/cplx_master2/x64/debug/cntk.exe configFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E/cntk.config RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E DeviceId=0
-------------------------------------------------------------------
Build info: 

		Built time: Oct 24 2015 13:33:25
		Last modified date: Thu Oct 22 16:00:27 2015
		Built by amitaga on Amitaga-Win-DT3           
		Build Path: E:\NetScale\CNTK\git_repos\cplx_master2\MachineLearning\CNTK\
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
-------------------------------------------------------------------
running on Amitaga-Win-DT3 at 2015/10/24 22:07:22
command line: 
E:\NetScale\CNTK\git_repos\cplx_master2\x64\debug\cntk.exe configFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E/cntk.config RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E DeviceId=0 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision=float
command=speechTrain
deviceId=$DeviceId$
parallelTrain=false
speechTrain=[
    action=train
    modelPath=$RunDir$/models/cntkSpeech.dnn
    deviceId=$DeviceId$
    traceLevel=1
    SimpleNetworkBuilder=[
        layerSizes=363:512:512:132
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    ExperimentalNetworkBuilder=[    // the same as above but with BS
        layerSizes=363:512:512:132
        trainingCriterion='CE'
        evalCriterion='Err'
        applyMeanVarNorm=true
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = if trainingCriterion == 'CE'
             then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
             else Fail('unknown trainingCriterion ' + trainingCriterion)
        Err = if evalCriterion == 'Err' then
              ErrorPrediction(labels, outZ, tag='eval')
              else Fail('unknown evalCriterion ' + evalCriterion)
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD=[
        epochSize=20480
        minibatchSize=64:256:1024:
        learningRatesPerMB=1.0:0.5:0.1
        numMBsToShowResult=10
        momentumPerMB=0.9:0.656119
        dropoutRate=0.0
        maxEpochs=3
        keepCheckPointFiles=true       
        AutoAdjust=[
            reduceLearnRateIfImproveLessThan=0
            loadBestModel=true
            increaseLearnRateIfImproveMoreThan=1000000000
            learnRateDecreaseFactor=0.5
            learnRateIncreaseFactor=1.382
            autoAdjustLR=AdjustAfterEpoch
        ]
        clippingThresholdPerSample=1#INF
    ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=glob_0000.scp
      ]
      labels=[
          mlfFile=$DataDir$/glob_0000.mlf
          labelMappingFile=$DataDir$/state.list
          labelDim=132
          labelType=Category
      ]
    ]
]
RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu
DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision=float
command=speechTrain
deviceId=0
parallelTrain=false
speechTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu/models/cntkSpeech.dnn
    deviceId=0
    traceLevel=1
    SimpleNetworkBuilder=[
        layerSizes=363:512:512:132
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    ExperimentalNetworkBuilder=[    // the same as above but with BS
        layerSizes=363:512:512:132
        trainingCriterion='CE'
        evalCriterion='Err'
        applyMeanVarNorm=true
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = if trainingCriterion == 'CE'
             then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
             else Fail('unknown trainingCriterion ' + trainingCriterion)
        Err = if evalCriterion == 'Err' then
              ErrorPrediction(labels, outZ, tag='eval')
              else Fail('unknown evalCriterion ' + evalCriterion)
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD=[
        epochSize=20480
        minibatchSize=64:256:1024:
        learningRatesPerMB=1.0:0.5:0.1
        numMBsToShowResult=10
        momentumPerMB=0.9:0.656119
        dropoutRate=0.0
        maxEpochs=3
        keepCheckPointFiles=true       
        AutoAdjust=[
            reduceLearnRateIfImproveLessThan=0
            loadBestModel=true
            increaseLearnRateIfImproveMoreThan=1000000000
            learnRateDecreaseFactor=0.5
            learnRateIncreaseFactor=1.382
            autoAdjustLR=AdjustAfterEpoch
        ]
        clippingThresholdPerSample=1#INF
    ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=glob_0000.scp
      ]
      labels=[
          mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
          labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
          labelDim=132
          labelType=Category
      ]
    ]
]
RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu
DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.config:command=speechTrain
configparameters: cntk.config:ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E
configparameters: cntk.config:DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
configparameters: cntk.config:deviceId=0
configparameters: cntk.config:parallelTrain=false
configparameters: cntk.config:precision=float
configparameters: cntk.config:RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu
configparameters: cntk.config:speechTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu/models/cntkSpeech.dnn
    deviceId=0
    traceLevel=1
    SimpleNetworkBuilder=[
        layerSizes=363:512:512:132
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    ExperimentalNetworkBuilder=[    // the same as above but with BS
        layerSizes=363:512:512:132
        trainingCriterion='CE'
        evalCriterion='Err'
        applyMeanVarNorm=true
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = if trainingCriterion == 'CE'
             then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
             else Fail('unknown trainingCriterion ' + trainingCriterion)
        Err = if evalCriterion == 'Err' then
              ErrorPrediction(labels, outZ, tag='eval')
              else Fail('unknown evalCriterion ' + evalCriterion)
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD=[
        epochSize=20480
        minibatchSize=64:256:1024:
        learningRatesPerMB=1.0:0.5:0.1
        numMBsToShowResult=10
        momentumPerMB=0.9:0.656119
        dropoutRate=0.0
        maxEpochs=3
        keepCheckPointFiles=true       
        AutoAdjust=[
            reduceLearnRateIfImproveLessThan=0
            loadBestModel=true
            increaseLearnRateIfImproveMoreThan=1000000000
            learnRateDecreaseFactor=0.5
            learnRateIncreaseFactor=1.382
            autoAdjustLR=AdjustAfterEpoch
        ]
        clippingThresholdPerSample=1#INF
    ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=glob_0000.scp
      ]
      labels=[
          mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
          labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
          labelDim=132
          labelType=Category
      ]
    ]
]

<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: speechTrain 
precision = float
CNTKModelPath: C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu/models/cntkSpeech.dnn
CNTKCommandTrainInfo: speechTrain : 3
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
CNTKCommandTrainBegin: speechTrain
SimpleNetworkBuilder Using GPU 0
reading script file glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
htkmlfreader: reading MLF file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
SetUniformRandomValue (GPU): creating curand object with seed 1
GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Validating for node CrossEntropyWithSoftmax. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 3]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 3]) -> [512, MBSize 3]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 3]) -> [512, MBSize 3]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 3], B1[512, 1]) -> [512, MBSize 3]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 3]) -> [512, MBSize 3]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 3]) -> [132, MBSize 3]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 3], B2[132, 1]) -> [132, MBSize 3]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 3], HLast[132, MBSize 3]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 3]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 3]) -> [512, MBSize 3]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 3]) -> [512, MBSize 3]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 3], B1[512, 1]) -> [512, MBSize 3]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 3]) -> [512, MBSize 3]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 3]) -> [132, MBSize 3]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 3], B2[132, 1]) -> [132, MBSize 3]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 3], HLast[132, MBSize 3]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 3]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 3]) -> [512, MBSize 3]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 3]) -> [512, MBSize 3]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 3], B1[512, 1]) -> [512, MBSize 3]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 3]) -> [512, MBSize 3]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 3]) -> [132, MBSize 3]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 3], B2[132, 1]) -> [132, MBSize 3]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 3], HLast[132, MBSize 3]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Precomputing --> 3 PreCompute nodes found.

	NodeName: InvStdOfFeatures
	NodeName: MeanOfFeatures
	NodeName: Prior
minibatchiterator: epoch 0: frames [0..252734] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms


Validating for node InvStdOfFeatures. 2 nodes to process in pass 1.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]

Validating for node InvStdOfFeatures, final verification.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.



Validating for node MeanOfFeatures. 2 nodes to process in pass 1.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]

Validating for node MeanOfFeatures, final verification.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.



Validating for node Prior. 2 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]

Validating for node Prior. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]

Validating for node Prior, final verification.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.


Precomputing --> Completed.

Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.015625  effective momentum = 0.900000 
minibatchiterator: epoch 0: frames [0..20480] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses


Validating for node EvalErrorPrediction. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 62]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 62]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 62]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 62]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 62], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 62]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 62]) -> [512, MBSize 62]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 62], B0[512, 1]) -> [512, MBSize 62]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 62]) -> [512, MBSize 62]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 62]) -> [512, MBSize 62]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 62], B1[512, 1]) -> [512, MBSize 62]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 62]) -> [512, MBSize 62]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 62]) -> [132, MBSize 62]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 62], B2[132, 1]) -> [132, MBSize 62]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 62], HLast[132, MBSize 62]) -> [1, 1]

Validating for node EvalErrorPrediction. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 62]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 62]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 62]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 62]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 62], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 62]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 62]) -> [512, MBSize 62]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 62], B0[512, 1]) -> [512, MBSize 62]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 62]) -> [512, MBSize 62]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 62]) -> [512, MBSize 62]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 62], B1[512, 1]) -> [512, MBSize 62]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 62]) -> [512, MBSize 62]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 62]) -> [132, MBSize 62]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 62], B2[132, 1]) -> [132, MBSize 62]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 62], HLast[132, MBSize 62]) -> [1, 1]

Validating for node EvalErrorPrediction, final verification.

Validating --> labels = InputValue -> [132, MBSize 62]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 62]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 62]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 62]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 62], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 62]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 62]) -> [512, MBSize 62]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 62], B0[512, 1]) -> [512, MBSize 62]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 62]) -> [512, MBSize 62]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 62]) -> [512, MBSize 62]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 62], B1[512, 1]) -> [512, MBSize 62]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 62]) -> [512, MBSize 62]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 62]) -> [132, MBSize 62]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 62], B2[132, 1]) -> [132, MBSize 62]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 62], HLast[132, MBSize 62]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Starting minibatch loop.
 Epoch[ 1 of 3]-Minibatch[   1-  10 of 320]: SamplesSeen = 640; TrainLossPerSample =  4.45645981; EvalErr[0]PerSample = 0.92500000; TotalTime = 0.15527s; TotalTimePerSample = 0.24261ms; SamplesPerSecond = 4121
 Epoch[ 1 of 3]-Minibatch[  11-  20 of 320]: SamplesSeen = 640; TrainLossPerSample =  4.22315750; EvalErr[0]PerSample = 0.90156250; TotalTime = 0.17254s; TotalTimePerSample = 0.26960ms; SamplesPerSecond = 3709
 Epoch[ 1 of 3]-Minibatch[  21-  30 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.95180664; EvalErr[0]PerSample = 0.84687500; TotalTime = 0.16283s; TotalTimePerSample = 0.25443ms; SamplesPerSecond = 3930
 Epoch[ 1 of 3]-Minibatch[  31-  40 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.94158020; EvalErr[0]PerSample = 0.89843750; TotalTime = 0.15770s; TotalTimePerSample = 0.24641ms; SamplesPerSecond = 4058
 Epoch[ 1 of 3]-Minibatch[  41-  50 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.85668945; EvalErr[0]PerSample = 0.91093750; TotalTime = 0.17209s; TotalTimePerSample = 0.26889ms; SamplesPerSecond = 3719
 Epoch[ 1 of 3]-Minibatch[  51-  60 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.72866364; EvalErr[0]PerSample = 0.89531250; TotalTime = 0.16186s; TotalTimePerSample = 0.25291ms; SamplesPerSecond = 3954
 Epoch[ 1 of 3]-Minibatch[  61-  70 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.51809235; EvalErr[0]PerSample = 0.82968750; TotalTime = 0.15901s; TotalTimePerSample = 0.24846ms; SamplesPerSecond = 4024
 Epoch[ 1 of 3]-Minibatch[  71-  80 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.48455200; EvalErr[0]PerSample = 0.80781250; TotalTime = 0.15480s; TotalTimePerSample = 0.24188ms; SamplesPerSecond = 4134
 Epoch[ 1 of 3]-Minibatch[  81-  90 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.33829346; EvalErr[0]PerSample = 0.76875000; TotalTime = 0.15737s; TotalTimePerSample = 0.24588ms; SamplesPerSecond = 4066
 Epoch[ 1 of 3]-Minibatch[  91- 100 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.50167236; EvalErr[0]PerSample = 0.79843750; TotalTime = 0.15904s; TotalTimePerSample = 0.24849ms; SamplesPerSecond = 4024
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 3]-Minibatch[ 101- 110 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.22861633; EvalErr[0]PerSample = 0.80000000; TotalTime = 0.16485s; TotalTimePerSample = 0.25757ms; SamplesPerSecond = 3882
 Epoch[ 1 of 3]-Minibatch[ 111- 120 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.32616882; EvalErr[0]PerSample = 0.79062500; TotalTime = 0.15116s; TotalTimePerSample = 0.23618ms; SamplesPerSecond = 4234
 Epoch[ 1 of 3]-Minibatch[ 121- 130 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.16897583; EvalErr[0]PerSample = 0.77968750; TotalTime = 0.15167s; TotalTimePerSample = 0.23699ms; SamplesPerSecond = 4219
 Epoch[ 1 of 3]-Minibatch[ 131- 140 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.08891907; EvalErr[0]PerSample = 0.77656250; TotalTime = 0.16170s; TotalTimePerSample = 0.25265ms; SamplesPerSecond = 3958
 Epoch[ 1 of 3]-Minibatch[ 141- 150 of 320]: SamplesSeen = 640; TrainLossPerSample =  3.06005249; EvalErr[0]PerSample = 0.72968750; TotalTime = 0.15522s; TotalTimePerSample = 0.24254ms; SamplesPerSecond = 4123
 Epoch[ 1 of 3]-Minibatch[ 151- 160 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.91128540; EvalErr[0]PerSample = 0.69531250; TotalTime = 0.15756s; TotalTimePerSample = 0.24618ms; SamplesPerSecond = 4062
 Epoch[ 1 of 3]-Minibatch[ 161- 170 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.90172119; EvalErr[0]PerSample = 0.72968750; TotalTime = 0.15992s; TotalTimePerSample = 0.24987ms; SamplesPerSecond = 4002
 Epoch[ 1 of 3]-Minibatch[ 171- 180 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.73261719; EvalErr[0]PerSample = 0.65312500; TotalTime = 0.16060s; TotalTimePerSample = 0.25093ms; SamplesPerSecond = 3985
 Epoch[ 1 of 3]-Minibatch[ 181- 190 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.66515503; EvalErr[0]PerSample = 0.68437500; TotalTime = 0.15478s; TotalTimePerSample = 0.24184ms; SamplesPerSecond = 4134
 Epoch[ 1 of 3]-Minibatch[ 191- 200 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.67383423; EvalErr[0]PerSample = 0.66406250; TotalTime = 0.14563s; TotalTimePerSample = 0.22755ms; SamplesPerSecond = 4394
 Epoch[ 1 of 3]-Minibatch[ 201- 210 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.52869263; EvalErr[0]PerSample = 0.63593750; TotalTime = 0.15331s; TotalTimePerSample = 0.23955ms; SamplesPerSecond = 4174
 Epoch[ 1 of 3]-Minibatch[ 211- 220 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.60032349; EvalErr[0]PerSample = 0.66718750; TotalTime = 0.15816s; TotalTimePerSample = 0.24713ms; SamplesPerSecond = 4046
 Epoch[ 1 of 3]-Minibatch[ 221- 230 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.51134033; EvalErr[0]PerSample = 0.64843750; TotalTime = 0.16587s; TotalTimePerSample = 0.25917ms; SamplesPerSecond = 3858
 Epoch[ 1 of 3]-Minibatch[ 231- 240 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.45362549; EvalErr[0]PerSample = 0.63750000; TotalTime = 0.15854s; TotalTimePerSample = 0.24772ms; SamplesPerSecond = 4036
 Epoch[ 1 of 3]-Minibatch[ 241- 250 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.41640015; EvalErr[0]PerSample = 0.61562500; TotalTime = 0.15948s; TotalTimePerSample = 0.24919ms; SamplesPerSecond = 4013
 Epoch[ 1 of 3]-Minibatch[ 251- 260 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.39745483; EvalErr[0]PerSample = 0.62812500; TotalTime = 0.16179s; TotalTimePerSample = 0.25280ms; SamplesPerSecond = 3955
 Epoch[ 1 of 3]-Minibatch[ 261- 270 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.16415405; EvalErr[0]PerSample = 0.56718750; TotalTime = 0.16235s; TotalTimePerSample = 0.25367ms; SamplesPerSecond = 3942
 Epoch[ 1 of 3]-Minibatch[ 271- 280 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.30347290; EvalErr[0]PerSample = 0.63593750; TotalTime = 0.15271s; TotalTimePerSample = 0.23861ms; SamplesPerSecond = 4190
 Epoch[ 1 of 3]-Minibatch[ 281- 290 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.24398804; EvalErr[0]PerSample = 0.60937500; TotalTime = 0.16522s; TotalTimePerSample = 0.25815ms; SamplesPerSecond = 3873
 Epoch[ 1 of 3]-Minibatch[ 291- 300 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.15322266; EvalErr[0]PerSample = 0.57968750; TotalTime = 0.15988s; TotalTimePerSample = 0.24982ms; SamplesPerSecond = 4002
 Epoch[ 1 of 3]-Minibatch[ 301- 310 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.21664429; EvalErr[0]PerSample = 0.59531250; TotalTime = 0.14906s; TotalTimePerSample = 0.23290ms; SamplesPerSecond = 4293
 Epoch[ 1 of 3]-Minibatch[ 311- 320 of 320]: SamplesSeen = 640; TrainLossPerSample =  2.25246582; EvalErr[0]PerSample = 0.60156250; TotalTime = 0.14161s; TotalTimePerSample = 0.22126ms; SamplesPerSecond = 4519
Finished Epoch[ 1 of 3]: [Training Set] TrainLossPerSample = 3.0000031; EvalErrPerSample = 0.72836918; AvgLearningRatePerSample = 0.015625; EpochTime=5.105428
Starting Epoch 2: learning rate per sample = 0.001953  effective momentum = 0.656119 
minibatchiterator: epoch 1: frames [20480..40960] (first utterance at frame 20480), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 3]-Minibatch[   1-  10 of 80]: SamplesSeen = 2560; TrainLossPerSample =  2.08151951; EvalErr[0]PerSample = 0.55859375; TotalTime = 0.35121s; TotalTimePerSample = 0.13719ms; SamplesPerSecond = 7289
 Epoch[ 2 of 3]-Minibatch[  11-  20 of 80]: SamplesSeen = 2560; TrainLossPerSample =  1.98395710; EvalErr[0]PerSample = 0.54257813; TotalTime = 0.31404s; TotalTimePerSample = 0.12267ms; SamplesPerSecond = 8151
 Epoch[ 2 of 3]-Minibatch[  21-  30 of 80]: SamplesSeen = 2560; TrainLossPerSample =  1.98575516; EvalErr[0]PerSample = 0.54492188; TotalTime = 0.27053s; TotalTimePerSample = 0.10567ms; SamplesPerSecond = 9463
 Epoch[ 2 of 3]-Minibatch[  31-  40 of 80]: SamplesSeen = 2560; TrainLossPerSample =  1.90485039; EvalErr[0]PerSample = 0.53164062; TotalTime = 0.24565s; TotalTimePerSample = 0.09596ms; SamplesPerSecond = 10421
 Epoch[ 2 of 3]-Minibatch[  41-  50 of 80]: SamplesSeen = 2560; TrainLossPerSample =  1.88324280; EvalErr[0]PerSample = 0.52539063; TotalTime = 0.22956s; TotalTimePerSample = 0.08967ms; SamplesPerSecond = 11151
 Epoch[ 2 of 3]-Minibatch[  51-  60 of 80]: SamplesSeen = 2560; TrainLossPerSample =  1.89109344; EvalErr[0]PerSample = 0.53359375; TotalTime = 0.22156s; TotalTimePerSample = 0.08655ms; SamplesPerSecond = 11554
 Epoch[ 2 of 3]-Minibatch[  61-  70 of 80]: SamplesSeen = 2560; TrainLossPerSample =  1.89496155; EvalErr[0]PerSample = 0.52890625; TotalTime = 0.21987s; TotalTimePerSample = 0.08589ms; SamplesPerSecond = 11643
 Epoch[ 2 of 3]-Minibatch[  71-  80 of 80]: SamplesSeen = 2560; TrainLossPerSample =  1.85944366; EvalErr[0]PerSample = 0.52265625; TotalTime = 0.19881s; TotalTimePerSample = 0.07766ms; SamplesPerSecond = 12876
Finished Epoch[ 2 of 3]: [Training Set] TrainLossPerSample = 1.935603; EvalErrPerSample = 0.53603518; AvgLearningRatePerSample = 0.001953125; EpochTime=2.098193
Starting Epoch 3: learning rate per sample = 0.000098  effective momentum = 0.656119 
minibatchiterator: epoch 2: frames [40960..61440] (first utterance at frame 40960), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 3]-Minibatch[   1-  10 of 20]: SamplesSeen = 10240; TrainLossPerSample =  1.86752853; EvalErr[0]PerSample = 0.52177734; TotalTime = 0.71783s; TotalTimePerSample = 0.07010ms; SamplesPerSecond = 14265
 Epoch[ 3 of 3]-Minibatch[  11-  20 of 20]: SamplesSeen = 10240; TrainLossPerSample =  1.87358818; EvalErr[0]PerSample = 0.51542969; TotalTime = 0.60551s; TotalTimePerSample = 0.05913ms; SamplesPerSecond = 16911
Finished Epoch[ 3 of 3]: [Training Set] TrainLossPerSample = 1.8705584; EvalErrPerSample = 0.5186035; AvgLearningRatePerSample = 9.765625146e-005; EpochTime=1.428405
CNTKCommandTrainEnd: speechTrain
__COMPLETED__
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running /cygdrive/e/NetScale/CNTK/git_repos/cplx_master2/x64/debug/cntk.exe configFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E/cntk.config RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E DeviceId=0
-------------------------------------------------------------------
Build info: 

		Built time: Oct 24 2015 13:33:25
		Last modified date: Thu Oct 22 16:00:27 2015
		Built by amitaga on Amitaga-Win-DT3           
		Build Path: E:\NetScale\CNTK\git_repos\cplx_master2\MachineLearning\CNTK\
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
-------------------------------------------------------------------
running on Amitaga-Win-DT3 at 2015/10/24 22:08:20
command line: 
E:\NetScale\CNTK\git_repos\cplx_master2\x64\debug\cntk.exe configFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E/cntk.config RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E DeviceId=0 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision=float
command=speechTrain
deviceId=$DeviceId$
parallelTrain=false
speechTrain=[
    action=train
    modelPath=$RunDir$/models/cntkSpeech.dnn
    deviceId=$DeviceId$
    traceLevel=1
    SimpleNetworkBuilder=[
        layerSizes=363:512:512:132
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    ExperimentalNetworkBuilder=[    // the same as above but with BS
        layerSizes=363:512:512:132
        trainingCriterion='CE'
        evalCriterion='Err'
        applyMeanVarNorm=true
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = if trainingCriterion == 'CE'
             then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
             else Fail('unknown trainingCriterion ' + trainingCriterion)
        Err = if evalCriterion == 'Err' then
              ErrorPrediction(labels, outZ, tag='eval')
              else Fail('unknown evalCriterion ' + evalCriterion)
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD=[
        epochSize=20480
        minibatchSize=64:256:1024:
        learningRatesPerMB=1.0:0.5:0.1
        numMBsToShowResult=10
        momentumPerMB=0.9:0.656119
        dropoutRate=0.0
        maxEpochs=3
        keepCheckPointFiles=true       
        AutoAdjust=[
            reduceLearnRateIfImproveLessThan=0
            loadBestModel=true
            increaseLearnRateIfImproveMoreThan=1000000000
            learnRateDecreaseFactor=0.5
            learnRateIncreaseFactor=1.382
            autoAdjustLR=AdjustAfterEpoch
        ]
        clippingThresholdPerSample=1#INF
    ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=glob_0000.scp
      ]
      labels=[
          mlfFile=$DataDir$/glob_0000.mlf
          labelMappingFile=$DataDir$/state.list
          labelDim=132
          labelType=Category
      ]
    ]
]
RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu
DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision=float
command=speechTrain
deviceId=0
parallelTrain=false
speechTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu/models/cntkSpeech.dnn
    deviceId=0
    traceLevel=1
    SimpleNetworkBuilder=[
        layerSizes=363:512:512:132
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    ExperimentalNetworkBuilder=[    // the same as above but with BS
        layerSizes=363:512:512:132
        trainingCriterion='CE'
        evalCriterion='Err'
        applyMeanVarNorm=true
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = if trainingCriterion == 'CE'
             then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
             else Fail('unknown trainingCriterion ' + trainingCriterion)
        Err = if evalCriterion == 'Err' then
              ErrorPrediction(labels, outZ, tag='eval')
              else Fail('unknown evalCriterion ' + evalCriterion)
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD=[
        epochSize=20480
        minibatchSize=64:256:1024:
        learningRatesPerMB=1.0:0.5:0.1
        numMBsToShowResult=10
        momentumPerMB=0.9:0.656119
        dropoutRate=0.0
        maxEpochs=3
        keepCheckPointFiles=true       
        AutoAdjust=[
            reduceLearnRateIfImproveLessThan=0
            loadBestModel=true
            increaseLearnRateIfImproveMoreThan=1000000000
            learnRateDecreaseFactor=0.5
            learnRateIncreaseFactor=1.382
            autoAdjustLR=AdjustAfterEpoch
        ]
        clippingThresholdPerSample=1#INF
    ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=glob_0000.scp
      ]
      labels=[
          mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
          labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
          labelDim=132
          labelType=Category
      ]
    ]
]
RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu
DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.config:command=speechTrain
configparameters: cntk.config:ConfigDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\QuickE2E
configparameters: cntk.config:DataDir=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data
configparameters: cntk.config:deviceId=0
configparameters: cntk.config:parallelTrain=false
configparameters: cntk.config:precision=float
configparameters: cntk.config:RunDir=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu
configparameters: cntk.config:speechTrain=[
    action=train
    modelPath=C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu/models/cntkSpeech.dnn
    deviceId=0
    traceLevel=1
    SimpleNetworkBuilder=[
        layerSizes=363:512:512:132
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    ExperimentalNetworkBuilder=[    // the same as above but with BS
        layerSizes=363:512:512:132
        trainingCriterion='CE'
        evalCriterion='Err'
        applyMeanVarNorm=true
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then SBFF(layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else SBFF(featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = if trainingCriterion == 'CE'
             then CrossEntropyWithSoftmax(labels, outZ, tag='criterion')
             else Fail('unknown trainingCriterion ' + trainingCriterion)
        Err = if evalCriterion == 'Err' then
              ErrorPrediction(labels, outZ, tag='eval')
              else Fail('unknown evalCriterion ' + evalCriterion)
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD=[
        epochSize=20480
        minibatchSize=64:256:1024:
        learningRatesPerMB=1.0:0.5:0.1
        numMBsToShowResult=10
        momentumPerMB=0.9:0.656119
        dropoutRate=0.0
        maxEpochs=3
        keepCheckPointFiles=true       
        AutoAdjust=[
            reduceLearnRateIfImproveLessThan=0
            loadBestModel=true
            increaseLearnRateIfImproveMoreThan=1000000000
            learnRateDecreaseFactor=0.5
            learnRateIncreaseFactor=1.382
            autoAdjustLR=AdjustAfterEpoch
        ]
        clippingThresholdPerSample=1#INF
    ]
    reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=0
      features=[
          dim=363
          type=Real
          scpFile=glob_0000.scp
      ]
      labels=[
          mlfFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf
          labelMappingFile=E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
          labelDim=132
          labelType=Category
      ]
    ]
]

<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: speechTrain 
precision = float
CNTKModelPath: C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu/models/cntkSpeech.dnn
CNTKCommandTrainInfo: speechTrain : 3
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
CNTKCommandTrainBegin: speechTrain
SimpleNetworkBuilder Using GPU 0
reading script file glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/state.list
htkmlfreader: reading MLF file E:\NetScale\CNTK\git_repos\cplx_master2\Tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File C:\cygwin64\tmp\cntk-test-20151024140721.927553\Speech_QuickE2E@debug_gpu/models/cntkSpeech.dnn.2.


Printing Gradient Computation Node Order ... 

CrossEntropyWithSoftmax[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], HLast[0, 0])
HLast[0, 0] = Plus(W2*H1[0, 0], B2[132, 1])
B2[132, 1] = LearnableParameter
W2*H1[0, 0] = Times(W2[132, 512], H2[0, 0])
H2[0, 0] = Sigmoid(W1*H1+B1[0, 0])
W1*H1+B1[0, 0] = Plus(W1*H1[0, 0], B1[512, 1])
B1[512, 1] = LearnableParameter
W1*H1[0, 0] = Times(W1[512, 512], H1[0, 0])
H1[0, 0] = Sigmoid(W0*features+B0[0, 0])
W0*features+B0[0, 0] = Plus(W0*features[0, 0], B0[512, 1])
B0[512, 1] = LearnableParameter
W0*features[0, 0] = Times(W0[512, 363], MVNormalizedFeatures[0, 0])
MVNormalizedFeatures[0, 0] = PerDimMeanVarNormalization(features[363, 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1])
InvStdOfFeatures[363, 1] = InvStdDev(features[363, 0])
MeanOfFeatures[363, 1] = Mean(features[363, 0])
features[363, 0] = InputValue
W0[512, 363] = LearnableParameter
W1[512, 512] = LearnableParameter
W2[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node CrossEntropyWithSoftmax. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node CrossEntropyWithSoftmax. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 22 nodes to process in pass 1.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 11 nodes to process in pass 2.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

10 out of 22 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 22 nodes to process in pass 1.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 9 nodes to process in pass 2.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

10 out of 22 nodes do not share the minibatch layout with the input data.



Validating for node EvalErrorPrediction. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node EvalErrorPrediction. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 3: learning rate per sample = 0.000098  effective momentum = 0.656119 
minibatchiterator: epoch 2: frames [40960..61440] (first utterance at frame 40960), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 3 of 3]-Minibatch[   1-  10 of 20]: SamplesSeen = 10240; TrainLossPerSample =  1.86752853; EvalErr[0]PerSample = 0.52177734; TotalTime = 1.50756s; TotalTimePerSample = 0.14722ms; SamplesPerSecond = 6792
 Epoch[ 3 of 3]-Minibatch[  11-  20 of 20]: SamplesSeen = 10240; TrainLossPerSample =  1.87358818; EvalErr[0]PerSample = 0.51542969; TotalTime = 0.86938s; TotalTimePerSample = 0.08490ms; SamplesPerSecond = 11778
Finished Epoch[ 3 of 3]: [Training Set] TrainLossPerSample = 1.8705584; EvalErrPerSample = 0.5186035; AvgLearningRatePerSample = 9.765625146e-005; EpochTime=6.283729
CNTKCommandTrainEnd: speechTrain
__COMPLETED__

-------------------------------------------------------------------
Build info: 

        Built time: Nov 23 2015 10:00:15
        Last modified date: Mon Nov 23 09:45:21 2015
        Built by alexeyk on alexey-rz           
        Build Path: C:\src\cntk\MachineLearning\CNTK\
        CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
-------------------------------------------------------------------
running on alexey-rz at 2015/11/23 10:36:55
command line: 
C:\src\cntk\x64\Debug\CNTK.exe configFile=C:\src\cntk\Tests\Image\QuickE2E\cntk.config RunDir=C:\src\cntk\Tests\Image\_run DataDir=C:\src\cntk\Tests\Image\Data ConfigDir=C:\src\cntk\Tests\Image\QuickE2E DeviceId=-1 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision=float
command=Train:Test
deviceId=$DeviceId$
ndlMacros=$ConfigDir$/Macros.ndl
parallelTrain=false
Train=[
    action=train
    modelPath=$RunDir$/models/cntk.dnn
    deviceId=$DeviceId$
    traceLevel=1
        NDLNetworkBuilder=[
                networkDescription=$ConfigDir$/Convolution.ndl
        ]
    SGD=[
        epochSize=100
        minibatchSize=10
        learningRatesPerMB=0.05
        momentumPerMB=0*10:0.7
        maxEpochs=12
    ]
    reader=[
        readerType=UCIFastReader
        file=$DataDir$/Train.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=$DataDir$/labelsmap.txt
        ]
    ]    
]
Test=[
    action=test
    modelPath=$RunDir$/models/cntk.dnn
     NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/Convolution.ndl
    ]
    reader=[
        readerType=UCIFastReader
        file=$DataDir$/Test.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=$DataDir$/labelsmap.txt
        ]
    ]    
]
RunDir=C:\src\cntk\Tests\Image\_run
DataDir=C:\src\cntk\Tests\Image\Data
ConfigDir=C:\src\cntk\Tests\Image\QuickE2E
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision=float
command=Train:Test
deviceId=-1
ndlMacros=C:\src\cntk\Tests\Image\QuickE2E/Macros.ndl
parallelTrain=false
Train=[
    action=train
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
    deviceId=-1
    traceLevel=1
        NDLNetworkBuilder=[
                networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
        ]
    SGD=[
        epochSize=100
        minibatchSize=10
        learningRatesPerMB=0.05
        momentumPerMB=0*10:0.7
        maxEpochs=12
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Train.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]
Test=[
    action=test
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
     NDLNetworkBuilder=[
        networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Test.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]
RunDir=C:\src\cntk\Tests\Image\_run
DataDir=C:\src\cntk\Tests\Image\Data
ConfigDir=C:\src\cntk\Tests\Image\QuickE2E
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.config:command=Train:Test
configparameters: cntk.config:ConfigDir=C:\src\cntk\Tests\Image\QuickE2E
configparameters: cntk.config:DataDir=C:\src\cntk\Tests\Image\Data
configparameters: cntk.config:deviceId=-1
configparameters: cntk.config:ndlMacros=C:\src\cntk\Tests\Image\QuickE2E/Macros.ndl
configparameters: cntk.config:parallelTrain=false
configparameters: cntk.config:precision=float
configparameters: cntk.config:RunDir=C:\src\cntk\Tests\Image\_run
configparameters: cntk.config:Test=[
    action=test
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
     NDLNetworkBuilder=[
        networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Test.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]

configparameters: cntk.config:Train=[
    action=train
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
    deviceId=-1
    traceLevel=1
        NDLNetworkBuilder=[
                networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
        ]
    SGD=[
        epochSize=100
        minibatchSize=10
        learningRatesPerMB=0.05
        momentumPerMB=0*10:0.7
        maxEpochs=12
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Train.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]

<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: Train Test 
precision = float
CNTKModelPath: C:\src\cntk\Tests\Image\_run/models/cntk.dnn
CNTKCommandTrainInfo: Train : 12
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 12
CNTKCommandTrainBegin: Train
NDLBuilder Using CPU
reading uci file C:\src\cntk\Tests\Image\Data/Train.txt


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

CE[0, 0] = CrossEntropyWithSoftmax(labels[10, 1], OutputNodes.z[0, 0])
OutputNodes.z[0, 0] = Plus(OutputNodes.t[0, 0], OutputNodes.b[10, 1])
OutputNodes.b[10, 1] = LearnableParameter
OutputNodes.t[0, 0] = Times(OutputNodes.W[10, 128], h1.y[0, 0])
h1.y[0, 0] = Sigmoid(h1.z[0, 0])
h1.z[0, 0] = Plus(h1.t[0, 0], h1.b[128, 1])
h1.b[128, 1] = LearnableParameter
h1.t[0, 0] = Times(h1.W[128, 512], pool2[0, 0])
pool2[0, 0] = AveragePooling(conv2_act.act[0, 0])
conv2_act.act[0, 0] = RectifiedLinear(conv2_act.convPlusB[0, 0])
conv2_act.convPlusB[0, 0] = Plus(conv2_act.conv[0, 0], conv2_act.convB[32, 1])
conv2_act.convB[32, 1] = LearnableParameter
conv2_act.conv[0, 0] = Convolution(conv2_act.convW[32, 400], pool1[0, 0])
pool1[0, 0] = MaxPooling(conv1_act.act[0, 0])
conv1_act.act[0, 0] = RectifiedLinear(conv1_act.convPlusB[0, 0])
conv1_act.convPlusB[0, 0] = Plus(conv1_act.conv[0, 0], conv1_act.convB[16, 1])
conv1_act.convB[16, 1] = LearnableParameter
conv1_act.conv[0, 0] = Convolution(conv1_act.convW[16, 25], featScaled[0, 0])
featScaled[0, 0] = Scale(featScale[1, 1], features[784, 1])
features[784, 1] = InputValue
featScale[1, 1] = LearnableParameter
conv1_act.convW[16, 25] = LearnableParameter
conv2_act.convW[32, 400] = LearnableParameter
h1.W[128, 512] = LearnableParameter
OutputNodes.W[10, 128] = LearnableParameter
labels[10, 1] = InputValue

Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node CE. 15 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node CE. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> Err = ErrorPrediction(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> Err = ErrorPrediction(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> Err = ErrorPrediction(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> Err = ErrorPrediction(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> Err = ErrorPrediction(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 1]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 1]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 1]) -> [784, MBSize 1]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 1]) -> [9216, MBSize 1]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 1], conv1_act.convB[16, 1]) -> [9216, MBSize 1]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 1]) -> [9216, MBSize 1]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 1]) -> [2304, MBSize 1]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 1]) -> [2048, MBSize 1]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 1], conv2_act.convB[32, 1]) -> [2048, MBSize 1]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 1]) -> [2048, MBSize 1]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 1]) -> [512, MBSize 1]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 1]) -> [128, MBSize 1]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 1], h1.b[128, 1]) -> [128, MBSize 1]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 1]) -> [128, MBSize 1]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 1]) -> [10, MBSize 1]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 1], OutputNodes.b[10, 1]) -> [10, MBSize 1]
Validating --> Err = ErrorPrediction(labels[10, MBSize 1], OutputNodes.z[10, MBSize 1]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.

SGD using CPU.
GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting at epoch 0 counting lines to determine record count

 1000 records found
starting epoch 0 at record count 0, and file position 0
already there from last epoch

Starting minibatch loop.
randomordering: 11 retries for 100 elements (11.0%) to ensure window condition
randomordering: recached sequence for seed 0: 15, 33, ...
 Epoch[ 1 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  2.37230545; EvalErr[0]PerSample = 0.90000000; TotalTime = 1.21884s; TotalTimePerSample = 12.18842ms; SamplesPerSecond = 82
Finished Epoch[ 1 of 12]: [Training Set] TrainLossPerSample = 2.3723054; EvalErrPerSample = 0.89999998; AvgLearningRatePerSample = 0.004999999888; EpochTime=1.250578
Starting Epoch 2: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 1 at record count 100, and file position 100
already there from last epoch

Starting minibatch loop.
randomordering: 26 retries for 100 elements (26.0%) to ensure window condition
randomordering: recached sequence for seed 1: 20, 26, ...
 Epoch[ 2 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  2.34000610; EvalErr[0]PerSample = 0.88000000; TotalTime = 0.65477s; TotalTimePerSample = 6.54771ms; SamplesPerSecond = 152
Finished Epoch[ 2 of 12]: [Training Set] TrainLossPerSample = 2.3400061; EvalErrPerSample = 0.88; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.65512
Starting Epoch 3: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 2 at record count 200, and file position 200
already there from last epoch

Starting minibatch loop.
randomordering: 28 retries for 100 elements (28.0%) to ensure window condition
randomordering: recached sequence for seed 2: 4, 35, ...
 Epoch[ 3 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  2.23482727; EvalErr[0]PerSample = 0.80000000; TotalTime = 0.66302s; TotalTimePerSample = 6.63021ms; SamplesPerSecond = 150
Finished Epoch[ 3 of 12]: [Training Set] TrainLossPerSample = 2.2348273; EvalErrPerSample = 0.79999995; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.663372
Starting Epoch 4: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 3 at record count 300, and file position 300
already there from last epoch

Starting minibatch loop.
randomordering: 17 retries for 100 elements (17.0%) to ensure window condition
randomordering: recached sequence for seed 3: 28, 7, ...
 Epoch[ 4 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  2.18458984; EvalErr[0]PerSample = 0.82000000; TotalTime = 0.66943s; TotalTimePerSample = 6.69434ms; SamplesPerSecond = 149
Finished Epoch[ 4 of 12]: [Training Set] TrainLossPerSample = 2.1845899; EvalErrPerSample = 0.81999999; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.669791
Starting Epoch 5: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 4 at record count 400, and file position 400
already there from last epoch

Starting minibatch loop.
randomordering: 15 retries for 100 elements (15.0%) to ensure window condition
randomordering: recached sequence for seed 4: 5, 36, ...
 Epoch[ 5 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  1.94950211; EvalErr[0]PerSample = 0.68000000; TotalTime = 0.65519s; TotalTimePerSample = 6.55185ms; SamplesPerSecond = 152
Finished Epoch[ 5 of 12]: [Training Set] TrainLossPerSample = 1.9495021; EvalErrPerSample = 0.68000001; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.655564
Starting Epoch 6: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 5 at record count 500, and file position 500
already there from last epoch

Starting minibatch loop.
randomordering: 13 retries for 100 elements (13.0%) to ensure window condition
randomordering: recached sequence for seed 5: 11, 48, ...
 Epoch[ 6 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  1.70178970; EvalErr[0]PerSample = 0.31000000; TotalTime = 0.66804s; TotalTimePerSample = 6.68037ms; SamplesPerSecond = 149
Finished Epoch[ 6 of 12]: [Training Set] TrainLossPerSample = 1.7017896; EvalErrPerSample = 0.31; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.668389
Starting Epoch 7: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 6 at record count 600, and file position 600
already there from last epoch

Starting minibatch loop.
randomordering: 13 retries for 100 elements (13.0%) to ensure window condition
randomordering: recached sequence for seed 6: 15, 3, ...
 Epoch[ 7 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  1.36245956; EvalErr[0]PerSample = 0.17000000; TotalTime = 0.65609s; TotalTimePerSample = 6.56091ms; SamplesPerSecond = 152
Finished Epoch[ 7 of 12]: [Training Set] TrainLossPerSample = 1.3624595; EvalErrPerSample = 0.17; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.656443
Starting Epoch 8: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 7 at record count 700, and file position 700
already there from last epoch

Starting minibatch loop.
randomordering: 22 retries for 100 elements (22.0%) to ensure window condition
randomordering: recached sequence for seed 7: 9, 19, ...
 Epoch[ 8 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  1.17384888; EvalErr[0]PerSample = 0.09000000; TotalTime = 0.65872s; TotalTimePerSample = 6.58715ms; SamplesPerSecond = 151
Finished Epoch[ 8 of 12]: [Training Set] TrainLossPerSample = 1.1738489; EvalErrPerSample = 0.089999996; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.659065
Starting Epoch 9: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 8 at record count 800, and file position 800
already there from last epoch

Starting minibatch loop.
randomordering: 16 retries for 100 elements (16.0%) to ensure window condition
randomordering: recached sequence for seed 8: 8, 5, ...
 Epoch[ 9 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  0.84733147; EvalErr[0]PerSample = 0.01000000; TotalTime = 0.66847s; TotalTimePerSample = 6.68467ms; SamplesPerSecond = 149
Finished Epoch[ 9 of 12]: [Training Set] TrainLossPerSample = 0.84733146; EvalErrPerSample = 0.0099999998; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.668841
Starting Epoch 10: learning rate per sample = 0.005000  effective momentum = 0.000000 
starting epoch 9 at record count 900, and file position 900
already there from last epoch

Starting minibatch loop.
randomordering: 16 retries for 100 elements (16.0%) to ensure window condition
randomordering: recached sequence for seed 9: 7, 10, ...
 Epoch[10 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  0.73448944; EvalErr[0]PerSample = 0.04000000; TotalTime = 0.67011s; TotalTimePerSample = 6.70111ms; SamplesPerSecond = 149
Finished Epoch[10 of 12]: [Training Set] TrainLossPerSample = 0.73448944; EvalErrPerSample = 0.039999999; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.670465
Starting Epoch 11: learning rate per sample = 0.005000  effective momentum = 0.700000 
starting epoch 10 at record count 1000, and file position 0
already there from last epoch

Starting minibatch loop.
randomordering: 22 retries for 100 elements (22.0%) to ensure window condition
randomordering: recached sequence for seed 10: 13, 22, ...
 Epoch[11 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  0.52700432; EvalErr[0]PerSample = 0.00000000; TotalTime = 0.65013s; TotalTimePerSample = 6.50128ms; SamplesPerSecond = 153
Finished Epoch[11 of 12]: [Training Set] TrainLossPerSample = 0.5270043; EvalErrPerSample = 0; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.650488
Starting Epoch 12: learning rate per sample = 0.005000  effective momentum = 0.700000 
starting epoch 11 at record count 1100, and file position 100
already there from last epoch

Starting minibatch loop.
randomordering: 21 retries for 100 elements (21.0%) to ensure window condition
randomordering: recached sequence for seed 11: 6, 31, ...
 Epoch[12 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  0.37077690; EvalErr[0]PerSample = 0.00000000; TotalTime = 0.65604s; TotalTimePerSample = 6.56038ms; SamplesPerSecond = 152
Finished Epoch[12 of 12]: [Training Set] TrainLossPerSample = 0.37077689; EvalErrPerSample = 0; AvgLearningRatePerSample = 0.004999999888; EpochTime=0.656382
CNTKCommandTrainEnd: Train


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

CE[0, 0] = CrossEntropyWithSoftmax(labels[10, 0], OutputNodes.z[0, 0])
OutputNodes.z[0, 0] = Plus(OutputNodes.t[0, 0], OutputNodes.b[10, 1])
OutputNodes.b[10, 1] = LearnableParameter
OutputNodes.t[0, 0] = Times(OutputNodes.W[10, 128], h1.y[0, 0])
h1.y[0, 0] = Sigmoid(h1.z[0, 0])
h1.z[0, 0] = Plus(h1.t[0, 0], h1.b[128, 1])
h1.b[128, 1] = LearnableParameter
h1.t[0, 0] = Times(h1.W[128, 512], pool2[0, 0])
pool2[0, 0] = AveragePooling(conv2_act.act[0, 0])
conv2_act.act[0, 0] = RectifiedLinear(conv2_act.convPlusB[0, 0])
conv2_act.convPlusB[0, 0] = Plus(conv2_act.conv[0, 0], conv2_act.convB[32, 1])
conv2_act.convB[32, 1] = LearnableParameter
conv2_act.conv[0, 0] = Convolution(conv2_act.convW[32, 400], pool1[0, 0])
pool1[0, 0] = MaxPooling(conv1_act.act[0, 0])
conv1_act.act[0, 0] = RectifiedLinear(conv1_act.convPlusB[0, 0])
conv1_act.convPlusB[0, 0] = Plus(conv1_act.conv[0, 0], conv1_act.convB[16, 1])
conv1_act.convB[16, 1] = LearnableParameter
conv1_act.conv[0, 0] = Convolution(conv1_act.convW[16, 25], featScaled[0, 0])
featScaled[0, 0] = Scale(featScale[1, 1], features[784, 0])
features[784, 0] = InputValue
featScale[1, 1] = LearnableParameter
conv1_act.convW[16, 25] = LearnableParameter
conv2_act.convW[32, 400] = LearnableParameter
h1.W[128, 512] = LearnableParameter
OutputNodes.W[10, 128] = LearnableParameter
labels[10, 0] = InputValue

Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE. 15 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.
starting epoch 0 at record count 0, and file position 0
already there from last epoch
randomordering: 11 retries for 100 elements (11.0%) to ensure window condition
randomordering: recached sequence for seed 0: 15, 33, ...
Final Results: Minibatch[1-1]: Samples Seen = 100    Err: ErrorPrediction/Sample = 0    CE: CrossEntropyWithSoftmax/Sample = 0.31800278    Perplexity = 1.3743801    
__COMPLETED__
=== Deleting last epoch data
==== Re-running from checkpoint
-------------------------------------------------------------------
Build info: 

        Built time: Nov 23 2015 10:00:15
        Last modified date: Mon Nov 23 09:45:21 2015
        Built by alexeyk on alexey-rz           
        Build Path: C:\src\cntk\MachineLearning\CNTK\
        CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
-------------------------------------------------------------------
running on alexey-rz at 2015/11/23 10:38:30
command line: 
C:\src\cntk\x64\Debug\CNTK.exe configFile=C:\src\cntk\Tests\Image\QuickE2E\cntk.config RunDir=C:\src\cntk\Tests\Image\_run DataDir=C:\src\cntk\Tests\Image\Data ConfigDir=C:\src\cntk\Tests\Image\QuickE2E DeviceId=-1 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision=float
command=Train:Test
deviceId=$DeviceId$
ndlMacros=$ConfigDir$/Macros.ndl
parallelTrain=false
Train=[
    action=train
    modelPath=$RunDir$/models/cntk.dnn
    deviceId=$DeviceId$
    traceLevel=1
        NDLNetworkBuilder=[
                networkDescription=$ConfigDir$/Convolution.ndl
        ]
    SGD=[
        epochSize=100
        minibatchSize=10
        learningRatesPerMB=0.05
        momentumPerMB=0*10:0.7
        maxEpochs=12
    ]
    reader=[
        readerType=UCIFastReader
        file=$DataDir$/Train.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=$DataDir$/labelsmap.txt
        ]
    ]    
]
Test=[
    action=test
    modelPath=$RunDir$/models/cntk.dnn
     NDLNetworkBuilder=[
        networkDescription=$ConfigDir$/Convolution.ndl
    ]
    reader=[
        readerType=UCIFastReader
        file=$DataDir$/Test.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=$DataDir$/labelsmap.txt
        ]
    ]    
]
RunDir=C:\src\cntk\Tests\Image\_run
DataDir=C:\src\cntk\Tests\Image\Data
ConfigDir=C:\src\cntk\Tests\Image\QuickE2E
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision=float
command=Train:Test
deviceId=-1
ndlMacros=C:\src\cntk\Tests\Image\QuickE2E/Macros.ndl
parallelTrain=false
Train=[
    action=train
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
    deviceId=-1
    traceLevel=1
        NDLNetworkBuilder=[
                networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
        ]
    SGD=[
        epochSize=100
        minibatchSize=10
        learningRatesPerMB=0.05
        momentumPerMB=0*10:0.7
        maxEpochs=12
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Train.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]
Test=[
    action=test
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
     NDLNetworkBuilder=[
        networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Test.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]
RunDir=C:\src\cntk\Tests\Image\_run
DataDir=C:\src\cntk\Tests\Image\Data
ConfigDir=C:\src\cntk\Tests\Image\QuickE2E
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.config:command=Train:Test
configparameters: cntk.config:ConfigDir=C:\src\cntk\Tests\Image\QuickE2E
configparameters: cntk.config:DataDir=C:\src\cntk\Tests\Image\Data
configparameters: cntk.config:deviceId=-1
configparameters: cntk.config:ndlMacros=C:\src\cntk\Tests\Image\QuickE2E/Macros.ndl
configparameters: cntk.config:parallelTrain=false
configparameters: cntk.config:precision=float
configparameters: cntk.config:RunDir=C:\src\cntk\Tests\Image\_run
configparameters: cntk.config:Test=[
    action=test
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
     NDLNetworkBuilder=[
        networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Test.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]

configparameters: cntk.config:Train=[
    action=train
    modelPath=C:\src\cntk\Tests\Image\_run/models/cntk.dnn
    deviceId=-1
    traceLevel=1
        NDLNetworkBuilder=[
                networkDescription=C:\src\cntk\Tests\Image\QuickE2E/Convolution.ndl
        ]
    SGD=[
        epochSize=100
        minibatchSize=10
        learningRatesPerMB=0.05
        momentumPerMB=0*10:0.7
        maxEpochs=12
    ]
    reader=[
        readerType=UCIFastReader
        file=C:\src\cntk\Tests\Image\Data/Train.txt
        features=[
            dim=784
            start=1
        ]
        labels=[
            dim=1
            start=0
            labelDim=10
            labelMappingFile=C:\src\cntk\Tests\Image\Data/labelsmap.txt
        ]
    ]    
]

<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: Train Test 
precision = float
CNTKModelPath: C:\src\cntk\Tests\Image\_run/models/cntk.dnn
CNTKCommandTrainInfo: Train : 12
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 12
CNTKCommandTrainBegin: Train
NDLBuilder Using CPU
reading uci file C:\src\cntk\Tests\Image\Data/Train.txt
Starting from checkpoint. Load Network From File C:\src\cntk\Tests\Image\_run/models/cntk.dnn.11.


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

CE[0, 0] = CrossEntropyWithSoftmax(labels[10, 0], OutputNodes.z[0, 0])
OutputNodes.z[0, 0] = Plus(OutputNodes.t[0, 0], OutputNodes.b[10, 1])
OutputNodes.b[10, 1] = LearnableParameter
OutputNodes.t[0, 0] = Times(OutputNodes.W[10, 128], h1.y[0, 0])
h1.y[0, 0] = Sigmoid(h1.z[0, 0])
h1.z[0, 0] = Plus(h1.t[0, 0], h1.b[128, 1])
h1.b[128, 1] = LearnableParameter
h1.t[0, 0] = Times(h1.W[128, 512], pool2[0, 0])
pool2[0, 0] = AveragePooling(conv2_act.act[0, 0])
conv2_act.act[0, 0] = RectifiedLinear(conv2_act.convPlusB[0, 0])
conv2_act.convPlusB[0, 0] = Plus(conv2_act.conv[0, 0], conv2_act.convB[32, 1])
conv2_act.convB[32, 1] = LearnableParameter
conv2_act.conv[0, 0] = Convolution(conv2_act.convW[32, 400], pool1[0, 0])
pool1[0, 0] = MaxPooling(conv1_act.act[0, 0])
conv1_act.act[0, 0] = RectifiedLinear(conv1_act.convPlusB[0, 0])
conv1_act.convPlusB[0, 0] = Plus(conv1_act.conv[0, 0], conv1_act.convB[16, 1])
conv1_act.convB[16, 1] = LearnableParameter
conv1_act.conv[0, 0] = Convolution(conv1_act.convW[16, 25], featScaled[0, 0])
featScaled[0, 0] = Scale(featScale[1, 1], features[784, 0])
features[784, 0] = InputValue
featScale[1, 1] = LearnableParameter
conv1_act.convW[16, 25] = LearnableParameter
conv2_act.convW[32, 400] = LearnableParameter
h1.W[128, 512] = LearnableParameter
OutputNodes.W[10, 128] = LearnableParameter
labels[10, 0] = InputValue

Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE. 15 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.

SGD using CPU.
GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...


Allocating matrices for gradient computing
No PreCompute nodes found, skipping PreCompute step
Warning: checkpoint file is missing. learning parameters will be initialized from 0
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 12: learning rate per sample = 0.005000  effective momentum = 0.700000 
starting at epoch 11 counting lines to determine record count

 1000 records found
starting epoch 11 at record count 1100, and file position 100
reading from record 0 to 100 to be positioned properly for epoch

Starting minibatch loop.
randomordering: 21 retries for 100 elements (21.0%) to ensure window condition
randomordering: recached sequence for seed 11: 6, 31, ...
 Epoch[12 of 12]-Minibatch[   1-  10 of 10]: SamplesSeen = 100; TrainLossPerSample =  0.37650299; EvalErr[0]PerSample = 0.00000000; TotalTime = 1.34518s; TotalTimePerSample = 13.45185ms; SamplesPerSecond = 74
Finished Epoch[12 of 12]: [Training Set] TrainLossPerSample = 0.37650299; EvalErrPerSample = 0; AvgLearningRatePerSample = 0.004999999888; EpochTime=1.371377
CNTKCommandTrainEnd: Train


Allocating matrices for forward propagation.


Printing Gradient Computation Node Order ... 

CE[0, 0] = CrossEntropyWithSoftmax(labels[10, 0], OutputNodes.z[0, 0])
OutputNodes.z[0, 0] = Plus(OutputNodes.t[0, 0], OutputNodes.b[10, 1])
OutputNodes.b[10, 1] = LearnableParameter
OutputNodes.t[0, 0] = Times(OutputNodes.W[10, 128], h1.y[0, 0])
h1.y[0, 0] = Sigmoid(h1.z[0, 0])
h1.z[0, 0] = Plus(h1.t[0, 0], h1.b[128, 1])
h1.b[128, 1] = LearnableParameter
h1.t[0, 0] = Times(h1.W[128, 512], pool2[0, 0])
pool2[0, 0] = AveragePooling(conv2_act.act[0, 0])
conv2_act.act[0, 0] = RectifiedLinear(conv2_act.convPlusB[0, 0])
conv2_act.convPlusB[0, 0] = Plus(conv2_act.conv[0, 0], conv2_act.convB[32, 1])
conv2_act.convB[32, 1] = LearnableParameter
conv2_act.conv[0, 0] = Convolution(conv2_act.convW[32, 400], pool1[0, 0])
pool1[0, 0] = MaxPooling(conv1_act.act[0, 0])
conv1_act.act[0, 0] = RectifiedLinear(conv1_act.convPlusB[0, 0])
conv1_act.convPlusB[0, 0] = Plus(conv1_act.conv[0, 0], conv1_act.convB[16, 1])
conv1_act.convB[16, 1] = LearnableParameter
conv1_act.conv[0, 0] = Convolution(conv1_act.convW[16, 25], featScaled[0, 0])
featScaled[0, 0] = Scale(featScale[1, 1], features[784, 0])
features[784, 0] = InputValue
featScale[1, 1] = LearnableParameter
conv1_act.convW[16, 25] = LearnableParameter
conv2_act.convW[32, 400] = LearnableParameter
h1.W[128, 512] = LearnableParameter
OutputNodes.W[10, 128] = LearnableParameter
labels[10, 0] = InputValue

Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE. 15 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node CE. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node CE, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> CE = CrossEntropyWithSoftmax(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node OutputNodes.z. 24 nodes to process in pass 1.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z. 13 nodes to process in pass 2.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

Validating for node OutputNodes.z, final verification.

Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]

9 out of 24 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 26 nodes to process in pass 1.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err. 14 nodes to process in pass 2.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [10, MBSize 0]
Validating --> OutputNodes.W = LearnableParameter -> [10, 128]
Validating --> h1.W = LearnableParameter -> [128, 512]
Validating --> conv2_act.convW = LearnableParameter -> [32, 400]
Validating --> conv1_act.convW = LearnableParameter -> [16, 25]
Validating --> featScale = LearnableParameter -> [1, 1]
Validating --> features = InputValue -> [784, MBSize 0]
Validating --> featScaled = Scale(featScale[1, 1], features[784 {W=28, H=28, C=1}, MBSize 0]) -> [784, MBSize 0]
Validating --> conv1_act.conv = Convolution(conv1_act.convW[16, 25], featScaled[784 {W=28, H=28, C=1}, MBSize 0]) -> [9216, MBSize 0]
Validating --> conv1_act.convB = LearnableParameter -> [16, 1]
Validating --> conv1_act.convPlusB = Plus(conv1_act.conv[9216 {W=24, H=24, C=16}, MBSize 0], conv1_act.convB[16, 1]) -> [9216, MBSize 0]
Validating --> conv1_act.act = RectifiedLinear(conv1_act.convPlusB[9216 {W=24, H=24, C=16}, MBSize 0]) -> [9216, MBSize 0]
Validating --> pool1 = MaxPooling(conv1_act.act[9216 {W=24, H=24, C=16}, MBSize 0]) -> [2304, MBSize 0]
Validating --> conv2_act.conv = Convolution(conv2_act.convW[32, 400], pool1[2304 {W=12, H=12, C=16}, MBSize 0]) -> [2048, MBSize 0]
Validating --> conv2_act.convB = LearnableParameter -> [32, 1]
Validating --> conv2_act.convPlusB = Plus(conv2_act.conv[2048 {W=8, H=8, C=32}, MBSize 0], conv2_act.convB[32, 1]) -> [2048, MBSize 0]
Validating --> conv2_act.act = RectifiedLinear(conv2_act.convPlusB[2048 {W=8, H=8, C=32}, MBSize 0]) -> [2048, MBSize 0]
Validating --> pool2 = AveragePooling(conv2_act.act[2048 {W=8, H=8, C=32}, MBSize 0]) -> [512, MBSize 0]
Validating --> h1.t = Times(h1.W[128, 512], pool2[512 {W=4, H=4, C=32}, MBSize 0]) -> [128, MBSize 0]
Validating --> h1.b = LearnableParameter -> [128, 1]
Validating --> h1.z = Plus(h1.t[128, MBSize 0], h1.b[128, 1]) -> [128, MBSize 0]
Validating --> h1.y = Sigmoid(h1.z[128, MBSize 0]) -> [128, MBSize 0]
Validating --> OutputNodes.t = Times(OutputNodes.W[10, 128], h1.y[128, MBSize 0]) -> [10, MBSize 0]
Validating --> OutputNodes.b = LearnableParameter -> [10, 1]
Validating --> OutputNodes.z = Plus(OutputNodes.t[10, MBSize 0], OutputNodes.b[10, 1]) -> [10, MBSize 0]
Validating --> Err = ErrorPrediction(labels[10, MBSize 0], OutputNodes.z[10, MBSize 0]) -> [1, 1]

10 out of 26 nodes do not share the minibatch layout with the input data.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.
starting epoch 0 at record count 0, and file position 0
already there from last epoch
randomordering: 11 retries for 100 elements (11.0%) to ensure window condition
randomordering: recached sequence for seed 0: 15, 33, ...
Final Results: Minibatch[1-1]: Samples Seen = 100    Err: ErrorPrediction/Sample = 0    CE: CrossEntropyWithSoftmax/Sample = 0.33039909    Perplexity = 1.3915234    
__COMPLETED__

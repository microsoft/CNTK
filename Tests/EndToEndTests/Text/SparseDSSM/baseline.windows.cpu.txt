=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 05/03/2016 14:29:58: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 05/03/2016 14:29:58: -------------------------------------------------------------------
MPI Rank 0: 05/03/2016 14:29:58: Build info: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: 		Built time: May  3 2016 13:23:06
MPI Rank 0: 05/03/2016 14:29:58: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 0: 05/03/2016 14:29:58: 		Build type: Release
MPI Rank 0: 05/03/2016 14:29:58: 		Build target: GPU
MPI Rank 0: 05/03/2016 14:29:58: 		With 1bit-SGD: no
MPI Rank 0: 05/03/2016 14:29:58: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 05/03/2016 14:29:58: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 05/03/2016 14:29:58: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 05/03/2016 14:29:58: 		Build Branch: HEAD
MPI Rank 0: 05/03/2016 14:29:58: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 0: 05/03/2016 14:29:58: 		Built by svcphil on LIANA-09-w
MPI Rank 0: 05/03/2016 14:29:58: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 05/03/2016 14:29:58: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: Running on cntk-muc02 at 2016/05/03 14:29:58
MPI Rank 0: 05/03/2016 14:29:58: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 14:29:58: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 14:29:58: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 05/03/2016 14:29:58: Commands: train
MPI Rank 0: 05/03/2016 14:29:58: Precision = "float"
MPI Rank 0: 05/03/2016 14:29:58: Using 1 CPU threads.
MPI Rank 0: 05/03/2016 14:29:58: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 05/03/2016 14:29:58: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 05/03/2016 14:29:58: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: ##############################################################################
MPI Rank 0: 05/03/2016 14:29:58: #                                                                            #
MPI Rank 0: 05/03/2016 14:29:58: # Action "train"                                                             #
MPI Rank 0: 05/03/2016 14:29:58: #                                                                            #
MPI Rank 0: 05/03/2016 14:29:58: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: Creating virgin network.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: Created model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:29:58: Training criterion node(s):
MPI Rank 0: 05/03/2016 14:29:58: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 0000007F9762B3B0: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0000007F9762B450: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0000007F9762B6D0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0000007F9762B810: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0000007F9762B9F0: {[Query Value[49292 x *]] }
MPI Rank 0: 0000007F976D1470: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 0000007F976D1650: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 0000007F976D16F0: {[Keyword Value[49292 x *]] }
MPI Rank 0: 0000007F976D1790: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 0000007F976D1830: {[SIM Gradient[51 x *]] }
MPI Rank 0: 0000007F976D1970: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 0000007F976D1A10: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0000007F976D1BF0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 0000007F976D1C90: {[ce Gradient[1]] }
MPI Rank 0: 0000007F976D2050: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 0000007F976D2190: {[S Value[1 x 1]] }
MPI Rank 0: 0000007F976D22D0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 0000007F976D2370: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 0000007F976D24B0: {[N Value[1 x 1]] }
MPI Rank 0: 0000007F976D2910: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 0000007F976D2B90: {[SIM Value[51 x *]] }
MPI Rank 0: 0000007F976D2CD0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 0000007F976D2D70: {[G Value[1 x 1]] }
MPI Rank 0: 0000007F976D2F50: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 0000007F976D3130: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 0000007F976D31D0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 0000007F976D3270: {[ce Value[1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 05/03/2016 14:29:58: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:30:01: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:30:01: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 14:30:24:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.41944122 * 10240; time = 22.4162s; samplesPerSecond = 456.8
MPI Rank 0: 05/03/2016 14:30:46:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38406754 * 10240; time = 21.8090s; samplesPerSecond = 469.5
MPI Rank 0: 05/03/2016 14:30:57: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=55.2174s
MPI Rank 0: 05/03/2016 14:31:02: Final Results: Minibatch[1-26]: ce = 2.50976878 * 102399; perplexity = 12.30208521
MPI Rank 0: 05/03/2016 14:31:02: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976878 * 102399
MPI Rank 0: 05/03/2016 14:31:04: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:31:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:31:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 14:31:27:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29922523 * 10240; time = 22.1458s; samplesPerSecond = 462.4
MPI Rank 0: 05/03/2016 14:31:49:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.08742371 * 10240; time = 21.8470s; samplesPerSecond = 468.7
MPI Rank 0: 05/03/2016 14:32:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057668 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=55.0878s
MPI Rank 0: 05/03/2016 14:32:06: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 0: 05/03/2016 14:32:06: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 0: 05/03/2016 14:32:08: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:32:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:32:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 14:32:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90042439 * 10240; time = 22.2209s; samplesPerSecond = 460.8
MPI Rank 0: 05/03/2016 14:32:54:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.85719700 * 10240; time = 22.3642s; samplesPerSecond = 457.9
MPI Rank 0: 05/03/2016 14:33:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=55.544s
MPI Rank 0: 05/03/2016 14:33:10: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313971
MPI Rank 0: 05/03/2016 14:33:10: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 0: 05/03/2016 14:33:12: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 05/03/2016 14:33:13: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:13: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:13: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 05/03/2016 14:29:58: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 05/03/2016 14:29:58: -------------------------------------------------------------------
MPI Rank 1: 05/03/2016 14:29:58: Build info: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: 		Built time: May  3 2016 13:23:06
MPI Rank 1: 05/03/2016 14:29:58: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 1: 05/03/2016 14:29:58: 		Build type: Release
MPI Rank 1: 05/03/2016 14:29:58: 		Build target: GPU
MPI Rank 1: 05/03/2016 14:29:58: 		With 1bit-SGD: no
MPI Rank 1: 05/03/2016 14:29:58: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 05/03/2016 14:29:58: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 05/03/2016 14:29:58: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 05/03/2016 14:29:58: 		Build Branch: HEAD
MPI Rank 1: 05/03/2016 14:29:58: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 1: 05/03/2016 14:29:58: 		Built by svcphil on LIANA-09-w
MPI Rank 1: 05/03/2016 14:29:58: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 05/03/2016 14:29:58: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: Running on cntk-muc02 at 2016/05/03 14:29:58
MPI Rank 1: 05/03/2016 14:29:58: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 14:29:58: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 14:29:58: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 05/03/2016 14:29:58: Commands: train
MPI Rank 1: 05/03/2016 14:29:58: Precision = "float"
MPI Rank 1: 05/03/2016 14:29:58: Using 1 CPU threads.
MPI Rank 1: 05/03/2016 14:29:58: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 05/03/2016 14:29:58: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 05/03/2016 14:29:58: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: ##############################################################################
MPI Rank 1: 05/03/2016 14:29:58: #                                                                            #
MPI Rank 1: 05/03/2016 14:29:58: # Action "train"                                                             #
MPI Rank 1: 05/03/2016 14:29:58: #                                                                            #
MPI Rank 1: 05/03/2016 14:29:58: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:58: Creating virgin network.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:59: Created model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:29:59: Training criterion node(s):
MPI Rank 1: 05/03/2016 14:29:59: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 0000008F4FA0B900: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0000008F4FA0BA40: {[Query Value[49292 x *]] }
MPI Rank 1: 0000008F4FA0C080: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0000008F4FA0C300: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0000008F4FA0C800: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0000008F4FACE550: {[S Value[1 x 1]] }
MPI Rank 1: 0000008F4FACE5F0: {[ce Value[1]] }
MPI Rank 1: 0000008F4FACE690: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 0000008F4FACE7D0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 0000008F4FACE9B0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 0000008F4FACEA50: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 0000008F4FACEAF0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 0000008F4FACEB90: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 0000008F4FACECD0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 0000008F4FACED70: {[Keyword Value[49292 x *]] }
MPI Rank 1: 0000008F4FACEF50: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 0000008F4FACF090: {[G Value[1 x 1]] }
MPI Rank 1: 0000008F4FACF130: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 0000008F4FACF450: {[N Value[1 x 1]] }
MPI Rank 1: 0000008F4FACF630: {[ce Gradient[1]] }
MPI Rank 1: 0000008F4FACF770: {[SIM Gradient[51 x *]] }
MPI Rank 1: 0000008F4FACF8B0: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 0000008F4FACF9F0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0000008F4FACFD10: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 0000008F4FACFEF0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 0000008F4FAD0210: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0000008F4FAD02B0: {[SIM Value[51 x *]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 05/03/2016 14:29:59: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:30:01: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:30:01: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 14:30:24:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.42829742 * 10240; time = 22.3646s; samplesPerSecond = 457.9
MPI Rank 1: 05/03/2016 14:30:46:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38771057 * 10240; time = 21.8103s; samplesPerSecond = 469.5
MPI Rank 1: 05/03/2016 14:30:56: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=55.0875s
MPI Rank 1: 05/03/2016 14:31:02: Final Results: Minibatch[1-26]: ce = 2.50976878 * 102399; perplexity = 12.30208521
MPI Rank 1: 05/03/2016 14:31:02: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976878 * 102399
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:31:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:31:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 14:31:27:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34065857 * 10240; time = 22.2137s; samplesPerSecond = 461.0
MPI Rank 1: 05/03/2016 14:31:49:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11009483 * 10240; time = 21.8462s; samplesPerSecond = 468.7
MPI Rank 1: 05/03/2016 14:32:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057668 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=55.2004s
MPI Rank 1: 05/03/2016 14:32:06: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 1: 05/03/2016 14:32:06: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:32:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:32:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 14:32:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93978729 * 10240; time = 22.1765s; samplesPerSecond = 461.8
MPI Rank 1: 05/03/2016 14:32:54:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86772957 * 10240; time = 22.3656s; samplesPerSecond = 457.8
MPI Rank 1: 05/03/2016 14:33:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=55.5988s
MPI Rank 1: 05/03/2016 14:33:10: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313971
MPI Rank 1: 05/03/2016 14:33:10: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 1: 05/03/2016 14:33:13: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:13: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:13: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 05/03/2016 14:29:59: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 05/03/2016 14:29:59: -------------------------------------------------------------------
MPI Rank 2: 05/03/2016 14:29:59: Build info: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: 		Built time: May  3 2016 13:23:06
MPI Rank 2: 05/03/2016 14:29:59: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 2: 05/03/2016 14:29:59: 		Build type: Release
MPI Rank 2: 05/03/2016 14:29:59: 		Build target: GPU
MPI Rank 2: 05/03/2016 14:29:59: 		With 1bit-SGD: no
MPI Rank 2: 05/03/2016 14:29:59: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 05/03/2016 14:29:59: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 05/03/2016 14:29:59: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 05/03/2016 14:29:59: 		Build Branch: HEAD
MPI Rank 2: 05/03/2016 14:29:59: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 2: 05/03/2016 14:29:59: 		Built by svcphil on LIANA-09-w
MPI Rank 2: 05/03/2016 14:29:59: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 05/03/2016 14:29:59: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: Running on cntk-muc02 at 2016/05/03 14:29:59
MPI Rank 2: 05/03/2016 14:29:59: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 14:29:59: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 14:29:59: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 05/03/2016 14:29:59: Commands: train
MPI Rank 2: 05/03/2016 14:29:59: Precision = "float"
MPI Rank 2: 05/03/2016 14:29:59: Using 1 CPU threads.
MPI Rank 2: 05/03/2016 14:29:59: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 05/03/2016 14:29:59: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 05/03/2016 14:29:59: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: ##############################################################################
MPI Rank 2: 05/03/2016 14:29:59: #                                                                            #
MPI Rank 2: 05/03/2016 14:29:59: # Action "train"                                                             #
MPI Rank 2: 05/03/2016 14:29:59: #                                                                            #
MPI Rank 2: 05/03/2016 14:29:59: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: Creating virgin network.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: Created model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:29:59: Training criterion node(s):
MPI Rank 2: 05/03/2016 14:29:59: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 0000000D7B60BCC0: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0000000D7B60BEA0: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0000000D7B60BF40: {[Query Value[49292 x *]] }
MPI Rank 2: 0000000D7B60BFE0: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0000000D7B60C580: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0000000D7B6CE730: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 0000000D7B6CE7D0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 0000000D7B6CE870: {[SIM Gradient[51 x *]] }
MPI Rank 2: 0000000D7B6CEB90: {[S Value[1 x 1]] }
MPI Rank 2: 0000000D7B6CEC30: {[N Value[1 x 1]] }
MPI Rank 2: 0000000D7B6CED70: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0000000D7B6CEFF0: {[Keyword Value[49292 x *]] }
MPI Rank 2: 0000000D7B6CF1D0: {[G Value[1 x 1]] }
MPI Rank 2: 0000000D7B6CF310: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 0000000D7B6CF4F0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 0000000D7B6CF590: {[SIM Value[51 x *]] }
MPI Rank 2: 0000000D7B6CF770: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 0000000D7B6CF810: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 0000000D7B6CF8B0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 0000000D7B6CF9F0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 0000000D7B6CFD10: {[ce Gradient[1]] }
MPI Rank 2: 0000000D7B6CFDB0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 0000000D7B6CFEF0: {[ce Value[1]] }
MPI Rank 2: 0000000D7B6CFF90: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 0000000D7B6D0210: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 0000000D7B6D02B0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 0000000D7B6D0350: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 05/03/2016 14:29:59: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:30:01: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:30:01: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 14:30:24:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.44101410 * 10240; time = 22.2983s; samplesPerSecond = 459.2
MPI Rank 2: 05/03/2016 14:30:46:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.39723740 * 10240; time = 21.8109s; samplesPerSecond = 469.5
MPI Rank 2: 05/03/2016 14:30:56: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=55.0254s
MPI Rank 2: 05/03/2016 14:31:02: Final Results: Minibatch[1-26]: ce = 2.50976878 * 102399; perplexity = 12.30208521
MPI Rank 2: 05/03/2016 14:31:02: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976878 * 102399
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:31:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:31:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 14:31:28:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34435501 * 10240; time = 22.2583s; samplesPerSecond = 460.1
MPI Rank 2: 05/03/2016 14:31:49:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.13005295 * 10240; time = 21.8455s; samplesPerSecond = 468.7
MPI Rank 2: 05/03/2016 14:32:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057668 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=55.0854s
MPI Rank 2: 05/03/2016 14:32:06: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 2: 05/03/2016 14:32:06: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:32:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:32:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 14:32:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95727177 * 10240; time = 22.1066s; samplesPerSecond = 463.2
MPI Rank 2: 05/03/2016 14:32:53:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88702602 * 10240; time = 22.3676s; samplesPerSecond = 457.8
MPI Rank 2: 05/03/2016 14:33:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=55.5415s
MPI Rank 2: 05/03/2016 14:33:10: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313971
MPI Rank 2: 05/03/2016 14:33:10: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 2: 05/03/2016 14:33:13: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:13: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:13: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 05/03/2016 14:29:59: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 05/03/2016 14:29:59: -------------------------------------------------------------------
MPI Rank 3: 05/03/2016 14:29:59: Build info: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: 		Built time: May  3 2016 13:23:06
MPI Rank 3: 05/03/2016 14:29:59: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 3: 05/03/2016 14:29:59: 		Build type: Release
MPI Rank 3: 05/03/2016 14:29:59: 		Build target: GPU
MPI Rank 3: 05/03/2016 14:29:59: 		With 1bit-SGD: no
MPI Rank 3: 05/03/2016 14:29:59: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 05/03/2016 14:29:59: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 05/03/2016 14:29:59: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 05/03/2016 14:29:59: 		Build Branch: HEAD
MPI Rank 3: 05/03/2016 14:29:59: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 3: 05/03/2016 14:29:59: 		Built by svcphil on LIANA-09-w
MPI Rank 3: 05/03/2016 14:29:59: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 05/03/2016 14:29:59: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: Running on cntk-muc02 at 2016/05/03 14:29:59
MPI Rank 3: 05/03/2016 14:29:59: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 14:29:59: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 14:29:59: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 05/03/2016 14:29:59: Commands: train
MPI Rank 3: 05/03/2016 14:29:59: Precision = "float"
MPI Rank 3: 05/03/2016 14:29:59: Using 1 CPU threads.
MPI Rank 3: 05/03/2016 14:29:59: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 05/03/2016 14:29:59: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 05/03/2016 14:29:59: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: ##############################################################################
MPI Rank 3: 05/03/2016 14:29:59: #                                                                            #
MPI Rank 3: 05/03/2016 14:29:59: # Action "train"                                                             #
MPI Rank 3: 05/03/2016 14:29:59: #                                                                            #
MPI Rank 3: 05/03/2016 14:29:59: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:29:59: Creating virgin network.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:30:00: Created model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:30:00: Training criterion node(s):
MPI Rank 3: 05/03/2016 14:30:00: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000473B9D9FA0: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000473B9DA540: {[Query Value[49292 x *]] }
MPI Rank 3: 000000473B9DA860: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000473B9DAA40: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000473B9DAB80: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000473B9E89B0: {[ce Gradient[1]] }
MPI Rank 3: 000000473B9E8AF0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 000000473B9E8CD0: {[SIM Gradient[51 x *]] }
MPI Rank 3: 000000473B9E8FF0: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 000000473B9E9090: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 000000473B9E9130: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000473B9E91D0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 000000473B9E9270: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 000000473B9E9310: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000473B9E96D0: {[G Value[1 x 1]] }
MPI Rank 3: 000000473B9E9770: {[S Value[1 x 1]] }
MPI Rank 3: 000000473B9E9810: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 000000473B9E98B0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 000000473B9E9950: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 000000473B9E99F0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000473B9E9B30: {[Keyword Value[49292 x *]] }
MPI Rank 3: 000000473B9E9D10: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 000000473B9E9E50: {[N Value[1 x 1]] }
MPI Rank 3: 000000473B9E9F90: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 000000473B9EA170: {[SIM Value[51 x *]] }
MPI Rank 3: 000000473B9EA490: {[ce Value[1]] }
MPI Rank 3: 000000473B9EA5D0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 05/03/2016 14:30:00: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:30:01: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:30:01: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 14:30:24:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.43899651 * 10240; time = 22.2550s; samplesPerSecond = 460.1
MPI Rank 3: 05/03/2016 14:30:45:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38633919 * 10240; time = 21.8077s; samplesPerSecond = 469.6
MPI Rank 3: 05/03/2016 14:30:57: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=55.3443s
MPI Rank 3: 05/03/2016 14:31:02: Final Results: Minibatch[1-26]: ce = 2.50976878 * 102399; perplexity = 12.30208521
MPI Rank 3: 05/03/2016 14:31:02: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976878 * 102399
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:31:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:31:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 14:31:28:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29999485 * 10240; time = 22.3262s; samplesPerSecond = 458.7
MPI Rank 3: 05/03/2016 14:31:49:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11324863 * 10240; time = 21.8443s; samplesPerSecond = 468.8
MPI Rank 3: 05/03/2016 14:32:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057668 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=55.0833s
MPI Rank 3: 05/03/2016 14:32:06: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 3: 05/03/2016 14:32:06: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:32:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:32:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 14:32:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90144348 * 10240; time = 22.0637s; samplesPerSecond = 464.1
MPI Rank 3: 05/03/2016 14:32:53:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87299423 * 10240; time = 22.3678s; samplesPerSecond = 457.8
MPI Rank 3: 05/03/2016 14:33:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=55.5398s
MPI Rank 3: 05/03/2016 14:33:10: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313971
MPI Rank 3: 05/03/2016 14:33:10: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 3: 05/03/2016 14:33:13: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:13: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:13: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 13:23:06
		Last modified date: Mon Apr 18 00:00:12 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 05/03/2016 14:33:16: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 05/03/2016 14:33:16: -------------------------------------------------------------------
MPI Rank 0: 05/03/2016 14:33:16: Build info: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: 		Built time: May  3 2016 13:23:06
MPI Rank 0: 05/03/2016 14:33:16: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 0: 05/03/2016 14:33:16: 		Build type: Release
MPI Rank 0: 05/03/2016 14:33:16: 		Build target: GPU
MPI Rank 0: 05/03/2016 14:33:16: 		With 1bit-SGD: no
MPI Rank 0: 05/03/2016 14:33:16: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 05/03/2016 14:33:16: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 05/03/2016 14:33:16: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 05/03/2016 14:33:16: 		Build Branch: HEAD
MPI Rank 0: 05/03/2016 14:33:16: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 0: 05/03/2016 14:33:16: 		Built by svcphil on LIANA-09-w
MPI Rank 0: 05/03/2016 14:33:16: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 05/03/2016 14:33:16: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: Running on cntk-muc02 at 2016/05/03 14:33:16
MPI Rank 0: 05/03/2016 14:33:16: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 14:33:16: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 14:33:16: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 05/03/2016 14:33:16: Commands: train
MPI Rank 0: 05/03/2016 14:33:16: Precision = "float"
MPI Rank 0: 05/03/2016 14:33:16: Using 1 CPU threads.
MPI Rank 0: 05/03/2016 14:33:16: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 05/03/2016 14:33:16: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 05/03/2016 14:33:16: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: ##############################################################################
MPI Rank 0: 05/03/2016 14:33:16: #                                                                            #
MPI Rank 0: 05/03/2016 14:33:16: # Action "train"                                                             #
MPI Rank 0: 05/03/2016 14:33:16: #                                                                            #
MPI Rank 0: 05/03/2016 14:33:16: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:16: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:17: Loaded model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:17: Training criterion node(s):
MPI Rank 0: 05/03/2016 14:33:17: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 000000DC60B7B260: {[WD1 Value[64 x 288]] }
MPI Rank 0: 000000DC60B7B620: {[Query Value[49292 x *1]] }
MPI Rank 0: 000000DC60B7B760: {[G Value[1 x 1]] }
MPI Rank 0: 000000DC60B7B8A0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 000000DC60B7BC60: {[S Value[1 x 1]] }
MPI Rank 0: 000000DC60B7BD00: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 000000DC60B7BEE0: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 000000DC60B7C0C0: {[N Value[1 x 1]] }
MPI Rank 0: 000000DC60BADE90: {[ce Gradient[1]] }
MPI Rank 0: 000000DC60BAE070: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 000000DC60BAE110: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 000000DC60BAE1B0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 000000DC60BAE2F0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 000000DC60BAE610: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 000000DC60BAE890: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 000000DC60BAEA70: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 000000DC60BAEBB0: {[SIM Value[51 x *1]] }
MPI Rank 0: 000000DC60BAEC50: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 000000DC60BAECF0: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 000000DC60BAED90: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 000000DC60BAF1F0: {[ce Value[1]] }
MPI Rank 0: 000000DC60BAF290: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 000000DC60BAF330: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 000000DC60BAF470: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 000000DC60BAF6F0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 000000DC60BAF8D0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 000000DC60BAF970: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 05/03/2016 14:33:18: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:21: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:33:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 14:33:43:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87912197 * 10240; time = 22.6905s; samplesPerSecond = 451.3
MPI Rank 0: 05/03/2016 14:34:06:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79176922 * 10240; time = 22.0090s; samplesPerSecond = 465.3
MPI Rank 0: 05/03/2016 14:34:17: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=55.8582s
MPI Rank 0: 05/03/2016 14:34:22: Final Results: Minibatch[1-26]: ce = 1.82106098 * 102399; perplexity = 6.17841016
MPI Rank 0: 05/03/2016 14:34:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106098 * 102399
MPI Rank 0: 05/03/2016 14:34:24: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 05/03/2016 14:34:25: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:34:25: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 14:34:25: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 05/03/2016 14:33:16: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 05/03/2016 14:33:16: -------------------------------------------------------------------
MPI Rank 1: 05/03/2016 14:33:16: Build info: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: 		Built time: May  3 2016 13:23:06
MPI Rank 1: 05/03/2016 14:33:16: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 1: 05/03/2016 14:33:16: 		Build type: Release
MPI Rank 1: 05/03/2016 14:33:16: 		Build target: GPU
MPI Rank 1: 05/03/2016 14:33:16: 		With 1bit-SGD: no
MPI Rank 1: 05/03/2016 14:33:16: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 05/03/2016 14:33:16: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 05/03/2016 14:33:16: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 05/03/2016 14:33:16: 		Build Branch: HEAD
MPI Rank 1: 05/03/2016 14:33:16: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 1: 05/03/2016 14:33:16: 		Built by svcphil on LIANA-09-w
MPI Rank 1: 05/03/2016 14:33:16: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 05/03/2016 14:33:16: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: Running on cntk-muc02 at 2016/05/03 14:33:16
MPI Rank 1: 05/03/2016 14:33:16: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 14:33:16: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 14:33:16: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 05/03/2016 14:33:16: Commands: train
MPI Rank 1: 05/03/2016 14:33:16: Precision = "float"
MPI Rank 1: 05/03/2016 14:33:16: Using 1 CPU threads.
MPI Rank 1: 05/03/2016 14:33:16: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 05/03/2016 14:33:16: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 05/03/2016 14:33:16: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: ##############################################################################
MPI Rank 1: 05/03/2016 14:33:16: #                                                                            #
MPI Rank 1: 05/03/2016 14:33:16: # Action "train"                                                             #
MPI Rank 1: 05/03/2016 14:33:16: #                                                                            #
MPI Rank 1: 05/03/2016 14:33:16: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:16: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:18: Loaded model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:18: Training criterion node(s):
MPI Rank 1: 05/03/2016 14:33:18: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 0000001431C48B70: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 0000001431C48CB0: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0000001431C48D50: {[S Value[1 x 1]] }
MPI Rank 1: 0000001431C48DF0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 0000001431C48F30: {[Query Value[49292 x *1]] }
MPI Rank 1: 0000001431C492F0: {[G Value[1 x 1]] }
MPI Rank 1: 0000001431C49570: {[N Value[1 x 1]] }
MPI Rank 1: 0000001431C49610: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0000001431C7AC80: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 0000001431C7AD20: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0000001431C7AE60: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 0000001431C7B720: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0000001431C7B7C0: {[ce Value[1]] }
MPI Rank 1: 0000001431C7B900: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 0000001431C7B9A0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0000001431C7BE00: {[SIM Value[51 x *1]] }
MPI Rank 1: 0000001431C7C260: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 0000001431C7C300: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 0000001431C7C3A0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 0000001431C7C440: {[ce Gradient[1]] }
MPI Rank 1: 0000001431C7C580: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 0000001431C7C620: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0000001431C7C6C0: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 0000001431C7C760: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 0000001431C7C8A0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 0000001431C7C9E0: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0000001431C7CA80: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 05/03/2016 14:33:18: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:21: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:33:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 14:33:43:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.94872780 * 10240; time = 22.6509s; samplesPerSecond = 452.1
MPI Rank 1: 05/03/2016 14:34:05:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89873848 * 10240; time = 22.0067s; samplesPerSecond = 465.3
MPI Rank 1: 05/03/2016 14:34:17: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=55.9942s
MPI Rank 1: 05/03/2016 14:34:22: Final Results: Minibatch[1-26]: ce = 1.82106098 * 102399; perplexity = 6.17841016
MPI Rank 1: 05/03/2016 14:34:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106098 * 102399
MPI Rank 1: 05/03/2016 14:34:25: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:34:25: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 14:34:25: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 05/03/2016 14:33:17: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 05/03/2016 14:33:17: -------------------------------------------------------------------
MPI Rank 2: 05/03/2016 14:33:17: Build info: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: 		Built time: May  3 2016 13:23:06
MPI Rank 2: 05/03/2016 14:33:17: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 2: 05/03/2016 14:33:17: 		Build type: Release
MPI Rank 2: 05/03/2016 14:33:17: 		Build target: GPU
MPI Rank 2: 05/03/2016 14:33:17: 		With 1bit-SGD: no
MPI Rank 2: 05/03/2016 14:33:17: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 05/03/2016 14:33:17: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 05/03/2016 14:33:17: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 05/03/2016 14:33:17: 		Build Branch: HEAD
MPI Rank 2: 05/03/2016 14:33:17: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 2: 05/03/2016 14:33:17: 		Built by svcphil on LIANA-09-w
MPI Rank 2: 05/03/2016 14:33:17: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 05/03/2016 14:33:17: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: Running on cntk-muc02 at 2016/05/03 14:33:17
MPI Rank 2: 05/03/2016 14:33:17: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 14:33:17: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 14:33:17: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 05/03/2016 14:33:17: Commands: train
MPI Rank 2: 05/03/2016 14:33:17: Precision = "float"
MPI Rank 2: 05/03/2016 14:33:17: Using 1 CPU threads.
MPI Rank 2: 05/03/2016 14:33:17: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 05/03/2016 14:33:17: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 05/03/2016 14:33:17: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: ##############################################################################
MPI Rank 2: 05/03/2016 14:33:17: #                                                                            #
MPI Rank 2: 05/03/2016 14:33:17: # Action "train"                                                             #
MPI Rank 2: 05/03/2016 14:33:17: #                                                                            #
MPI Rank 2: 05/03/2016 14:33:17: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:17: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:18: Loaded model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:18: Training criterion node(s):
MPI Rank 2: 05/03/2016 14:33:18: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000C57E0376B0: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000C57E037930: {[S Value[1 x 1]] }
MPI Rank 2: 000000C57E037CF0: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 000000C57E037D90: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 000000C57E038150: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 000000C57E038290: {[G Value[1 x 1]] }
MPI Rank 2: 000000C57E038330: {[Query Value[49292 x *1]] }
MPI Rank 2: 000000C57E038510: {[N Value[1 x 1]] }
MPI Rank 2: 000000C57E069C20: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 000000C57E069CC0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 000000C57E069EA0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 000000C57E069F40: {[ce Value[1]] }
MPI Rank 2: 000000C57E069FE0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 000000C57E06A260: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 000000C57E06A620: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000C57E06A9E0: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000C57E06AA80: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 000000C57E06AB20: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 000000C57E06AC60: {[ce Gradient[1]] }
MPI Rank 2: 000000C57E06AEE0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000C57E06B0C0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 000000C57E06B160: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000C57E06B2A0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 000000C57E06B5C0: {[SIM Value[51 x *1]] }
MPI Rank 2: 000000C57E06B700: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 000000C57E06B8E0: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 000000C57E06BAC0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 05/03/2016 14:33:19: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:21: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:33:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 14:33:44:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96648235 * 10240; time = 22.8600s; samplesPerSecond = 447.9
MPI Rank 2: 05/03/2016 14:34:06:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91512318 * 10240; time = 22.0091s; samplesPerSecond = 465.3
MPI Rank 2: 05/03/2016 14:34:17: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=56.1135s
MPI Rank 2: 05/03/2016 14:34:22: Final Results: Minibatch[1-26]: ce = 1.82106098 * 102399; perplexity = 6.17841016
MPI Rank 2: 05/03/2016 14:34:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106098 * 102399
MPI Rank 2: 05/03/2016 14:34:25: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:34:25: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 14:34:25: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 05/03/2016 14:33:17: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 05/03/2016 14:33:17: -------------------------------------------------------------------
MPI Rank 3: 05/03/2016 14:33:17: Build info: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: 		Built time: May  3 2016 13:23:06
MPI Rank 3: 05/03/2016 14:33:17: 		Last modified date: Mon Apr 18 00:00:12 2016
MPI Rank 3: 05/03/2016 14:33:17: 		Build type: Release
MPI Rank 3: 05/03/2016 14:33:17: 		Build target: GPU
MPI Rank 3: 05/03/2016 14:33:17: 		With 1bit-SGD: no
MPI Rank 3: 05/03/2016 14:33:17: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 05/03/2016 14:33:17: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 05/03/2016 14:33:17: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 05/03/2016 14:33:17: 		Build Branch: HEAD
MPI Rank 3: 05/03/2016 14:33:17: 		Build SHA1: af96f7cce6c3c78a4f1e9315e061291c79360e12
MPI Rank 3: 05/03/2016 14:33:17: 		Built by svcphil on LIANA-09-w
MPI Rank 3: 05/03/2016 14:33:17: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 05/03/2016 14:33:17: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: Running on cntk-muc02 at 2016/05/03 14:33:17
MPI Rank 3: 05/03/2016 14:33:17: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 14:33:17: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 14:33:17: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 05/03/2016 14:33:17: Commands: train
MPI Rank 3: 05/03/2016 14:33:17: Precision = "float"
MPI Rank 3: 05/03/2016 14:33:17: Using 1 CPU threads.
MPI Rank 3: 05/03/2016 14:33:17: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 05/03/2016 14:33:17: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 05/03/2016 14:33:17: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: ##############################################################################
MPI Rank 3: 05/03/2016 14:33:17: #                                                                            #
MPI Rank 3: 05/03/2016 14:33:17: # Action "train"                                                             #
MPI Rank 3: 05/03/2016 14:33:17: #                                                                            #
MPI Rank 3: 05/03/2016 14:33:17: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:17: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160503142201.423154\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:19: Loaded model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:19: Training criterion node(s):
MPI Rank 3: 05/03/2016 14:33:19: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000AECC88B900: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000AECC88BC20: {[N Value[1 x 1]] }
MPI Rank 3: 000000AECC88BCC0: {[Query Value[49292 x *1]] }
MPI Rank 3: 000000AECC88BD60: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000AECC88C120: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 000000AECC88C1C0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 000000AECC88C260: {[S Value[1 x 1]] }
MPI Rank 3: 000000AECC88C300: {[G Value[1 x 1]] }
MPI Rank 3: 000000AECC8DA4A0: {[SIM Value[51 x *1]] }
MPI Rank 3: 000000AECC8DA540: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 000000AECC8DA860: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 000000AECC8DA900: {[ce Gradient[1]] }
MPI Rank 3: 000000AECC8DAA40: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000AECC8DAD60: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000AECC8DAEA0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000AECC8DB120: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000AECC8DB1C0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 000000AECC8DB3A0: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000AECC8DB580: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 000000AECC8DB620: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 000000AECC8DB6C0: {[ce Value[1]] }
MPI Rank 3: 000000AECC8DB760: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 000000AECC8DB8A0: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 000000AECC8DB9E0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000AECC8DBA80: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 000000AECC8DC160: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 000000AECC8DC200: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 05/03/2016 14:33:19: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:21: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:33:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 14:33:44:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91057034 * 10240; time = 22.8191s; samplesPerSecond = 448.7
MPI Rank 3: 05/03/2016 14:34:06:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90474224 * 10240; time = 22.0083s; samplesPerSecond = 465.3
MPI Rank 3: 05/03/2016 14:34:16: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=55.8012s
MPI Rank 3: 05/03/2016 14:34:22: Final Results: Minibatch[1-26]: ce = 1.82106098 * 102399; perplexity = 6.17841016
MPI Rank 3: 05/03/2016 14:34:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106098 * 102399
MPI Rank 3: 05/03/2016 14:34:25: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:34:25: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 14:34:25: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu DeviceId=0 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
05/03/2016 18:21:04: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank0
05/03/2016 18:21:05: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank1
05/03/2016 18:21:05: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank2
05/03/2016 18:21:06: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank3
MPI Rank 0: 05/03/2016 18:21:04: -------------------------------------------------------------------
MPI Rank 0: 05/03/2016 18:21:04: Build info: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: 		Built time: May  3 2016 17:56:15
MPI Rank 0: 05/03/2016 18:21:04: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 0: 05/03/2016 18:21:04: 		Build type: release
MPI Rank 0: 05/03/2016 18:21:04: 		Build target: GPU
MPI Rank 0: 05/03/2016 18:21:04: 		With 1bit-SGD: no
MPI Rank 0: 05/03/2016 18:21:04: 		Math lib: acml
MPI Rank 0: 05/03/2016 18:21:04: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 05/03/2016 18:21:04: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 05/03/2016 18:21:04: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 05/03/2016 18:21:04: 		Build Branch: HEAD
MPI Rank 0: 05/03/2016 18:21:04: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 0: 05/03/2016 18:21:04: 		Built by philly on 18750d26eb32
MPI Rank 0: 05/03/2016 18:21:04: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 05/03/2016 18:21:04: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: Running on localhost at 2016/05/03 18:21:04
MPI Rank 0: 05/03/2016 18:21:04: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:21:04: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:21:04: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 05/03/2016 18:21:04: Commands: train
MPI Rank 0: 05/03/2016 18:21:04: Precision = "float"
MPI Rank 0: 05/03/2016 18:21:04: Using 6 CPU threads.
MPI Rank 0: 05/03/2016 18:21:04: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: 05/03/2016 18:21:04: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 05/03/2016 18:21:04: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: ##############################################################################
MPI Rank 0: 05/03/2016 18:21:04: #                                                                            #
MPI Rank 0: 05/03/2016 18:21:04: # Action "train"                                                             #
MPI Rank 0: 05/03/2016 18:21:04: #                                                                            #
MPI Rank 0: 05/03/2016 18:21:04: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:04: Creating virgin network.
MPI Rank 0: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:05: Created model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:05: Training criterion node(s):
MPI Rank 0: 05/03/2016 18:21:05: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x2fe0d08: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0x2fe61a8: {[ce Value[1]] }
MPI Rank 0: 0x3d2be98: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x3d2c9f8: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x3d2d7f8: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x3d2e368: {[Query Value[49292 x *]] }
MPI Rank 0: 0x3d30178: {[Keyword Value[49292 x *]] }
MPI Rank 0: 0x3d31c58: {[S Value[1 x 1]] }
MPI Rank 0: 0x3d32a18: {[N Value[1 x 1]] }
MPI Rank 0: 0x3d33378: {[G Value[1 x 1]] }
MPI Rank 0: 0x3d33df8: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 0x3e28198: {[SIM Value[51 x *]] }
MPI Rank 0: 0x3e28e48: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 0x3e2a3e8: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 0x3e2a728: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 0x3e2abd8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 0x3e2afe8: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 0x3e2b1a8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 0x3e2b368: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 0x3e2b528: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 0x3e2bde8: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x3e2c358: {[ce Gradient[1]] }
MPI Rank 0: 0x3e2c518: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x3e2c6d8: {[SIM Gradient[51 x *]] }
MPI Rank 0: 0x3e2ca58: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 0x3e2ce38: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x3e2ced8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 05/03/2016 18:21:05: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:07: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:07: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:21:10:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.34696808 * 10240; time = 2.4176s; samplesPerSecond = 4235.6
MPI Rank 0: 05/03/2016 18:21:12:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.34277344 * 10240; time = 2.2846s; samplesPerSecond = 4482.3
MPI Rank 0: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=5.89806s
MPI Rank 0: 05/03/2016 18:21:13: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226601
MPI Rank 0: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 0: 05/03/2016 18:21:14: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:21:18:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.30270958 * 10240; time = 2.2716s; samplesPerSecond = 4507.8
MPI Rank 0: 05/03/2016 18:21:20:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.09883804 * 10240; time = 2.2703s; samplesPerSecond = 4510.3
MPI Rank 0: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=5.68469s
MPI Rank 0: 05/03/2016 18:21:21: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 0: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 0: 05/03/2016 18:21:22: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:23: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:23: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:21:26:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.89778175 * 10240; time = 2.2629s; samplesPerSecond = 4525.1
MPI Rank 0: 05/03/2016 18:21:28:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86335983 * 10240; time = 2.2716s; samplesPerSecond = 4507.9
MPI Rank 0: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.67285s
MPI Rank 0: 05/03/2016 18:21:29: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 0: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 0: 05/03/2016 18:21:30: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net'
MPI Rank 0: 05/03/2016 18:21:31: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:31: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:31: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 05/03/2016 18:21:05: -------------------------------------------------------------------
MPI Rank 1: 05/03/2016 18:21:05: Build info: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: 		Built time: May  3 2016 17:56:15
MPI Rank 1: 05/03/2016 18:21:05: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 1: 05/03/2016 18:21:05: 		Build type: release
MPI Rank 1: 05/03/2016 18:21:05: 		Build target: GPU
MPI Rank 1: 05/03/2016 18:21:05: 		With 1bit-SGD: no
MPI Rank 1: 05/03/2016 18:21:05: 		Math lib: acml
MPI Rank 1: 05/03/2016 18:21:05: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 05/03/2016 18:21:05: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 05/03/2016 18:21:05: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 05/03/2016 18:21:05: 		Build Branch: HEAD
MPI Rank 1: 05/03/2016 18:21:05: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 1: 05/03/2016 18:21:05: 		Built by philly on 18750d26eb32
MPI Rank 1: 05/03/2016 18:21:05: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 05/03/2016 18:21:05: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: Running on localhost at 2016/05/03 18:21:05
MPI Rank 1: 05/03/2016 18:21:05: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:21:05: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:21:05: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 05/03/2016 18:21:05: Commands: train
MPI Rank 1: 05/03/2016 18:21:05: Precision = "float"
MPI Rank 1: 05/03/2016 18:21:05: Using 6 CPU threads.
MPI Rank 1: 05/03/2016 18:21:05: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: 05/03/2016 18:21:05: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 05/03/2016 18:21:05: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: ##############################################################################
MPI Rank 1: 05/03/2016 18:21:05: #                                                                            #
MPI Rank 1: 05/03/2016 18:21:05: # Action "train"                                                             #
MPI Rank 1: 05/03/2016 18:21:05: #                                                                            #
MPI Rank 1: 05/03/2016 18:21:05: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: Creating virgin network.
MPI Rank 1: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: Created model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:05: Training criterion node(s):
MPI Rank 1: 05/03/2016 18:21:05: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x2a14fa8: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x7f95e96dcd48: {[SIM Value[51 x *]] }
MPI Rank 1: 0x7f95e96dd428: {[ce Value[1]] }
MPI Rank 1: 0x7f95eb3a29d8: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 0x7f95eb3a4388: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 0x7f95eb3a4798: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 0x7f95eb3a4958: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 0x7f95eb3a4b18: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 0x7f95eb3a4cd8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 0x7f95eb3a4e98: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 0x7f95eb3a5058: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 0x7f95eb3a5918: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x7f95eb3a5e58: {[ce Gradient[1]] }
MPI Rank 1: 0x7f95eb3a6018: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x7f95eb3a61d8: {[SIM Gradient[51 x *]] }
MPI Rank 1: 0x7f95eb3a6558: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 0x7f95eb3a6908: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7f95eb3a69a8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7f95ec75caa8: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x7f95ec75d918: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x7f95ec75e798: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x7f95ec75efa8: {[Query Value[49292 x *]] }
MPI Rank 1: 0x7f95ec761098: {[Keyword Value[49292 x *]] }
MPI Rank 1: 0x7f95ec7616e8: {[S Value[1 x 1]] }
MPI Rank 1: 0x7f95ec761d68: {[N Value[1 x 1]] }
MPI Rank 1: 0x7f95ec7642f8: {[G Value[1 x 1]] }
MPI Rank 1: 0x7f95ec764d18: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 05/03/2016 18:21:05: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:07: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:07: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:21:10:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32159615 * 10240; time = 2.4038s; samplesPerSecond = 4259.9
MPI Rank 1: 05/03/2016 18:21:12:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.33525505 * 10240; time = 2.2846s; samplesPerSecond = 4482.3
MPI Rank 1: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=5.89808s
MPI Rank 1: 05/03/2016 18:21:13: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226601
MPI Rank 1: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:21:18:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32732925 * 10240; time = 2.2716s; samplesPerSecond = 4507.8
MPI Rank 1: 05/03/2016 18:21:20:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11035995 * 10240; time = 2.2703s; samplesPerSecond = 4510.3
MPI Rank 1: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=5.68468s
MPI Rank 1: 05/03/2016 18:21:21: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 1: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:23: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:23: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:21:26:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.92909813 * 10240; time = 2.2630s; samplesPerSecond = 4525.0
MPI Rank 1: 05/03/2016 18:21:28:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86598778 * 10240; time = 2.2716s; samplesPerSecond = 4507.9
MPI Rank 1: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.67286s
MPI Rank 1: 05/03/2016 18:21:29: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 1: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 1: 05/03/2016 18:21:31: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:31: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:31: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 05/03/2016 18:21:05: -------------------------------------------------------------------
MPI Rank 2: 05/03/2016 18:21:05: Build info: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: 		Built time: May  3 2016 17:56:15
MPI Rank 2: 05/03/2016 18:21:05: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 2: 05/03/2016 18:21:05: 		Build type: release
MPI Rank 2: 05/03/2016 18:21:05: 		Build target: GPU
MPI Rank 2: 05/03/2016 18:21:05: 		With 1bit-SGD: no
MPI Rank 2: 05/03/2016 18:21:05: 		Math lib: acml
MPI Rank 2: 05/03/2016 18:21:05: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 05/03/2016 18:21:05: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 05/03/2016 18:21:05: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 05/03/2016 18:21:05: 		Build Branch: HEAD
MPI Rank 2: 05/03/2016 18:21:05: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 2: 05/03/2016 18:21:05: 		Built by philly on 18750d26eb32
MPI Rank 2: 05/03/2016 18:21:05: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 05/03/2016 18:21:05: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: Running on localhost at 2016/05/03 18:21:05
MPI Rank 2: 05/03/2016 18:21:05: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:21:05: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:21:05: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 05/03/2016 18:21:05: Commands: train
MPI Rank 2: 05/03/2016 18:21:05: Precision = "float"
MPI Rank 2: 05/03/2016 18:21:05: Using 6 CPU threads.
MPI Rank 2: 05/03/2016 18:21:05: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: 05/03/2016 18:21:05: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 05/03/2016 18:21:05: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: ##############################################################################
MPI Rank 2: 05/03/2016 18:21:05: #                                                                            #
MPI Rank 2: 05/03/2016 18:21:05: # Action "train"                                                             #
MPI Rank 2: 05/03/2016 18:21:05: #                                                                            #
MPI Rank 2: 05/03/2016 18:21:05: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:05: Creating virgin network.
MPI Rank 2: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:06: Created model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:06: Training criterion node(s):
MPI Rank 2: 05/03/2016 18:21:06: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x1dc03a8: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x1dc0f58: {[ce Value[1]] }
MPI Rank 2: 0x2b06c58: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x2b07aa8: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x2b08578: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x2b090e8: {[Query Value[49292 x *]] }
MPI Rank 2: 0x2b0aef8: {[Keyword Value[49292 x *]] }
MPI Rank 2: 0x2b0b2d8: {[S Value[1 x 1]] }
MPI Rank 2: 0x2b0d748: {[N Value[1 x 1]] }
MPI Rank 2: 0x2b0e0e8: {[G Value[1 x 1]] }
MPI Rank 2: 0x2b0eb88: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 0x2c02e98: {[SIM Value[51 x *]] }
MPI Rank 2: 0x2c05098: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 0x2c05318: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 0x2c05888: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 0x2c05c38: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 0x2c05df8: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 0x2c05fb8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 0x2c06178: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 0x2c06338: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 0x2c06bf8: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x2c07168: {[ce Gradient[1]] }
MPI Rank 2: 0x2c07328: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x2c074e8: {[SIM Gradient[51 x *]] }
MPI Rank 2: 0x2c07868: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 0x2c07c48: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x2c07ce8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 05/03/2016 18:21:06: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:07: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:07: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:21:10:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32837563 * 10240; time = 2.4179s; samplesPerSecond = 4235.1
MPI Rank 2: 05/03/2016 18:21:12:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35655479 * 10240; time = 2.2846s; samplesPerSecond = 4482.3
MPI Rank 2: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=5.89804s
MPI Rank 2: 05/03/2016 18:21:13: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226601
MPI Rank 2: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:21:18:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32893600 * 10240; time = 2.2716s; samplesPerSecond = 4507.9
MPI Rank 2: 05/03/2016 18:21:20:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11646919 * 10240; time = 2.2704s; samplesPerSecond = 4510.3
MPI Rank 2: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=5.68468s
MPI Rank 2: 05/03/2016 18:21:21: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 2: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:23: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:23: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:21:26:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95308418 * 10240; time = 2.2629s; samplesPerSecond = 4525.1
MPI Rank 2: 05/03/2016 18:21:28:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87902641 * 10240; time = 2.2715s; samplesPerSecond = 4508.0
MPI Rank 2: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.67283s
MPI Rank 2: 05/03/2016 18:21:29: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 2: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 2: 05/03/2016 18:21:31: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:31: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:31: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 05/03/2016 18:21:06: -------------------------------------------------------------------
MPI Rank 3: 05/03/2016 18:21:06: Build info: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: 		Built time: May  3 2016 17:56:15
MPI Rank 3: 05/03/2016 18:21:06: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 3: 05/03/2016 18:21:06: 		Build type: release
MPI Rank 3: 05/03/2016 18:21:06: 		Build target: GPU
MPI Rank 3: 05/03/2016 18:21:06: 		With 1bit-SGD: no
MPI Rank 3: 05/03/2016 18:21:06: 		Math lib: acml
MPI Rank 3: 05/03/2016 18:21:06: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 05/03/2016 18:21:06: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 05/03/2016 18:21:06: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 05/03/2016 18:21:06: 		Build Branch: HEAD
MPI Rank 3: 05/03/2016 18:21:06: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 3: 05/03/2016 18:21:06: 		Built by philly on 18750d26eb32
MPI Rank 3: 05/03/2016 18:21:06: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 05/03/2016 18:21:06: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: Running on localhost at 2016/05/03 18:21:06
MPI Rank 3: 05/03/2016 18:21:06: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:21:06: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:21:06: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 05/03/2016 18:21:06: Commands: train
MPI Rank 3: 05/03/2016 18:21:06: Precision = "float"
MPI Rank 3: 05/03/2016 18:21:06: Using 6 CPU threads.
MPI Rank 3: 05/03/2016 18:21:06: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: 05/03/2016 18:21:06: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 05/03/2016 18:21:06: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: ##############################################################################
MPI Rank 3: 05/03/2016 18:21:06: #                                                                            #
MPI Rank 3: 05/03/2016 18:21:06: # Action "train"                                                             #
MPI Rank 3: 05/03/2016 18:21:06: #                                                                            #
MPI Rank 3: 05/03/2016 18:21:06: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: Creating virgin network.
MPI Rank 3: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: Created model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:06: Training criterion node(s):
MPI Rank 3: 05/03/2016 18:21:06: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x2908f78: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x290a608: {[ce Value[1]] }
MPI Rank 3: 0x3650328: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x3651118: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x3651f78: {[WD1 Value[64 x 288]] }
MPI Rank 3: 0x36527f8: {[Query Value[49292 x *]] }
MPI Rank 3: 0x3654608: {[Keyword Value[49292 x *]] }
MPI Rank 3: 0x36560e8: {[S Value[1 x 1]] }
MPI Rank 3: 0x3656ea8: {[N Value[1 x 1]] }
MPI Rank 3: 0x3657808: {[G Value[1 x 1]] }
MPI Rank 3: 0x3658288: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 0x374c598: {[SIM Value[51 x *]] }
MPI Rank 3: 0x374e798: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 0x374ea18: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 0x374ef88: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 0x374f338: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 0x374f4f8: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 0x374f6b8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 0x374f878: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 0x374fa38: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 0x37502f8: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x3750868: {[ce Gradient[1]] }
MPI Rank 3: 0x3750a28: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x3750be8: {[SIM Gradient[51 x *]] }
MPI Rank 3: 0x3750f68: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 0x3751348: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x37513e8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 05/03/2016 18:21:06: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:07: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:07: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:21:10:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32287750 * 10240; time = 2.4282s; samplesPerSecond = 4217.2
MPI Rank 3: 05/03/2016 18:21:12:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35470428 * 10240; time = 2.2846s; samplesPerSecond = 4482.3
MPI Rank 3: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=5.89806s
MPI Rank 3: 05/03/2016 18:21:13: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226601
MPI Rank 3: 05/03/2016 18:21:13: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:21:18:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29653873 * 10240; time = 2.2716s; samplesPerSecond = 4507.8
MPI Rank 3: 05/03/2016 18:21:20:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11679478 * 10240; time = 2.2703s; samplesPerSecond = 4510.3
MPI Rank 3: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=5.68469s
MPI Rank 3: 05/03/2016 18:21:21: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 3: 05/03/2016 18:21:21: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:23: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:23: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:21:26:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90347176 * 10240; time = 2.2629s; samplesPerSecond = 4525.1
MPI Rank 3: 05/03/2016 18:21:28:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88304176 * 10240; time = 2.2715s; samplesPerSecond = 4508.0
MPI Rank 3: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.67284s
MPI Rank 3: 05/03/2016 18:21:29: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 3: 05/03/2016 18:21:29: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 3: 05/03/2016 18:21:31: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:31: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:31: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu DeviceId=0 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
MPIWrapper: initializing MPI
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
05/03/2016 18:21:32: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank0
05/03/2016 18:21:32: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank1
05/03/2016 18:21:33: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank2
05/03/2016 18:21:33: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr_train.logrank3
MPI Rank 0: 05/03/2016 18:21:32: -------------------------------------------------------------------
MPI Rank 0: 05/03/2016 18:21:32: Build info: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: 		Built time: May  3 2016 17:56:15
MPI Rank 0: 05/03/2016 18:21:32: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 0: 05/03/2016 18:21:32: 		Build type: release
MPI Rank 0: 05/03/2016 18:21:32: 		Build target: GPU
MPI Rank 0: 05/03/2016 18:21:32: 		With 1bit-SGD: no
MPI Rank 0: 05/03/2016 18:21:32: 		Math lib: acml
MPI Rank 0: 05/03/2016 18:21:32: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 05/03/2016 18:21:32: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 05/03/2016 18:21:32: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 05/03/2016 18:21:32: 		Build Branch: HEAD
MPI Rank 0: 05/03/2016 18:21:32: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 0: 05/03/2016 18:21:32: 		Built by philly on 18750d26eb32
MPI Rank 0: 05/03/2016 18:21:32: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 05/03/2016 18:21:32: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: Running on localhost at 2016/05/03 18:21:32
MPI Rank 0: 05/03/2016 18:21:32: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:21:32: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:21:32: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 05/03/2016 18:21:32: Commands: train
MPI Rank 0: 05/03/2016 18:21:32: Precision = "float"
MPI Rank 0: 05/03/2016 18:21:32: Using 6 CPU threads.
MPI Rank 0: 05/03/2016 18:21:32: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: 05/03/2016 18:21:32: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 05/03/2016 18:21:32: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: ##############################################################################
MPI Rank 0: 05/03/2016 18:21:32: #                                                                            #
MPI Rank 0: 05/03/2016 18:21:32: # Action "train"                                                             #
MPI Rank 0: 05/03/2016 18:21:32: #                                                                            #
MPI Rank 0: 05/03/2016 18:21:32: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:32: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:33: Loaded model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:33: Training criterion node(s):
MPI Rank 0: 05/03/2016 18:21:33: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x24c6c48: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 0x29ac1a8: {[N Value[1 x 1]] }
MPI Rank 0: 0x29ac528: {[G Value[1 x 1]] }
MPI Rank 0: 0x2de2038: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x2de2198: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 0x2de2518: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 0x2de26d8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 0x2de28f8: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x2df49f8: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0x2df6258: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x2dfb9b8: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 0x2dfbf08: {[Query Value[49292 x *1]] }
MPI Rank 0: 0x2dfdd78: {[S Value[1 x 1]] }
MPI Rank 0: 0x2dffcc8: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x2e01108: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x2e04018: {[SIM Value[51 x *1]] }
MPI Rank 0: 0x2e04248: {[ce Value[1]] }
MPI Rank 0: 0x2e05ac8: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 0x2e05e08: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 0x2e06318: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 0x2e064d8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 0x2e06698: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 0x2e06858: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 0x2e06a18: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 0x2e06bd8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 0x2e07498: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x2e079d8: {[ce Gradient[1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 05/03/2016 18:21:33: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:35: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:35: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:21:38:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87577038 * 10240; time = 2.3989s; samplesPerSecond = 4268.6
MPI Rank 0: 05/03/2016 18:21:40:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79361134 * 10240; time = 2.2817s; samplesPerSecond = 4487.8
MPI Rank 0: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.87162s
MPI Rank 0: 05/03/2016 18:21:41: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241646
MPI Rank 0: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 0: 05/03/2016 18:21:42: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net'
MPI Rank 0: 05/03/2016 18:21:44: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:44: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:44: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 05/03/2016 18:21:32: -------------------------------------------------------------------
MPI Rank 1: 05/03/2016 18:21:32: Build info: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: 		Built time: May  3 2016 17:56:15
MPI Rank 1: 05/03/2016 18:21:32: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 1: 05/03/2016 18:21:32: 		Build type: release
MPI Rank 1: 05/03/2016 18:21:32: 		Build target: GPU
MPI Rank 1: 05/03/2016 18:21:32: 		With 1bit-SGD: no
MPI Rank 1: 05/03/2016 18:21:32: 		Math lib: acml
MPI Rank 1: 05/03/2016 18:21:32: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 05/03/2016 18:21:32: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 05/03/2016 18:21:32: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 05/03/2016 18:21:32: 		Build Branch: HEAD
MPI Rank 1: 05/03/2016 18:21:32: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 1: 05/03/2016 18:21:32: 		Built by philly on 18750d26eb32
MPI Rank 1: 05/03/2016 18:21:32: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 05/03/2016 18:21:32: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: Running on localhost at 2016/05/03 18:21:32
MPI Rank 1: 05/03/2016 18:21:32: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:21:32: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:21:32: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 05/03/2016 18:21:32: Commands: train
MPI Rank 1: 05/03/2016 18:21:32: Precision = "float"
MPI Rank 1: 05/03/2016 18:21:32: Using 6 CPU threads.
MPI Rank 1: 05/03/2016 18:21:32: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: 05/03/2016 18:21:32: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 05/03/2016 18:21:32: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: ##############################################################################
MPI Rank 1: 05/03/2016 18:21:32: #                                                                            #
MPI Rank 1: 05/03/2016 18:21:32: # Action "train"                                                             #
MPI Rank 1: 05/03/2016 18:21:32: #                                                                            #
MPI Rank 1: 05/03/2016 18:21:32: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:32: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:33: Loaded model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:33: Training criterion node(s):
MPI Rank 1: 05/03/2016 18:21:33: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x1cb3888: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 0x29fe698: {[G Value[1 x 1]] }
MPI Rank 1: 0x29ff308: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 0x29ff598: {[Query Value[49292 x *1]] }
MPI Rank 1: 0x2a00348: {[N Value[1 x 1]] }
MPI Rank 1: 0x2a01d08: {[S Value[1 x 1]] }
MPI Rank 1: 0x2a03228: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x2a04998: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x2a05168: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x2a05468: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x2a09928: {[SIM Value[51 x *1]] }
MPI Rank 1: 0x2a09b58: {[ce Value[1]] }
MPI Rank 1: 0x2a0b3d8: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 0x2a0b718: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 0x2a0bc28: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 0x2a0bde8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 0x2a0bfa8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 0x2a0c168: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 0x2a0c328: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 0x2a0c4e8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 0x2a0cda8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x2a0d2e8: {[ce Gradient[1]] }
MPI Rank 1: 0x2a0d4a8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x2a0d668: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 0x2a0d9e8: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 0x2a0ddc8: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x2a0de68: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 05/03/2016 18:21:33: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:35: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:35: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:21:38:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93745022 * 10240; time = 2.4004s; samplesPerSecond = 4265.9
MPI Rank 1: 05/03/2016 18:21:40:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89571209 * 10240; time = 2.2817s; samplesPerSecond = 4487.8
MPI Rank 1: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.87162s
MPI Rank 1: 05/03/2016 18:21:41: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241646
MPI Rank 1: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 1: 05/03/2016 18:21:44: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:44: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:44: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 05/03/2016 18:21:33: -------------------------------------------------------------------
MPI Rank 2: 05/03/2016 18:21:33: Build info: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: 		Built time: May  3 2016 17:56:15
MPI Rank 2: 05/03/2016 18:21:33: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 2: 05/03/2016 18:21:33: 		Build type: release
MPI Rank 2: 05/03/2016 18:21:33: 		Build target: GPU
MPI Rank 2: 05/03/2016 18:21:33: 		With 1bit-SGD: no
MPI Rank 2: 05/03/2016 18:21:33: 		Math lib: acml
MPI Rank 2: 05/03/2016 18:21:33: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 05/03/2016 18:21:33: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 05/03/2016 18:21:33: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 05/03/2016 18:21:33: 		Build Branch: HEAD
MPI Rank 2: 05/03/2016 18:21:33: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 2: 05/03/2016 18:21:33: 		Built by philly on 18750d26eb32
MPI Rank 2: 05/03/2016 18:21:33: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 05/03/2016 18:21:33: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: Running on localhost at 2016/05/03 18:21:33
MPI Rank 2: 05/03/2016 18:21:33: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:21:33: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:21:33: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 05/03/2016 18:21:33: Commands: train
MPI Rank 2: 05/03/2016 18:21:33: Precision = "float"
MPI Rank 2: 05/03/2016 18:21:33: Using 6 CPU threads.
MPI Rank 2: 05/03/2016 18:21:33: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: 05/03/2016 18:21:33: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 05/03/2016 18:21:33: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: ##############################################################################
MPI Rank 2: 05/03/2016 18:21:33: #                                                                            #
MPI Rank 2: 05/03/2016 18:21:33: # Action "train"                                                             #
MPI Rank 2: 05/03/2016 18:21:33: #                                                                            #
MPI Rank 2: 05/03/2016 18:21:33: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:33: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:34: Loaded model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:34: Training criterion node(s):
MPI Rank 2: 05/03/2016 18:21:34: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x10a8cc8: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 0x1db7ea8: {[N Value[1 x 1]] }
MPI Rank 2: 0x1dbbdf8: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 0x1dbc108: {[G Value[1 x 1]] }
MPI Rank 2: 0x1decbf8: {[S Value[1 x 1]] }
MPI Rank 2: 0x1dedb28: {[Query Value[49292 x *1]] }
MPI Rank 2: 0x1def178: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x1df0eb8: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x1df1988: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x1df2dc8: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x1df5e48: {[SIM Value[51 x *1]] }
MPI Rank 2: 0x1df6078: {[ce Value[1]] }
MPI Rank 2: 0x1df78f8: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 0x1df7c38: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 0x1df8148: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 0x1df8308: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 0x1df84c8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 0x1df8688: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 0x1df8848: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 0x1df8a08: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 0x1df92c8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x1df9808: {[ce Gradient[1]] }
MPI Rank 2: 0x1df99c8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x1df9b88: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 0x1df9f08: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 0x1dfa2e8: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x1dfa388: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 05/03/2016 18:21:34: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:35: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:35: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:21:38:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96188030 * 10240; time = 2.3991s; samplesPerSecond = 4268.2
MPI Rank 2: 05/03/2016 18:21:40:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90950069 * 10240; time = 2.2817s; samplesPerSecond = 4487.8
MPI Rank 2: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.87164s
MPI Rank 2: 05/03/2016 18:21:41: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241646
MPI Rank 2: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 2: 05/03/2016 18:21:44: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:44: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:44: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 05/03/2016 18:21:33: -------------------------------------------------------------------
MPI Rank 3: 05/03/2016 18:21:33: Build info: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: 		Built time: May  3 2016 17:56:15
MPI Rank 3: 05/03/2016 18:21:33: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 3: 05/03/2016 18:21:33: 		Build type: release
MPI Rank 3: 05/03/2016 18:21:33: 		Build target: GPU
MPI Rank 3: 05/03/2016 18:21:33: 		With 1bit-SGD: no
MPI Rank 3: 05/03/2016 18:21:33: 		Math lib: acml
MPI Rank 3: 05/03/2016 18:21:33: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 05/03/2016 18:21:33: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 05/03/2016 18:21:33: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 05/03/2016 18:21:33: 		Build Branch: HEAD
MPI Rank 3: 05/03/2016 18:21:33: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 3: 05/03/2016 18:21:33: 		Built by philly on 18750d26eb32
MPI Rank 3: 05/03/2016 18:21:33: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 05/03/2016 18:21:33: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: Running on localhost at 2016/05/03 18:21:33
MPI Rank 3: 05/03/2016 18:21:33: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:21:33: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:21:33: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 05/03/2016 18:21:33: Commands: train
MPI Rank 3: 05/03/2016 18:21:33: Precision = "float"
MPI Rank 3: 05/03/2016 18:21:33: Using 6 CPU threads.
MPI Rank 3: 05/03/2016 18:21:33: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: 05/03/2016 18:21:33: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 05/03/2016 18:21:33: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: ##############################################################################
MPI Rank 3: 05/03/2016 18:21:33: #                                                                            #
MPI Rank 3: 05/03/2016 18:21:33: # Action "train"                                                             #
MPI Rank 3: 05/03/2016 18:21:33: #                                                                            #
MPI Rank 3: 05/03/2016 18:21:33: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:33: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:34: Loaded model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:34: Training criterion node(s):
MPI Rank 3: 05/03/2016 18:21:34: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x218c138: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 0x266c258: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x266ede8: {[WD1 Value[64 x 288]] }
MPI Rank 3: 0x266fda8: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x2671208: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x2674258: {[SIM Value[51 x *1]] }
MPI Rank 3: 0x2674488: {[ce Value[1]] }
MPI Rank 3: 0x2675d08: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 0x2676048: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 0x2676558: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 0x2676718: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 0x26768d8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 0x2676a98: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 0x2676c58: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 0x2676e18: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 0x26776d8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x2677c18: {[ce Gradient[1]] }
MPI Rank 3: 0x2677dd8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x2677f98: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 0x2678318: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 0x26786f8: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x2678798: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 0x2ac8868: {[S Value[1 x 1]] }
MPI Rank 3: 0x2ac8be8: {[G Value[1 x 1]] }
MPI Rank 3: 0x2ac8da8: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 0x2ac99a8: {[Query Value[49292 x *1]] }
MPI Rank 3: 0x2aca928: {[N Value[1 x 1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 05/03/2016 18:21:34: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:35: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:35: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:21:38:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91174297 * 10240; time = 2.4090s; samplesPerSecond = 4250.7
MPI Rank 3: 05/03/2016 18:21:40:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91370487 * 10240; time = 2.2817s; samplesPerSecond = 4487.8
MPI Rank 3: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=5.87163s
MPI Rank 3: 05/03/2016 18:21:41: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241646
MPI Rank 3: 05/03/2016 18:21:41: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 3: 05/03/2016 18:21:44: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:44: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:44: __COMPLETED__
MPI Rank 3: ~MPIWrapper
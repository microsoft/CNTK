=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
MPIWrapper: initializing MPI
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
05/03/2016 18:19:49: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank0
05/03/2016 18:19:49: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank1
05/03/2016 18:19:50: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank2
05/03/2016 18:19:50: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 0: 05/03/2016 18:19:49: -------------------------------------------------------------------
MPI Rank 0: 05/03/2016 18:19:49: Build info: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: 		Built time: May  3 2016 17:56:15
MPI Rank 0: 05/03/2016 18:19:49: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 0: 05/03/2016 18:19:49: 		Build type: release
MPI Rank 0: 05/03/2016 18:19:49: 		Build target: GPU
MPI Rank 0: 05/03/2016 18:19:49: 		With 1bit-SGD: no
MPI Rank 0: 05/03/2016 18:19:49: 		Math lib: acml
MPI Rank 0: 05/03/2016 18:19:49: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 05/03/2016 18:19:49: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 05/03/2016 18:19:49: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 05/03/2016 18:19:49: 		Build Branch: HEAD
MPI Rank 0: 05/03/2016 18:19:49: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 0: 05/03/2016 18:19:49: 		Built by philly on 18750d26eb32
MPI Rank 0: 05/03/2016 18:19:49: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 05/03/2016 18:19:49: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: Running on localhost at 2016/05/03 18:19:49
MPI Rank 0: 05/03/2016 18:19:49: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:19:49: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:19:49: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 05/03/2016 18:19:49: Commands: train
MPI Rank 0: 05/03/2016 18:19:49: Precision = "float"
MPI Rank 0: 05/03/2016 18:19:49: Using 6 CPU threads.
MPI Rank 0: 05/03/2016 18:19:49: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 05/03/2016 18:19:49: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 05/03/2016 18:19:49: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: ##############################################################################
MPI Rank 0: 05/03/2016 18:19:49: #                                                                            #
MPI Rank 0: 05/03/2016 18:19:49: # Action "train"                                                             #
MPI Rank 0: 05/03/2016 18:19:49: #                                                                            #
MPI Rank 0: 05/03/2016 18:19:49: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: Creating virgin network.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: Created model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:49: Training criterion node(s):
MPI Rank 0: 05/03/2016 18:19:49: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x24c4d08: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0x24c8c98: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x24c9548: {[Query Value[49292 x *]] }
MPI Rank 0: 0x24c9f28: {[Keyword Value[49292 x *]] }
MPI Rank 0: 0x24ca1a8: {[ce Value[1]] }
MPI Rank 0: 0x24ca6b8: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x24cac48: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x251d3c8: {[S Value[1 x 1]] }
MPI Rank 0: 0x251d7e8: {[N Value[1 x 1]] }
MPI Rank 0: 0x251dc08: {[G Value[1 x 1]] }
MPI Rank 0: 0x251e118: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 0x25201d8: {[SIM Value[51 x *]] }
MPI Rank 0: 0x2522ef8: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 0x2523178: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 0x25236e8: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 0x2523848: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 0x25239a8: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 0x2523b08: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 0x2523cc8: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 0x2523e88: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 0x2524748: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x2524c88: {[ce Gradient[1]] }
MPI Rank 0: 0x2524e48: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x2525008: {[SIM Gradient[51 x *]] }
MPI Rank 0: 0x2525388: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 0x2525738: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x25257d8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 05/03/2016 18:19:49: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:52: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:19:52: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:19:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.44191360 * 10240; time = 5.1175s; samplesPerSecond = 2001.0
MPI Rank 0: 05/03/2016 18:20:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.68707848 * 10240; time = 4.5411s; samplesPerSecond = 2255.0
MPI Rank 0: 05/03/2016 18:20:04: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=12.0115s
MPI Rank 0: 05/03/2016 18:20:06: Final Results: Minibatch[1-26]: ce = 2.13785051 * 102399; perplexity = 8.48118778
MPI Rank 0: 05/03/2016 18:20:06: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785051 * 102399
MPI Rank 0: 05/03/2016 18:20:08: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:09: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:20:13:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.99931793 * 10240; time = 4.6696s; samplesPerSecond = 2192.9
MPI Rank 0: 05/03/2016 18:20:18:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89505386 * 10240; time = 4.6692s; samplesPerSecond = 2193.1
MPI Rank 0: 05/03/2016 18:20:20: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=11.6723s
MPI Rank 0: 05/03/2016 18:20:23: Final Results: Minibatch[1-26]: ce = 1.81478809 * 102399; perplexity = 6.13977497
MPI Rank 0: 05/03/2016 18:20:23: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478809 * 102399
MPI Rank 0: 05/03/2016 18:20:24: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:25: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:25: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:20:30:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.75707302 * 10240; time = 4.8144s; samplesPerSecond = 2126.9
MPI Rank 0: 05/03/2016 18:20:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.74772396 * 10240; time = 4.7230s; samplesPerSecond = 2168.1
MPI Rank 0: 05/03/2016 18:20:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639891 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=11.8878s
MPI Rank 0: 05/03/2016 18:20:39: Final Results: Minibatch[1-26]: ce = 1.69809270 * 102399; perplexity = 5.46351687
MPI Rank 0: 05/03/2016 18:20:39: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809270 * 102399
MPI Rank 0: 05/03/2016 18:20:41: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 05/03/2016 18:20:42: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 05/03/2016 18:19:49: -------------------------------------------------------------------
MPI Rank 1: 05/03/2016 18:19:49: Build info: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: 		Built time: May  3 2016 17:56:15
MPI Rank 1: 05/03/2016 18:19:49: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 1: 05/03/2016 18:19:49: 		Build type: release
MPI Rank 1: 05/03/2016 18:19:49: 		Build target: GPU
MPI Rank 1: 05/03/2016 18:19:49: 		With 1bit-SGD: no
MPI Rank 1: 05/03/2016 18:19:49: 		Math lib: acml
MPI Rank 1: 05/03/2016 18:19:49: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 05/03/2016 18:19:49: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 05/03/2016 18:19:49: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 05/03/2016 18:19:49: 		Build Branch: HEAD
MPI Rank 1: 05/03/2016 18:19:49: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 1: 05/03/2016 18:19:49: 		Built by philly on 18750d26eb32
MPI Rank 1: 05/03/2016 18:19:49: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 05/03/2016 18:19:49: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: Running on localhost at 2016/05/03 18:19:49
MPI Rank 1: 05/03/2016 18:19:49: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:19:49: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:19:49: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 05/03/2016 18:19:49: Commands: train
MPI Rank 1: 05/03/2016 18:19:49: Precision = "float"
MPI Rank 1: 05/03/2016 18:19:49: Using 6 CPU threads.
MPI Rank 1: 05/03/2016 18:19:49: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 05/03/2016 18:19:49: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 05/03/2016 18:19:49: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: ##############################################################################
MPI Rank 1: 05/03/2016 18:19:49: #                                                                            #
MPI Rank 1: 05/03/2016 18:19:49: # Action "train"                                                             #
MPI Rank 1: 05/03/2016 18:19:49: #                                                                            #
MPI Rank 1: 05/03/2016 18:19:49: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:49: Creating virgin network.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:50: Created model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:50: Training criterion node(s):
MPI Rank 1: 05/03/2016 18:19:50: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x1b4f158: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x1b52f98: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x1b54568: {[ce Value[1]] }
MPI Rank 1: 0x1b54bb8: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x1b54d18: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x1b55098: {[Query Value[49292 x *]] }
MPI Rank 1: 0x1b92f18: {[Keyword Value[49292 x *]] }
MPI Rank 1: 0x1ba7798: {[S Value[1 x 1]] }
MPI Rank 1: 0x1ba7bb8: {[N Value[1 x 1]] }
MPI Rank 1: 0x1ba7fd8: {[G Value[1 x 1]] }
MPI Rank 1: 0x1ba84e8: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 0x1baaeb8: {[SIM Value[51 x *]] }
MPI Rank 1: 0x1baba38: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 0x1bad358: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 0x1bad698: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 0x1badba8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 0x1badd68: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 0x1badf28: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 0x1bae0e8: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 0x1bae2a8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 0x1baeb68: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x1baf0a8: {[ce Gradient[1]] }
MPI Rank 1: 0x1baf268: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x1baf428: {[SIM Gradient[51 x *]] }
MPI Rank 1: 0x1baf7a8: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 0x1bafb58: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x1bafbf8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 05/03/2016 18:19:50: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:52: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:19:52: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:19:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.47101059 * 10240; time = 5.1185s; samplesPerSecond = 2000.6
MPI Rank 1: 05/03/2016 18:20:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.69970703 * 10240; time = 4.5411s; samplesPerSecond = 2255.0
MPI Rank 1: 05/03/2016 18:20:04: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=12.0114s
MPI Rank 1: 05/03/2016 18:20:06: Final Results: Minibatch[1-26]: ce = 2.13785051 * 102399; perplexity = 8.48118778
MPI Rank 1: 05/03/2016 18:20:06: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785051 * 102399
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:09: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:20:13:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.02775860 * 10240; time = 4.6696s; samplesPerSecond = 2192.9
MPI Rank 1: 05/03/2016 18:20:18:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90990696 * 10240; time = 4.6692s; samplesPerSecond = 2193.1
MPI Rank 1: 05/03/2016 18:20:20: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=11.6723s
MPI Rank 1: 05/03/2016 18:20:23: Final Results: Minibatch[1-26]: ce = 1.81478809 * 102399; perplexity = 6.13977497
MPI Rank 1: 05/03/2016 18:20:23: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478809 * 102399
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:25: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:25: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:20:30:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.77812557 * 10240; time = 4.8144s; samplesPerSecond = 2126.9
MPI Rank 1: 05/03/2016 18:20:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.74922504 * 10240; time = 4.7230s; samplesPerSecond = 2168.1
MPI Rank 1: 05/03/2016 18:20:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639891 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=11.8878s
MPI Rank 1: 05/03/2016 18:20:39: Final Results: Minibatch[1-26]: ce = 1.69809270 * 102399; perplexity = 5.46351687
MPI Rank 1: 05/03/2016 18:20:39: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809270 * 102399
MPI Rank 1: 05/03/2016 18:20:42: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:42: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:42: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 05/03/2016 18:19:50: -------------------------------------------------------------------
MPI Rank 2: 05/03/2016 18:19:50: Build info: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: 		Built time: May  3 2016 17:56:15
MPI Rank 2: 05/03/2016 18:19:50: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 2: 05/03/2016 18:19:50: 		Build type: release
MPI Rank 2: 05/03/2016 18:19:50: 		Build target: GPU
MPI Rank 2: 05/03/2016 18:19:50: 		With 1bit-SGD: no
MPI Rank 2: 05/03/2016 18:19:50: 		Math lib: acml
MPI Rank 2: 05/03/2016 18:19:50: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 05/03/2016 18:19:50: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 05/03/2016 18:19:50: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 05/03/2016 18:19:50: 		Build Branch: HEAD
MPI Rank 2: 05/03/2016 18:19:50: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 2: 05/03/2016 18:19:50: 		Built by philly on 18750d26eb32
MPI Rank 2: 05/03/2016 18:19:50: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 05/03/2016 18:19:50: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: Running on localhost at 2016/05/03 18:19:50
MPI Rank 2: 05/03/2016 18:19:50: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:19:50: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:19:50: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 05/03/2016 18:19:50: Commands: train
MPI Rank 2: 05/03/2016 18:19:50: Precision = "float"
MPI Rank 2: 05/03/2016 18:19:50: Using 6 CPU threads.
MPI Rank 2: 05/03/2016 18:19:50: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 05/03/2016 18:19:50: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 05/03/2016 18:19:50: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: ##############################################################################
MPI Rank 2: 05/03/2016 18:19:50: #                                                                            #
MPI Rank 2: 05/03/2016 18:19:50: # Action "train"                                                             #
MPI Rank 2: 05/03/2016 18:19:50: #                                                                            #
MPI Rank 2: 05/03/2016 18:19:50: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: Creating virgin network.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: Created model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:50: Training criterion node(s):
MPI Rank 2: 05/03/2016 18:19:50: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x2036738: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x2037208: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x2037d38: {[ce Value[1]] }
MPI Rank 2: 0x2038308: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x20383b8: {[Keyword Value[49292 x *]] }
MPI Rank 2: 0x2038898: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x2039348: {[Query Value[49292 x *]] }
MPI Rank 2: 0x208afa8: {[S Value[1 x 1]] }
MPI Rank 2: 0x208b3c8: {[N Value[1 x 1]] }
MPI Rank 2: 0x208b7e8: {[G Value[1 x 1]] }
MPI Rank 2: 0x208bd18: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 0x208e6e8: {[SIM Value[51 x *]] }
MPI Rank 2: 0x208f268: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 0x2090b88: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 0x2090ec8: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 0x20913d8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 0x2091598: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 0x2091758: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 0x2091918: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 0x2091ad8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 0x2092398: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x20928d8: {[ce Gradient[1]] }
MPI Rank 2: 0x2092a98: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x2092c58: {[SIM Gradient[51 x *]] }
MPI Rank 2: 0x2092fd8: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 0x2093388: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x2093428: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 05/03/2016 18:19:50: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:52: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:19:52: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:19:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.44945602 * 10240; time = 5.1177s; samplesPerSecond = 2000.9
MPI Rank 2: 05/03/2016 18:20:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.72469749 * 10240; time = 4.5411s; samplesPerSecond = 2255.0
MPI Rank 2: 05/03/2016 18:20:04: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=12.0115s
MPI Rank 2: 05/03/2016 18:20:06: Final Results: Minibatch[1-26]: ce = 2.13785051 * 102399; perplexity = 8.48118778
MPI Rank 2: 05/03/2016 18:20:06: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785051 * 102399
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:09: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:20:13:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.05766125 * 10240; time = 4.6696s; samplesPerSecond = 2192.9
MPI Rank 2: 05/03/2016 18:20:18:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.92501583 * 10240; time = 4.6692s; samplesPerSecond = 2193.1
MPI Rank 2: 05/03/2016 18:20:20: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=11.6723s
MPI Rank 2: 05/03/2016 18:20:23: Final Results: Minibatch[1-26]: ce = 1.81478809 * 102399; perplexity = 6.13977497
MPI Rank 2: 05/03/2016 18:20:23: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478809 * 102399
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:25: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:25: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:20:30:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.82333508 * 10240; time = 4.8144s; samplesPerSecond = 2126.9
MPI Rank 2: 05/03/2016 18:20:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.76784000 * 10240; time = 4.7230s; samplesPerSecond = 2168.1
MPI Rank 2: 05/03/2016 18:20:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639891 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=11.8878s
MPI Rank 2: 05/03/2016 18:20:39: Final Results: Minibatch[1-26]: ce = 1.69809270 * 102399; perplexity = 5.46351687
MPI Rank 2: 05/03/2016 18:20:39: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809270 * 102399
MPI Rank 2: 05/03/2016 18:20:42: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:42: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:42: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 05/03/2016 18:19:50: -------------------------------------------------------------------
MPI Rank 3: 05/03/2016 18:19:50: Build info: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: 		Built time: May  3 2016 17:56:15
MPI Rank 3: 05/03/2016 18:19:50: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 3: 05/03/2016 18:19:50: 		Build type: release
MPI Rank 3: 05/03/2016 18:19:50: 		Build target: GPU
MPI Rank 3: 05/03/2016 18:19:50: 		With 1bit-SGD: no
MPI Rank 3: 05/03/2016 18:19:50: 		Math lib: acml
MPI Rank 3: 05/03/2016 18:19:50: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 05/03/2016 18:19:50: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 05/03/2016 18:19:50: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 05/03/2016 18:19:50: 		Build Branch: HEAD
MPI Rank 3: 05/03/2016 18:19:50: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 3: 05/03/2016 18:19:50: 		Built by philly on 18750d26eb32
MPI Rank 3: 05/03/2016 18:19:50: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 05/03/2016 18:19:50: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: Running on localhost at 2016/05/03 18:19:50
MPI Rank 3: 05/03/2016 18:19:50: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:19:50: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:19:50: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 05/03/2016 18:19:50: Commands: train
MPI Rank 3: 05/03/2016 18:19:50: Precision = "float"
MPI Rank 3: 05/03/2016 18:19:50: Using 6 CPU threads.
MPI Rank 3: 05/03/2016 18:19:50: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 05/03/2016 18:19:50: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 05/03/2016 18:19:50: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: ##############################################################################
MPI Rank 3: 05/03/2016 18:19:50: #                                                                            #
MPI Rank 3: 05/03/2016 18:19:50: # Action "train"                                                             #
MPI Rank 3: 05/03/2016 18:19:50: #                                                                            #
MPI Rank 3: 05/03/2016 18:19:50: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:50: Creating virgin network.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:51: Created model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:51: Training criterion node(s):
MPI Rank 3: 05/03/2016 18:19:51: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x13b5138: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x13b8f78: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x13ba548: {[ce Value[1]] }
MPI Rank 3: 0x13bab98: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x13bacf8: {[WD1 Value[64 x 288]] }
MPI Rank 3: 0x13bb078: {[Query Value[49292 x *]] }
MPI Rank 3: 0x13f8ef8: {[Keyword Value[49292 x *]] }
MPI Rank 3: 0x140d778: {[S Value[1 x 1]] }
MPI Rank 3: 0x140db98: {[N Value[1 x 1]] }
MPI Rank 3: 0x140dfb8: {[G Value[1 x 1]] }
MPI Rank 3: 0x140e4c8: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 0x1410e98: {[SIM Value[51 x *]] }
MPI Rank 3: 0x1411a18: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 0x1413338: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 0x1413678: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 0x1413b88: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 0x1413d48: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 0x1413f08: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 0x14140c8: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 0x1414288: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 0x1414b48: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x1415088: {[ce Gradient[1]] }
MPI Rank 3: 0x1415248: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x1415408: {[SIM Gradient[51 x *]] }
MPI Rank 3: 0x1415788: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 0x1415b38: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x1415bd8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 05/03/2016 18:19:51: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:52: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:19:52: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:19:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.44398956 * 10240; time = 5.1276s; samplesPerSecond = 1997.0
MPI Rank 3: 05/03/2016 18:20:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.71102791 * 10240; time = 4.5411s; samplesPerSecond = 2255.0
MPI Rank 3: 05/03/2016 18:20:04: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=12.0114s
MPI Rank 3: 05/03/2016 18:20:06: Final Results: Minibatch[1-26]: ce = 2.13785051 * 102399; perplexity = 8.48118778
MPI Rank 3: 05/03/2016 18:20:06: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785051 * 102399
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:09: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:20:13:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.00262756 * 10240; time = 4.6696s; samplesPerSecond = 2192.9
MPI Rank 3: 05/03/2016 18:20:18:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90484047 * 10240; time = 4.6691s; samplesPerSecond = 2193.1
MPI Rank 3: 05/03/2016 18:20:20: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=11.6723s
MPI Rank 3: 05/03/2016 18:20:23: Final Results: Minibatch[1-26]: ce = 1.81478809 * 102399; perplexity = 6.13977497
MPI Rank 3: 05/03/2016 18:20:23: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478809 * 102399
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:25: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:25: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:20:30:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.75186119 * 10240; time = 4.8144s; samplesPerSecond = 2126.9
MPI Rank 3: 05/03/2016 18:20:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.73868065 * 10240; time = 4.7230s; samplesPerSecond = 2168.1
MPI Rank 3: 05/03/2016 18:20:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639891 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=11.8878s
MPI Rank 3: 05/03/2016 18:20:39: Final Results: Minibatch[1-26]: ce = 1.69809270 * 102399; perplexity = 5.46351687
MPI Rank 3: 05/03/2016 18:20:39: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809270 * 102399
MPI Rank 3: 05/03/2016 18:20:42: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:42: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:42: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: May  3 2016 17:56:15
		Last modified date: Tue May  3 11:36:22 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
		Built by philly on 18750d26eb32
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
05/03/2016 18:20:42: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank0
05/03/2016 18:20:43: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank1
05/03/2016 18:20:43: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank2
05/03/2016 18:20:44: Redirecting stderr to file /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 0: 05/03/2016 18:20:42: -------------------------------------------------------------------
MPI Rank 0: 05/03/2016 18:20:42: Build info: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: 		Built time: May  3 2016 17:56:15
MPI Rank 0: 05/03/2016 18:20:42: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 0: 05/03/2016 18:20:42: 		Build type: release
MPI Rank 0: 05/03/2016 18:20:42: 		Build target: GPU
MPI Rank 0: 05/03/2016 18:20:42: 		With 1bit-SGD: no
MPI Rank 0: 05/03/2016 18:20:42: 		Math lib: acml
MPI Rank 0: 05/03/2016 18:20:42: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 05/03/2016 18:20:42: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 05/03/2016 18:20:42: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 05/03/2016 18:20:42: 		Build Branch: HEAD
MPI Rank 0: 05/03/2016 18:20:42: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 0: 05/03/2016 18:20:42: 		Built by philly on 18750d26eb32
MPI Rank 0: 05/03/2016 18:20:42: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 05/03/2016 18:20:42: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: Running on localhost at 2016/05/03 18:20:42
MPI Rank 0: 05/03/2016 18:20:42: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:20:42: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 05/03/2016 18:20:42: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 05/03/2016 18:20:42: Commands: train
MPI Rank 0: 05/03/2016 18:20:42: Precision = "float"
MPI Rank 0: 05/03/2016 18:20:42: Using 6 CPU threads.
MPI Rank 0: 05/03/2016 18:20:42: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 05/03/2016 18:20:42: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 05/03/2016 18:20:42: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: ##############################################################################
MPI Rank 0: 05/03/2016 18:20:42: #                                                                            #
MPI Rank 0: 05/03/2016 18:20:42: # Action "train"                                                             #
MPI Rank 0: 05/03/2016 18:20:42: #                                                                            #
MPI Rank 0: 05/03/2016 18:20:42: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:42: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:43: Loaded model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:43: Training criterion node(s):
MPI Rank 0: 05/03/2016 18:20:43: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x1629c48: {[G Value[1 x 1]] }
MPI Rank 0: 0x1633c48: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 0x1634ad8: {[SIM Value[51 x *1]] }
MPI Rank 0: 0x1635848: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 0x16358e8: {[Query Value[49292 x *1]] }
MPI Rank 0: 0x1649628: {[N Value[1 x 1]] }
MPI Rank 0: 0x165d748: {[S Value[1 x 1]] }
MPI Rank 0: 0x165e1c8: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x165ec98: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x165f768: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0x1660258: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x1662db8: {[ce Value[1]] }
MPI Rank 0: 0x16647e8: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 0x1664b28: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 0x1664f18: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 0x16650d8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 0x1665298: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 0x1665458: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 0x1665618: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 0x16657d8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 0x1666098: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x1666608: {[ce Gradient[1]] }
MPI Rank 0: 0x16667c8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x1666988: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 0x1666d08: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 0x1667098: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x1667138: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 05/03/2016 18:20:43: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:20:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 05/03/2016 18:20:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.73774796 * 10240; time = 5.4619s; samplesPerSecond = 1874.8
MPI Rank 0: 05/03/2016 18:20:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.68121185 * 10240; time = 4.7040s; samplesPerSecond = 2176.9
MPI Rank 0: 05/03/2016 18:20:58: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=12.6382s
MPI Rank 0: 05/03/2016 18:21:01: Final Results: Minibatch[1-26]: ce = 1.70737100 * 102399; perplexity = 5.51444493
MPI Rank 0: 05/03/2016 18:21:01: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737100 * 102399
MPI Rank 0: 05/03/2016 18:21:02: SGD: Saving checkpoint model '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 05/03/2016 18:21:03: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:03: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 05/03/2016 18:21:03: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 05/03/2016 18:20:43: -------------------------------------------------------------------
MPI Rank 1: 05/03/2016 18:20:43: Build info: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: 		Built time: May  3 2016 17:56:15
MPI Rank 1: 05/03/2016 18:20:43: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 1: 05/03/2016 18:20:43: 		Build type: release
MPI Rank 1: 05/03/2016 18:20:43: 		Build target: GPU
MPI Rank 1: 05/03/2016 18:20:43: 		With 1bit-SGD: no
MPI Rank 1: 05/03/2016 18:20:43: 		Math lib: acml
MPI Rank 1: 05/03/2016 18:20:43: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 05/03/2016 18:20:43: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 05/03/2016 18:20:43: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 05/03/2016 18:20:43: 		Build Branch: HEAD
MPI Rank 1: 05/03/2016 18:20:43: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 1: 05/03/2016 18:20:43: 		Built by philly on 18750d26eb32
MPI Rank 1: 05/03/2016 18:20:43: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 05/03/2016 18:20:43: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: Running on localhost at 2016/05/03 18:20:43
MPI Rank 1: 05/03/2016 18:20:43: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:20:43: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 05/03/2016 18:20:43: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 05/03/2016 18:20:43: Commands: train
MPI Rank 1: 05/03/2016 18:20:43: Precision = "float"
MPI Rank 1: 05/03/2016 18:20:43: Using 6 CPU threads.
MPI Rank 1: 05/03/2016 18:20:43: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 05/03/2016 18:20:43: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 05/03/2016 18:20:43: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: ##############################################################################
MPI Rank 1: 05/03/2016 18:20:43: #                                                                            #
MPI Rank 1: 05/03/2016 18:20:43: # Action "train"                                                             #
MPI Rank 1: 05/03/2016 18:20:43: #                                                                            #
MPI Rank 1: 05/03/2016 18:20:43: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:43: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:44: Loaded model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:44: Training criterion node(s):
MPI Rank 1: 05/03/2016 18:20:44: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x24afa18: {[G Value[1 x 1]] }
MPI Rank 1: 0x24b5158: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 0x24b90d8: {[N Value[1 x 1]] }
MPI Rank 1: 0x24babd8: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 0x24ced38: {[Query Value[49292 x *1]] }
MPI Rank 1: 0x24cee48: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x24e2b18: {[S Value[1 x 1]] }
MPI Rank 1: 0x24e40f8: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x24e4bc8: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x24e56b8: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x24e6258: {[SIM Value[51 x *1]] }
MPI Rank 1: 0x24e81f8: {[ce Value[1]] }
MPI Rank 1: 0x24e9ba8: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 0x24e9e28: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 0x24ea398: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 0x24ea4f8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 0x24ea6b8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 0x24ea878: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 0x24eaa38: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 0x24eabf8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 0x24eb4b8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x24eba28: {[ce Gradient[1]] }
MPI Rank 1: 0x24ebbe8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x24ebda8: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 0x24ec128: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 0x24ec508: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x24ec5a8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 05/03/2016 18:20:44: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:20:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 05/03/2016 18:20:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.78650436 * 10240; time = 5.4623s; samplesPerSecond = 1874.7
MPI Rank 1: 05/03/2016 18:20:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.77671204 * 10240; time = 4.7040s; samplesPerSecond = 2176.9
MPI Rank 1: 05/03/2016 18:20:58: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=12.6382s
MPI Rank 1: 05/03/2016 18:21:01: Final Results: Minibatch[1-26]: ce = 1.70737100 * 102399; perplexity = 5.51444493
MPI Rank 1: 05/03/2016 18:21:01: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737100 * 102399
MPI Rank 1: 05/03/2016 18:21:03: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:03: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 05/03/2016 18:21:03: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 05/03/2016 18:20:43: -------------------------------------------------------------------
MPI Rank 2: 05/03/2016 18:20:43: Build info: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: 		Built time: May  3 2016 17:56:15
MPI Rank 2: 05/03/2016 18:20:43: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 2: 05/03/2016 18:20:43: 		Build type: release
MPI Rank 2: 05/03/2016 18:20:43: 		Build target: GPU
MPI Rank 2: 05/03/2016 18:20:43: 		With 1bit-SGD: no
MPI Rank 2: 05/03/2016 18:20:43: 		Math lib: acml
MPI Rank 2: 05/03/2016 18:20:43: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 05/03/2016 18:20:43: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 05/03/2016 18:20:43: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 05/03/2016 18:20:43: 		Build Branch: HEAD
MPI Rank 2: 05/03/2016 18:20:43: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 2: 05/03/2016 18:20:43: 		Built by philly on 18750d26eb32
MPI Rank 2: 05/03/2016 18:20:43: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 05/03/2016 18:20:43: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: Running on localhost at 2016/05/03 18:20:43
MPI Rank 2: 05/03/2016 18:20:43: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:20:43: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 05/03/2016 18:20:43: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 05/03/2016 18:20:43: Commands: train
MPI Rank 2: 05/03/2016 18:20:43: Precision = "float"
MPI Rank 2: 05/03/2016 18:20:43: Using 6 CPU threads.
MPI Rank 2: 05/03/2016 18:20:43: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 05/03/2016 18:20:43: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 05/03/2016 18:20:43: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: ##############################################################################
MPI Rank 2: 05/03/2016 18:20:43: #                                                                            #
MPI Rank 2: 05/03/2016 18:20:43: # Action "train"                                                             #
MPI Rank 2: 05/03/2016 18:20:43: #                                                                            #
MPI Rank 2: 05/03/2016 18:20:43: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:43: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:44: Loaded model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:44: Training criterion node(s):
MPI Rank 2: 05/03/2016 18:20:44: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x24eecc8: {[G Value[1 x 1]] }
MPI Rank 2: 0x24f8cc8: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 0x24f9b58: {[SIM Value[51 x *1]] }
MPI Rank 2: 0x24fa8c8: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 0x24fa968: {[Query Value[49292 x *1]] }
MPI Rank 2: 0x250e6a8: {[N Value[1 x 1]] }
MPI Rank 2: 0x25227c8: {[S Value[1 x 1]] }
MPI Rank 2: 0x2523248: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x2523d18: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x25247e8: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x25252d8: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x2527e38: {[ce Value[1]] }
MPI Rank 2: 0x2529868: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 0x2529ba8: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 0x2529f98: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 0x252a158: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 0x252a318: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 0x252a4d8: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 0x252a698: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 0x252a858: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 0x252b118: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x252b688: {[ce Gradient[1]] }
MPI Rank 2: 0x252b848: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x252ba08: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 0x252bd88: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 0x252c118: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x252c1b8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 05/03/2016 18:20:44: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:20:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 05/03/2016 18:20:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.83119640 * 10240; time = 5.4619s; samplesPerSecond = 1874.8
MPI Rank 2: 05/03/2016 18:20:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79677334 * 10240; time = 4.7040s; samplesPerSecond = 2176.9
MPI Rank 2: 05/03/2016 18:20:58: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=12.6382s
MPI Rank 2: 05/03/2016 18:21:01: Final Results: Minibatch[1-26]: ce = 1.70737100 * 102399; perplexity = 5.51444493
MPI Rank 2: 05/03/2016 18:21:01: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737100 * 102399
MPI Rank 2: 05/03/2016 18:21:03: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:03: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 05/03/2016 18:21:03: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 05/03/2016 18:20:44: -------------------------------------------------------------------
MPI Rank 3: 05/03/2016 18:20:44: Build info: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: 		Built time: May  3 2016 17:56:15
MPI Rank 3: 05/03/2016 18:20:44: 		Last modified date: Tue May  3 11:36:22 2016
MPI Rank 3: 05/03/2016 18:20:44: 		Build type: release
MPI Rank 3: 05/03/2016 18:20:44: 		Build target: GPU
MPI Rank 3: 05/03/2016 18:20:44: 		With 1bit-SGD: no
MPI Rank 3: 05/03/2016 18:20:44: 		Math lib: acml
MPI Rank 3: 05/03/2016 18:20:44: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 05/03/2016 18:20:44: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 05/03/2016 18:20:44: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 05/03/2016 18:20:44: 		Build Branch: HEAD
MPI Rank 3: 05/03/2016 18:20:44: 		Build SHA1: 571b092d60e131fd529081a5ed52af2dc815dc82
MPI Rank 3: 05/03/2016 18:20:44: 		Built by philly on 18750d26eb32
MPI Rank 3: 05/03/2016 18:20:44: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 05/03/2016 18:20:44: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: Running on localhost at 2016/05/03 18:20:44
MPI Rank 3: 05/03/2016 18:20:44: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:20:44: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 05/03/2016 18:20:44: modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 05/03/2016 18:20:44: Commands: train
MPI Rank 3: 05/03/2016 18:20:44: Precision = "float"
MPI Rank 3: 05/03/2016 18:20:44: Using 6 CPU threads.
MPI Rank 3: 05/03/2016 18:20:44: CNTKModelPath: /tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 05/03/2016 18:20:44: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 05/03/2016 18:20:44: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: ##############################################################################
MPI Rank 3: 05/03/2016 18:20:44: #                                                                            #
MPI Rank 3: 05/03/2016 18:20:44: # Action "train"                                                             #
MPI Rank 3: 05/03/2016 18:20:44: #                                                                            #
MPI Rank 3: 05/03/2016 18:20:44: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:44: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160503181449.303380/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:45: Loaded model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:45: Training criterion node(s):
MPI Rank 3: 05/03/2016 18:20:45: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x273baf8: {[G Value[1 x 1]] }
MPI Rank 3: 0x2745af8: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 0x2746988: {[SIM Value[51 x *1]] }
MPI Rank 3: 0x27476f8: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 0x2747798: {[Query Value[49292 x *1]] }
MPI Rank 3: 0x275b4d8: {[N Value[1 x 1]] }
MPI Rank 3: 0x276f5f8: {[S Value[1 x 1]] }
MPI Rank 3: 0x2770078: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x2770b48: {[WD1 Value[64 x 288]] }
MPI Rank 3: 0x2771618: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x2772108: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x2774c68: {[ce Value[1]] }
MPI Rank 3: 0x2776698: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 0x27769d8: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 0x2776dc8: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 0x2776f88: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 0x2777148: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 0x2777308: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 0x27774c8: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 0x2777688: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 0x2777f48: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x27784b8: {[ce Gradient[1]] }
MPI Rank 3: 0x2778678: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x2778838: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 0x2778bb8: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 0x2778f48: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x2778fe8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 05/03/2016 18:20:45: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:20:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 05/03/2016 18:20:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.75967293 * 10240; time = 5.4719s; samplesPerSecond = 1871.4
MPI Rank 3: 05/03/2016 18:20:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.76906281 * 10240; time = 4.7040s; samplesPerSecond = 2176.9
MPI Rank 3: 05/03/2016 18:20:58: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=12.6382s
MPI Rank 3: 05/03/2016 18:21:01: Final Results: Minibatch[1-26]: ce = 1.70737100 * 102399; perplexity = 5.51444493
MPI Rank 3: 05/03/2016 18:21:01: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737100 * 102399
MPI Rank 3: 05/03/2016 18:21:03: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:03: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 05/03/2016 18:21:03: __COMPLETED__
MPI Rank 3: ~MPIWrapper
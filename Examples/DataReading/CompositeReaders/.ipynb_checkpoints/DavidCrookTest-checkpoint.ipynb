{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cntk as C\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the data dimensions\n",
    "num_channels = 3\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "\n",
    "input_conv_model_shape = (num_channels, image_width, image_height)    # images are 32 x 32 with 3 channels of color\n",
    "conv_model_num_features = 32\n",
    "input_tab_features = 3\n",
    "num_regression_outputs = 4\n",
    "\n",
    "x_i = C.input_variable(input_conv_model_shape)\n",
    "x_t = C.input_variable(input_tab_features)\n",
    "y = C.input_variable(num_regression_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Format** \n",
    "\n",
    "**The image map file looks like:**\n",
    "\n",
    "  image1.png    0  \n",
    "  image3.png    0\n",
    "\n",
    "**The CTF file looks like:**\n",
    "\n",
    "  |label 1 2 3 4   \n",
    "  |label 4 3 2 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk.io.transforms as xforms \n",
    "\n",
    "# Read a COMPOSITE reader to read data from both the image map and CTF files\n",
    "def create_reader(map_file, ctf_file, is_training, num_regression_outputs):\n",
    "    \n",
    "    # create transforms\n",
    "    transforms = []\n",
    "    \n",
    "    # train uses data augmentation (translation only)\n",
    "    if is_training:\n",
    "        transforms += [ xforms.crop(crop_type='randomside', side_ratio=0.8)  ]\n",
    "        \n",
    "    transforms +=    [\n",
    "        xforms.scale(width=image_width, height=image_height, channels=num_channels, interpolations='linear')]\n",
    "\n",
    "    # create IMAGE DESERIALIZER for map file\n",
    "    feature_source = C.io.ImageDeserializer(map_file, C.io.StreamDefs(\n",
    "        features_image = C.io.StreamDef(field='image', transforms=transforms)))\n",
    "    \n",
    "    # create CTF DESERIALIZER for CTF file\n",
    "    label_source = C.io.CTFDeserializer(ctf_file, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field=\"label\", shape=num_regression_outputs, is_sparse=False),\n",
    "        features_tabular = C.io.StreamDef(field=\"features\", shape=3)))\n",
    "\n",
    "    # create a minibatch source by compositing them together \n",
    "    return C.io.MinibatchSource([feature_source, label_source], max_samples=sys.maxsize, randomize=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to build model\n",
    "def create_model(x_i, x_t):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.relu):\n",
    "            h = x_i\n",
    "            \n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), num_filters=8, strides=(1,1), pad=True, name=\"first_conv\")(h)            \n",
    "            h = C.layers.MaxPooling(filter_shape=(2,2), strides=(2,2), name=\"first_max\")(h)            \n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), num_filters=16, strides=(1,1), pad=True, name=\"second_conv\")(h)            \n",
    "            h = C.layers.MaxPooling(filter_shape=(3,3), strides=(3,3), name=\"second_max\")(h)\n",
    "            \n",
    "            # create a feature map\n",
    "            h = C.layers.Dense(conv_model_num_features, name=\"feature_map\")(h)\n",
    "            \n",
    "            #merge the convolutional feature map with raw tabular data\n",
    "            h = C.splice(h, x_t, axis=0)\n",
    "            \n",
    "            #mix up the data in a dense output sequence\n",
    "            h = C.layers.Dense(conv_model_num_features, name=\"merged_dense_1\")(h)\n",
    "            p = C.layers.Dense(num_regression_outputs, activation = None, name=\"prediction\")(h)\n",
    "            \n",
    "            return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = C.losses.squared_error(model, labels)\n",
    "    errs = loss\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training\n",
    "\n",
    "In the previous tutorials we have described the concepts of `loss` function, the optimizers or [learners](https://cntk.ai/pythondocs/cntk.learners.html) and the associated machinery needed to train a model. Please refer to earlier tutorials for gaining familiarility with these concepts. In this tutorial, we combine model training and testing in a helper function below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, num_sweeps_to_train_with=10):\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(z, y)\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.00001\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(z.parameters, lr_schedule)\n",
    "    trainer = C.Trainer(z, (loss, label_error), [learner])\n",
    "    \n",
    "    # Initialize the parameters for the trainer\n",
    "    minibatch_size = 64\n",
    "    num_samples_per_sweep = 60000\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    # Map the data streams to the input and labels.\n",
    "    input_map={\n",
    "        y  : train_reader.streams.labels,\n",
    "        x_i  : train_reader.streams.features_image,\n",
    "        x_t  : train_reader.streams.features_tabular\n",
    "    } \n",
    "    \n",
    "    training_progress_output_freq = 500\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        # Read a mini batch from the training data file\n",
    "        data=train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        \n",
    "        if (i==0):\n",
    "            print(\"data=\", data)\n",
    "        \n",
    "        # train with this minibatch\n",
    "        trainer.train_minibatch(data)\n",
    "        \n",
    "        # print progress \n",
    "        print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "     \n",
    "    # Print training time\n",
    "    print(\"Training took {:.1f} sec\".format(time.time() - start))\n",
    "    \n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        y  : test_reader.streams.labels,\n",
    "        x_i  : test_reader.streams.features_image,\n",
    "        x_t : test_reader.streams.features_tabular\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 512\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "\n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data= {Input('Input3', [#], [3 x 32 x 32]): MinibatchData(data=Value([64 x 1 x 3 x 32 x 32], GPU), samples=64, seqs=64), Input('Input5', [#], [4]): MinibatchData(data=Value([64 x 1 x 4], GPU), samples=64, seqs=64), Input('Input4', [#], [3]): MinibatchData(data=Value([64 x 1 x 3], GPU), samples=64, seqs=64)}\n",
      "Minibatch: 0, Loss: 22050.9980, Error: 2205099.80%\n",
      "Minibatch: 500, Loss: 27.9702, Error: 2797.02%\n",
      "Minibatch: 1000, Loss: 26.6438, Error: 2664.38%\n",
      "Minibatch: 1500, Loss: 25.3646, Error: 2536.46%\n",
      "Minibatch: 2000, Loss: 24.1232, Error: 2412.32%\n",
      "Minibatch: 2500, Loss: 22.9158, Error: 2291.58%\n",
      "Minibatch: 3000, Loss: 21.7319, Error: 2173.19%\n",
      "Minibatch: 3500, Loss: 20.5669, Error: 2056.69%\n",
      "Minibatch: 4000, Loss: 19.4209, Error: 1942.09%\n",
      "Minibatch: 4500, Loss: 18.2953, Error: 1829.53%\n",
      "Minibatch: 5000, Loss: 17.2099, Error: 1720.99%\n",
      "Minibatch: 5500, Loss: 16.1485, Error: 1614.85%\n",
      "Minibatch: 6000, Loss: 15.1144, Error: 1511.44%\n",
      "Minibatch: 6500, Loss: 14.1117, Error: 1411.17%\n",
      "Minibatch: 7000, Loss: 13.1446, Error: 1314.46%\n",
      "Minibatch: 7500, Loss: 12.2283, Error: 1222.83%\n",
      "Minibatch: 8000, Loss: 11.3593, Error: 1135.93%\n",
      "Minibatch: 8500, Loss: 10.5346, Error: 1053.46%\n",
      "Minibatch: 9000, Loss: 9.7633, Error: 976.33%\n",
      "Training took 31.3 sec\n",
      "Average test error: 926.62%\n"
     ]
    }
   ],
   "source": [
    "z = create_model(x_i, x_t)\n",
    "reader_train = create_reader(\"train.map\", \"train.ctf\", True, num_regression_outputs)\n",
    "reader_test = create_reader(\"test.map\", \"test.ctf\", False, num_regression_outputs)\n",
    "\n",
    "train_test(reader_train, reader_test, num_sweeps_to_train_with = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "your graph has 2 inputs, but you specified only one",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-920efadcc58c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labels=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpredict4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict4=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\cntk\\ops\\functions.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, arguments, outputs, device, as_numpy)\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_variable_value_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\cntk\\internal\\swig_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\cntk\\ops\\functions.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, arguments, outputs, keep_for_backward, device, as_numpy)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         in_var_map = sanitize_var_map(self.arguments, arguments,\n\u001b[1;32m--> 750\u001b[1;33m                                       None, device)\n\u001b[0m\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\cntk\\internal\\sanitize.py\u001b[0m in \u001b[0;36msanitize_var_map\u001b[1;34m(op_arguments, arguments, precision, device, extract_values_from_minibatch_data)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_arguments\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             raise ValueError('your graph has %i inputs, but you specified '\n\u001b[1;32m--> 367\u001b[1;33m                              'only one' % len(op_arguments))\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0marguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mop_arguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marguments\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: your graph has 2 inputs, but you specified only one"
     ]
    }
   ],
   "source": [
    "# perform manual prediction (for debugging)\n",
    "reader_predict = create_reader(\"test.map\", \"test.ctf\", False, num_regression_outputs)\n",
    "predict_map = { y  : reader_predict.streams.labels,  x_i  : reader_predict.streams.features_image, x_t : reader_predict.streams.features_tabular    }\n",
    "\n",
    "data = reader_predict.next_minibatch(1)\n",
    "#print(\"data=\", data)\n",
    "\n",
    "for dd in data:\n",
    "    d = data[dd].data\n",
    "    s = np.shape(d)\n",
    "    if (s[2] == 4):\n",
    "        lab = d.data.asarray().reshape(4)\n",
    "        print(\"labels=\", lab)\n",
    "    if (len(s) == 5):\n",
    "        predict4 = z.eval(d)\n",
    "        print(\"predict4=\", predict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.data.asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

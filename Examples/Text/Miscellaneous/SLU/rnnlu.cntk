# configuration file for CNTK ATIS for language understanding tasks

precision="float"
deviceId = $DeviceId$
command=LSTM:LSTMTest
traceLevel=1

LSTM=[
    action=train
	makeMode=true

    # output model path
    modelPath=$ExpDir$/cntkdebug.dnn

    # uncomment NDLNetworkBuilder to use NDL
    # need to comment out SimpleNetworkBuilder section
    #    NDLNetworkBuilder=[
    #        networkDescription=$NdlDir$\lstmNDL.txt
    #    ]

    SimpleNetworkBuilder=[
        rnnType=LSTM

        # first layer, second layer, and output layer size
        layerSizes=2832:50:300:300:300:127

        lookupTableOrder=3
        recurrentLayer=2:3:4

        # default hidden layer activity
        defaultHiddenActivity=0.1

        # randomization range
        initValueScale=6.0

        trainingCriterion=crossentropywithsoftmax
        evalCriterion=crossentropywithsoftmax
    ]

    # configuration file, base parameters
    SGD=[
        # maximum number of epochs
        maxEpochs=100   #00

        # for each epoch, maximum number of input words is set below
        epochSize=71524     #4430000

        # minibatchSize should be larger than the maximum sentence length
        minibatchSize=10000

        learningRatesPerSample=0.1
        momentumAsTimeConstant=1000

        gradientClippingWithTruncation=true
        clippingThresholdPerSample=15.0

        # for information purpose, number of minibatches to report progress
        numMBsToShowResult=50

        # if validation shows that the model has no improvement, then do back-up to the previously
        # estimated model and reduce learning rate
        loadBestModel=true

        # settings for Auto Adjust Learning Rate
        AutoAdjust=[
            # auto learning rate adjustment
            autoAdjustLR=adjustafterepoch
            reduceLearnRateIfImproveLessThan=0
            increaseLearnRateIfImproveMoreThan=1000000000

            # how much learning rate is reduced
            learnRateDecreaseFactor=0.5

            # if continously improved, can increase learning rate by the following ratio
            learnRateIncreaseFactor=1.382
        ]
    ]

    reader=[
        # reader to use
        readerType=LUSequenceReader
        
        #TODO: use negative numbers
        wordContext=0:1:2
        randomize=None

        # number of utterances to be allocated for each minibatch
        nbruttsineachrecurrentiter=10  #50

        # do not require utterances in a minibatch to be of equal length
        equalLength = false

        # if writerType is set, we will cache to a binary file
        # if the binary file exists, we will use it instead of parsing this file
        # writerType=BinaryReader

        #### write definition
        wfile=$ExpDir$/sequenceSentence.bin
        #wsize - inital size of the file in MB
        # if calculated size would be bigger, that is used instead
        wsize=256

        #wrecords - number of records we should allocate space for in the file
        # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
        wrecords=1000
        #windowSize - number of records we should include in BinaryWriter window
        windowSize=10000

        unk="<unk>"
          wordmap=$DataDir$/inputmap.txt
          file=$DataDir$/atis.train.apos.pred.pos.head.IOB.simple

          #additional features sections
          #for now store as expanded category data (including label in)
          features=[
            # sentence has no features, so need to set dimension to zero
            dim=0
            ### write definition
            sectionType=data
          ]
          # sequence break table, list indexes into sequence records, so we know when a sequence starts/stops
          sequence=[
            dim=1
            wrecords=2
            ### write definition
            sectionType=data
          ]
          #labels sections
          labelIn=[
            dim=1
            usewordmap=true

            # vocabulary size
            labelDim=10000
            labelMappingFile=$ExpDir$/sentenceLabels.txt
            labelType=Category
            beginSequence="BOS"
            endSequence="EOS"
            usewordmap=true

            # input word list
            token=$DataDir$/input.txt
            
            #### Write definition ####
            # sizeof(unsigned) which is the label index type
            elementSize=4
            sectionType=labels
            mapping=[
              #redefine number of records for this section, since we don't need to save it for each data record
              wrecords=11
              #variable size so use an average string size
              elementSize=10
              sectionType=labelMapping
            ]
            category=[
              dim=11
              #elementSize=sizeof(ElemType) is default
              sectionType=categoryLabels
            ]
          ]
          #labels sections
          labels=[
            dim=1
            labelType=Category

            # output token list
            token=$DataDir$/output.txt

            labelMappingFile=$ExpDir$/sentenceLabels.out.txt
            #### Write definition ####
            # sizeof(unsigned) which is the label index type
            sectionType=labels
            mapping=[
              sectionType=labelMapping
            ]
            category=[
              sectionType=categoryLabels
            ]
          ]
    ]

    cvReader=[
      # reader to use
      readerType=LUSequenceReader
      randomize=None
      wordContext=0:1:2
      
      # if writerType is set, we will cache to a binary file
      # if the binary file exists, we will use it instead of parsing this file
      # writerType=BinaryReader

      # for CV we can batch sequences as many as GPU memory will hold
      nbruttsineachrecurrentiter=200

      equalLength = false

      #### write definition
      wfile=$ExpDir$/sequenceSentence.valid.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      unk="<unk>"
      wordmap=$DataDir$/inputmap.txt
      file=$DataDir$/atis.dev.IOB.simple

      #additional features sections
      #for now store as expanded category data (including label in)
      features=[
        # sentence has no features, so need to set dimension to zero
        dim=0
        ### write definition
        sectionType=data
      ]
      # sequence break table, list indexes into sequence records, so we know when a sequence starts/stops
      sequence=[
        dim=1
        wrecords=2
        ### write definition
        sectionType=data
      ]
      #labels sections
      # it should be the same as that in the training set
      labelIn=[
        dim=1

        # vocabulary size
        labelDim=10000
        labelMappingFile=$ExpDir$/sentenceLabels.in.txt
        labelType=Category
        beginSequence="BOS"
        endSequence="EOS"
        usewordmap=true

        token=$DataDir$/input.txt

        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping=[
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=11
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category=[
          dim=11
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
      ]
      #labels sections
      labels=[
        dim=1
        labelType=Category

        token=$DataDir$/output.txt

        labelDim=10000
        labelMappingFile=$ExpDir$/sentenceLabels.out.txt
        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping=[
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=3
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category=[
          dim=3
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
      ]
    ]
]

# set output files path
# set the nodes for outputs
# for LSTM
# accuracy:  98.16%; precision:  94.37%; recall:  94.57%; FB1:  94.47
 
LSTMTest=[
    action=write

    epochSize=4430000
    # which is 886 * 5000
    #recurrentLayer=1
    defaultHiddenActivity=0.1

    modelPath=$ExpDir$/cntkdebug.dnn

    outputNodeNames=outputs:labels

    reader=[
      # reader to use
      readerType=LUSequenceReader
      randomize=None
      wordContext=0:1:2
      unk="<unk>"
      wordmap=$DataDir$/inputmap.txt
      file=$DataDir$/atis.test.apos.pred.pos.head.IOB.simple

      # if writerType is set, we will cache to a binary file
      # if the binary file exists, we will use it instead of parsing this file
      # writerType=BinaryReader

      #### write definition
      wfile=$ExpDir$/sequenceSentence.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      #additional features sections
      #for now store as expanded category data (including label in)
      features=[
        # sentence has no features, so need to set dimension to zero
        dim=0 
        ### write definition
        sectionType=data
      ]
      # sequence break table, list indexes into sequence records, so we know when a sequence starts/stops
      sequence=[
        dim=1
        wrecords=2
        ### write definition
        sectionType=data
      ]
      #labels sections
      labelIn=[
        dim=1

        # vocabulary size
        labelDim=10000
        labelMappingFile=$ExpDir$/sentenceLabels.txt
        labelType=Category
        beginSequence="BOS"
        endSequence="EOS"
        usewordmap=true

        token=$DataDir$/input.txt

        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping=[
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=11
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category=[
          dim=11
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
      ]
      #labels sections
      labels=[
        dim=1
        labelType=Category
        beginSequence="BOS"
        endSequence="EOS"

        token=$DataDir$/output.txt

        # vocabulary size
        labelDim=127

        labelMappingFile=$ExpDir$/sentenceLabels.out.txt
        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping=[
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=3
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category=[
          dim=3
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
      ]
    ]

    writer=[
        writerType=LUSequenceWriter

        outputs=[
            file=$OutDir$/output.rec.txt
            token=$DataDir$/output.txt
        ]
        
        labels=[
            file=$OutDir$/output.lbl.txt
            token=$DataDir$/output.txt
        ]
    ]
]


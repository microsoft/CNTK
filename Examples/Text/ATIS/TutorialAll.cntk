# The configuration file to build language understanding model with ATIS corpus.
# An LSTM model is built to tag each word in sentences with its semantic label.

makeMode = false ; traceLevel = 1 ; deviceId = -1

WorkDir = Work
DataDir = Data

modelPath = $WorkDir$/slot.model
parallelTrain = true

#stderr = $WorkDir$/log

command = TrainATIS:RunATIS:EvalATIS

vocabSize = 943    # number of words
numLabels = 129    # number of slot labels
numIntents = 26    # number of intent labels

# The command to train the LSTM model
TrainATIS = [
    action = "train"
    BrainScriptNetworkBuilder = [
        inputDim = $vocabSize$
        labelDim = $numLabels$
        embDim = 150
        hiddenDim = 300
        #hiddenDim = 150

        model = Sequential (
            Parallel ((DelayLayer{T=1} : Identity : DelayLayer{T=-1}), Splice) :  # 3-word window
            EmbeddingLayer {embDim} :                                             # embedding
            RecurrentLSTMLayer {hiddenDim} :              # LSTM
            #Parallel ((RecurrentLSTMLayer {hiddenDim} : RecurrentLSTMLayer {hiddenDim, goBackwards=true}), Splice) :  # bidirectional LSTM
            #Parallel ((RecurrentLSTMLayer {hiddenDim} : RecurrentLSTMLayer {hiddenDim, goBackwards=true}), Splice) :  # bidirectional LSTM
            DenseLayer {labelDim, initValueScale=7}        # output layer
        )

        # features
        query      = Input {inputDim}
        slotLabels = Input {labelDim}

        # model application
        z = model (query)

        # loss and metric
        ce   = CrossEntropyWithSoftmax (slotLabels, z)
        errs = ErrorPrediction         (slotLabels, z)

        featureNodes    = (query)
        labelNodes      = (slotLabels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    ]
    # rename this to BrainScriptNetworkBuilder to switch to intent-classification task
    Intent_BrainScriptNetworkBuilder = [
        inputDim = $vocabSize$
        labelDim = $numIntents$
        embDim = 150
        #hiddenDim = 300
        hiddenDim = 150

        model = Sequential (
            Parallel ((DelayLayer{T=1} : Identity : DelayLayer{T=-1}), Splice) :  # 3-word window
            EmbeddingLayer {embDim} :                                             # embedding
            RecurrentLSTMLayer {hiddenDim} : BS.Sequences.Last :                  # LSTM state, final state
            #Parallel ((Sequential (RecurrentLSTMLayer {hiddenDim}                   : BS.Sequences.Last):
                       Sequential (RecurrentLSTMLayer {hiddenDim, goBackwards=true} : BS.Sequences.First)), Splice) :  # bidirectional LSTM
            DenseLayer {labelDim, initValueScale=7}        # output layer
        )

        # features
        t = DynamicAxis{}
        query        = Input {inputDim, dynamicAxis=t}
        intentLabels = Input {labelDim}

        # model application
        z = model (query)

        # loss and metric
        ce   = CrossEntropyWithSoftmax (intentLabels, z)
        errs = ErrorPrediction         (intentLabels, z)

        featureNodes    = (query)
        labelNodes      = (intentLabels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    ]

    SGD = [
        maxEpochs = 20 ; epochSize = 36000

        minibatchSize = 70

        learningRatesPerSample = 0.01*2:0.005*12:0.001

        gradUpdateType = "FSAdaGrad"

        gradientClippingWithTruncation = true ; clippingThresholdPerSample = 15.0

        # number of minibatches to report progress
        firstMBsToShowResult = 10 ; numMBsToShowResult = 100

        parallelTrain = [
            parallelizationMethod = "DataParallelSGD"
            parallelizationStartEpoch = 2
            distributedMBReading = true
            dataParallelSGD = [
                gradientBits = 1
            ]
        ]
    ]

    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.train.ctf"
        randomize = true
        input = [
            query        = [ alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" ]
            intentLabels = [ alias = "S1" ; dim = $numIntents$ ; format = "sparse" ]
            slotLabels   = [ alias = "S2" ; dim = $numLabels$ ;  format = "sparse" ]
        ]
    ]
]

# Run the model to predict slot labels
RunATIS = [
    action = "write"
    BrainScriptNetworkBuilder = [
        modelAsTrained = BS.Network.Load ("$modelPath$")
        final = Hardmax (modelAsTrained.z)  # make a decision
        #labels = Pass (modelAsTrained.slotLabels)
        # enable this for intent classification:
        labels = Pass (modelAsTrained.intentLabels)
        t = DynamicAxis()
    ]

    outputPath = $WorkDir$/model.writeaction
    outputNodeNames = intentLabels:slotLabels:final

    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.test.ctf"
        randomize = false
        input = [
            query        = [ alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" ]
            intentLabels = [ alias = "S1" ; dim = $numIntents$ ; format = "sparse" ]
            slotLabels   = [ alias = "S2" ; dim = $numLabels$ ;  format = "sparse" ]
        ]
    ]
]

# Evaluate the model's slot-tagging accuracy (as an error count)
EvalATIS = [
    action = "eval"
    modelPath = $modelPath$  # from outside
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.test.ctf"
        randomize = false
        input = [
            query        = [ alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" ]
            intentLabels = [ alias = "S1" ; dim = $numIntents$ ; format = "sparse" ]
            slotLabels   = [ alias = "S2" ; dim = $numLabels$ ;  format = "sparse" ]
        ]
    ]
]

# TASK
# configFile=$(SolutionDir)Examples/Text/PennTreebank/Config/S2SAutoEncoder.cntk  RunRootDir=$(SolutionDir)Examples/Text/PennTreebank
# configFile=$(SolutionDir)Examples/Text/PennTreebank/Config/S2SAutoEncoder.cntk  RunRootDir=$(SolutionDir)g2p
####################
# WORK IN PROGRESS #
# WORK IN PROGRESS #
# WORK IN PROGRESS #
####################

makeMode = false

# Command line to run in debugger:
# configFile=$(SolutionDir)Examples/Text/PennTreebank/Config/S2SAutoEncoder.cntk  RunRootDir=$(SolutionDir)Examples/Text/PennTreebank  train=[SGD=[maxEpochs=1]]  confVocabSize=1000  DeviceId=-1  makeMode=false

# directory defaults (if not overridden)
RunRootDir = "../.."             # default if not overridden
DataDir    = "$RunRootDir$/Data"
CacheDir   = "$DataDir$/cache"   # (not used currently)
ExpRootDir = "$RunRootDir$"

command = train:test:write
#command = writeWordAndClassInfo:train:test:write
#command = write
#command = dump

# experiment id

beamDepth = 1                    # 0=predict; 1=greedy; >1=beam

# 30-5: same as 29-5 but rerun with logging of stabilizers
# 29-3: same as 29-5, repro test
# 29-5: same as 29-4 but right-to-left encoder
# 29-4: trying once more with new reader, MB size 70, LR changed to 0.0035,then 0.002; shared stab weights in LSTMP
# 29-2: switched to new reader again, emulating 69-dim outputs  --gives comparable 'ce', but not comparable convergence; SOME att weights are totally flat
# 29-1: same as 29-0 but also switched back to 128 MBSize  --also GLITCH??
# 29-0: switched back to old reader   --not quite the same :( what am I missing?
# 28-5: like 28-4 but using default axis for labels  --minor glitch, got worse
# 28-4: like 28-3 but with momentum changed from 2500 to 1250 (since applied to different #samples)  --GLITCH
# 28-3: like 28-2 but with randomization enabled
# 28-2: like 28-0 but after yet another reader fix   --GLITCH
# 28-1: like 28-0 but halving the MB size--CNTKTextFormatReader interprets the length differently
# 28-0: CNTK reader after data-format fix
# 27-5: trying CNTK reader again after Ryan's bug fix --data format bad
# 27-4: back to LMSequenceReader (regression test)
# 27-3: used </s> for sent end
# 27-2: some refactoring, went back to 26-4 from LMSequenceReader
# 27-1: fixed slicing
# 27-0: incorrect slicing, dropped first input
deviceId = 5
ExpId = 30-$deviceId$-g2p
#ExpId = 22-3-g2p # for decoding a different model
decodeModel = 21
#ExpId = 41-$deviceId$-s2sae    # TASK

hiddenDim = 512  #420 # 1024 # 200
maxLayer = 2
isBidirectional=false
#hiddenDim = 256 # 1024 # 200
#maxLayer = 2
#isBidirectional=true

# directories
ExpDir    = "$ExpRootDir$/$ExpId$"
ModelDir  = "$ExpDir$/Models"

stderr = $ExpDir$/S2SAutoEncoder.log1

precision  = "float"
traceLevel = 1
modelPath  = "$ModelDir$/S2SAutoEncoder.dnn"

# decoding config
decodeModelPath = "$modelPath$.$decodeModel$" # epoch to decode appended
decodeOutputPath = "$decodeModelPath$.bl$beamDepth$"

dumpModelPath = "$modelPath$.2" # model to dump if needed

#confVocabSize = 10000
#confClassSize = 50
#inputVocabSize = $confVocabSize$
#labelVocabSize = $confVocabSize$
#maxLength = 84
#isAutoEncoder=true
#
#trainFile = "ptb.train.txt"
##trainFile = "ptb.small.train.txt"
#validFile = "ptb.valid.txt"
##validFile = "ptb.small.valid.txt"
#testFile  = "ptb.test.txt"
##testFile  = "ptb.test.txt-econ1"
##testFile = "ptb.small.train.txt" # test on train, to see whether model makes sense at all
#startSymbol = "</s>"
#useCNTKTextFormatReader = false
#shareEmbeddings = true

confClassSize = 0
maxLength = 20
isAutoEncoder=false

#inputVocabSize = 69
#labelVocabSize = 69
#trainFile = "g014b2b.train-dev-20-21.bsf.joint"
##trainFile = "g014b2b.train-dev-1-21.bsf.joint" # small one for debugging
#validFile = "g014b2b.train-dev-1-21.bsf.joint"
#testFile  = "g014b2b.test.bsf.joint"
##testFile  = "g014b2b.test.bsf.joint.masked"
#startSymbol = "<s>"
#useCNTKTextFormatReader = false
#shareEmbeddings = true

inputVocabSize = 29     # 26 letters plus start, end, apostrophe
labelVocabSize = 41     # 39 phonemes (~AX missing), plus start and end symbol (in index 0)
trainFile = "g014b2b.train-dev-20-21.bsf.ctf"
#trainFile = "g014b2b.train-dev-1-21.bsf.ctf" # small one for debugging
validFile = "g014b2b.train-dev-1-21.bsf.ctf"
testFile  = "g014b2b.test.bsf.ctf"
#testFile  = "g014b2b.test.bsf.ctf.masked"   # not existing yet
startSymbol = "<s>"
useCNTKTextFormatReader = true
shareEmbeddings = false

#######################################
#  network definition                 #
#######################################

BrainScriptNetworkBuilder = (new ComputationNetwork [

    enableTracing = true
    traceFrequency = 100
    tracingLabelMappingFile = "$ModelDir$/vocab.wl"
    beamDepth=3 // for above Trace macros only, need to clean that up
    include "S2SLib.bs"

    # import general config options from outside config values
    inputVocabDim = $inputVocabSize$
    labelVocabDim = $labelVocabSize$
    nbrClass = $confClassSize$          # for labels only; not functional currently

    isAutoencoder = $isAutoEncoder$     # input is only one sequence, meant to reproduce itself
    attentionSpan = $maxLength$         # 0 to disable. We only support fixed-size attention windows for now. 0 means no attention; exactly 20 is needed for the g2p CMUDict task
    useBidirectionalEncoder = $isBidirectional$ # bi-directional LSTM for encoder

    useStabilizer = true
    useEncoder    = true                # if false, this becomes a regular RNN
    useNYUStyle   = false               # if true use thought vector for all inputs, NYU-style

    # import some names
    Parameters = BS.Parameters
    Constants  = BS.Constants
    Sequences  = BS.Sequences
    Loop       = BS.Loop
    Boolean    = BS.Boolean

    # dimensions
    inputEmbeddingDim = inputVocabDim # 300
    labelEmbeddingDim = labelVocabDim # 300
    shareEmbeddings = $shareEmbeddings$
    hiddenDim    = $hiddenDim$
    attentionDim = 128 # dim of attention  projection
    maxLayer = $maxLayer$

    encoderDims[i:0..maxLayer] = hiddenDim # this defines the number of hidden layers in each
    decoderDims[i:0..maxLayer] = hiddenDim # both are one LSTM layer only for now

    # inputs
    # Inputs must be defined on top-scope level in order to get a clean name.
    useCNTKTextFormatReader = $useCNTKTextFormatReader$
    #input = if !useCNTKTextFormatReader then Input (inputVocabDim, tag='feature') else Fail("'input' defined when using the CNTKTextFormatReader") # LMSequenceReader

    inputAxis = DynamicAxis()
    //labelsAxis = DynamicAxis()
    src = /*Sparse*/Input (inputVocabDim, dynamicAxis=inputAxis) # CNTKTextFormatReader  --TODO: may need to guard as well
    tgt = /*Sparse*/Input ($labelVocabSize$/*labelVocabDim*//*, dynamicAxis=labelsAxis*/)

    # get out input and label data
    # This will go away once we can switch to CNTKTextFormatReader.
    streams = [
        rawInput = input
        out = if isAutoencoder
        then [
            # for an auto-encoder, both are the same
            input  = rawInput
            # strip separators
            labels = Slice (1,  0, rawInput,  axis=-1)  # e.g. A   B   C   </s>
            labelSentenceStart = First (rawInput)
        ]
        else if useCNTKTextFormatReader then [
            input  = TraceSparse (src, 'inp')
            #tgt1 = RowStack (tgt : ConstantTensor (0, labelVocabDim-$labelVocabSize$))   # pad with zeroes, just like the LMSequenceReader
            labels = TraceSparse (    Slice (1,  0, tgt,  axis=-1),   'lbl')  # e.g. A   B   C   </s>
            labelSentenceStart = First (tgt)
        ]
        else [
            # we encode input and label as a single input; this splits it into two
            # This dance will become unnecessary once the new Reader API is fully hooked in.
            separatorRow = 2                                                                          # row index of separator symbokl 
            isSeparator = RowSlice (separatorRow, 1, rawInput)                                        # cut out the separator as a flag
            inInput  = Boolean.Or (FutureValue (1, inInput , defaultHiddenActivation=0), isSeparator) # flag sequence: word is input...
            inLabels = Boolean.Or (PastValue   (1, inLabels, defaultHiddenActivation=0), isSeparator) # ...or labels
            input   = Sequences.Gather (inInput,  rawInput)                                           # use flags to split raw input into input and labels
            labels1 = Sequences.Gather (inLabels, rawInput)                                           # (both have different lengths)
            # strip separators
            labels  = Slice (1,  0, labels1,  axis=-1)  # e.g. A   B   C   </s>
            labelSentenceStart = First (labels1)
        ]
    ].out
    inputSequence  = Pass (streams.input)                   # e.g. <s> A   B   C    </s>
    labelSequence  = Pass (streams.labels)                  # e.g. A   B   C   </s>
    labelSentenceStart = First (streams.labelSentenceStart) # e.g. <s>

    inputSequenceDim = inputVocabDim # TODO: they are the same; but route these through the struct above
    labelSequenceDim = labelVocabDim

    # helpers   --TODO: move to CNTK.core.bs
    First (x) = Slice (0,  1, x,  axis=-1)
    Last (x)  = Slice (-1, 0, x,  axis=-1)

    isFirstLabel = Loop.IsFirst (labelSequence)

    # embeddings  --as long as we cannot read multiple sequences, we got one embedding
    # Note: Embeddings are linear, so better stabilize. We really should use BatchNorm.

    # note: this is assumed to be applied transposed, hence the swapped dimensions. Actually--why? Still needed?
    Einput  =                                     Parameters.WeightParam (inputSequenceDim, inputEmbeddingDim)
    Elabels = if shareEmbeddings then Einput else Parameters.WeightParam (labelSequenceDim, labelEmbeddingDim)
    EmbedInput (x)  = if inputSequenceDim == inputEmbeddingDim then x else TransposeTimes (Einput, x)
    EmbedLabels (x) = if labelSequenceDim == labelEmbeddingDim then x else TransposeTimes (Elabels, x)

    inputEmbedded  = EmbedInput  (inputSequence)
    labelsEmbedded = EmbedLabels (labelSequence)
    #labelSentenceStartEmbedded = EmbedLabels (labelSentenceStart)
    labelSentenceStartEmbedded = Pass (EmbedLabels (labelSentenceStart))  # TODO: remove Pass() if not actually needed in decoder
    labelSentenceStartEmbeddedScattered = BS.Sequences.Scatter (isFirstLabel, labelSentenceStartEmbedded) # unfortunately needed presently

    S(x) = Parameters.Stabilize (x, enabled=useStabilizer)

    #############################################################
    # encoder (processes inputEmbedded)
    #############################################################

    # TODO: do not reverse our inputs; instead, if needed, use a backwards-running loop here
    encoderFunction = if useBidirectionalEncoder then BS.RNNs.RecurrentBirectionalLSTMPStack else BS.RNNs.RecurrentLSTMPStack
    encoder = encoderFunction (encoderDims, cellDims=encoderDims, S(inputEmbedded), inputDim=inputEmbeddingDim,
        previousHook=BS.RNNs.NextHC,
        enableSelfStabilization=useStabilizer)
    encoderOutput = encoder[Length (encoderDims)-1]

    # that last frame should be fed as an additional input to every decoder step
    # Three ways of passing encoder state:
    #  1. as initial state for decoder (Google style)
    #  2. as side information for every decoder step (NYU style)
    #  3. attention

    thoughtVector = [
        h = Last (encoderOutput.h)
        c = Last (encoderOutput.c)
        dim = encoderOutput.dim
    ]

    thoughtVectorPadded = [ # padded with zeroes until end of target sequence
        h = Sequences.BroadcastSequenceAs (labelsEmbedded, thoughtVector.h)
        c = Sequences.BroadcastSequenceAs (labelsEmbedded, thoughtVector.c)
        dim = thoughtVector.dim
    ]

    # NYU style: expand h to all, drop c
    # TODO: just use use thoughtVectorPadded.h (do this when we next test this branch again)
    thoughtVectorEverywhere = Boolean.If (Loop.IsFirst (thoughtVectorPadded.h),    # if first entry
                                 /*then*/ thoughtVectorPadded.h,                   # then copy that
                                 /*else*/ Loop.Previous (thoughtVectorEverywhere)) # else just propagate to the front
    # TODO: use thoughtVectorPadded.h  --TODO: use the new LSTM with augmentation

    # decoder
    # NYU style:
    # The decoder starts with hidden state 0
    # and takes as input [thoughtVectorEverywhere; previous word].

    # we bake into the LSTMs to multiply h and c with beamSearchReorderHook, which we will patch in decoding
    # ReorderTopN (past_h_or_c) = Times (TraceState (past_h_or_c, 'past'), TraceDense (tokens.from, 'backp'))

    #############################################################
    # decoder
    #############################################################

    beamSearchReorderHook = Pass (Constants.OnesTensor (1:1))

    # helper functions to delay h and c with possibility to later hook in a different matrix

    PreviousHCFromThoughtVectorWithReorderingHook (lstmState) = [ # with thought vector and beam-search hook
       isFirst = Loop.IsFirst (initialState.h)
       # BUGBUG: Should be thoughtVector, but Scatter() can't expand from inside a loop
       h = Boolean.If (isFirst, thoughtVectorPadded.h, Loop.Previous (lstmState.h * beamSearchReorderHook))             // hidden state(t-1)
       c = Boolean.If (isFirst, thoughtVectorPadded.c, Loop.Previous (lstmState.c * beamSearchReorderHook))             // cell(t-1)
       dim = lstmState.dim
    ]

    PreviousHCWithReorderingHook (lstmState) = [
       h = Loop.Previous (lstmState.h * beamSearchReorderHook)             // hidden state(t-1)
       c = Loop.Previous (lstmState.c * beamSearchReorderHook)             // cell(t-1)
       dim = lstmState.dim
    ]

    decoderHistoryFromGroundTruth = labelsEmbedded              # decoder input for training is ground truth...
    decoderHistoryFromOutput = Pass (EmbedLabels (Hardmax (z))) # ...but for (greedy) decoding, the decoder's output is its input

    # during training, we use ground truth. For decoding, we will rewire decoderHistoryHook = decoderHistoryFromOutput
    decoderHistoryHook = Pass (decoderHistoryFromGroundTruth) # this gets redirected in decoding to feed back decoding output instead

    decoderInput    = Pass (Boolean.If (isFirstLabel/*Loop.IsFirst (labelSentenceStartEmbeddedScattered)*/, labelSentenceStartEmbeddedScattered, Loop.Previous (decoderHistoryHook)))
    decoderInputDim = labelEmbeddingDim #labelsEmbedded.dim

    decoderDynamicAxis = labelsEmbedded
    FixedWindowAttentionHook = BS.Seq2Seq.CreateAugmentWithFixedWindowAttentionHook (attentionDim, attentionSpan, decoderDynamicAxis, encoderOutput, enableSelfStabilization=useStabilizer)

    # TODO: collapse this into a single first-layer function; factor to lib; then merge with RecurrentLSTMPStack()
    decoderOutputLayer = Length (decoderDims)-1
    decoder[i:0..decoderOutputLayer] =
        if i == 0
        then if useEncoder && useNYUStyle then BS.RNNs.RecurrentLSTMP (decoderDims[i], cellDim=decoderDims[i],
                                                                       RowStack (S(thoughtVectorEverywhere) : S(decoderInput)), inputDim=(thoughtVector.dim + decoderInputDim),
                                                                       previousHook=PreviousHCWithReorderingHook,
                                                                       enableSelfStabilization=useStabilizer)
             else if useEncoder && attentionSpan > 0 then BS.RNNs.RecurrentLSTMPWithAttentionWindow2 (decoderDims[i], cellDim=decoderDims[i],
                                                                                              S(decoderInput), inputDim=decoderInputDim,
                                                                                              augmentInputHook=FixedWindowAttentionHook, augmentInputDim=encoderOutput.dim,
                                                                                              previousHook=PreviousHCWithReorderingHook,
                                                                                              enableSelfStabilization=useStabilizer)
             else BS.RNNs.RecurrentLSTMP (decoderDims[i], cellDim=decoderDims[i],
                                          S(decoderInput), inputDim=decoderInputDim,
                                          previousHook=PreviousHCFromThoughtVectorWithReorderingHook, # Previous() function with thought vector as initial state
                                          enableSelfStabilization=useStabilizer)
        else BS.RNNs.RecurrentLSTMP (decoderDims[i], cellDim=decoderDims[i],
                                     S(decoder[i-1].h), inputDim=/*decoderDims[i-1]*/ decoder[i-1].dim,
                                     previousHook=PreviousHCWithReorderingHook,
                                     enableSelfStabilization=useStabilizer)
    decoderOutput = decoder[decoderOutputLayer].h
    #decoderDim = decoderOutput.dim
    decoderDim = decoderDims[decoderOutputLayer]

    # and add a softmax layer on top

    W = Parameters.WeightParam (labelSequenceDim, decoderDim)
    B = Parameters.BiasParam (labelSequenceDim)

    z = W * S(decoderOutput) + B;  // top-level input to Softmax

    #############################################################
    # training criteria
    #############################################################

    ce = NewCrossEntropyWithSoftmax (labelSequence, z, tag='criterion')
    #ce2 = Negate (ReduceSum (labelSequence .* LogSoftmax (z)), tag='evaluation')
    #ce1 = CrossEntropyWithSoftmax (labelSequence, z, tag='evaluation')   // this is the training objective
    #errs = ErrorPrediction         (labelSequence, z, tag='evaluation')  // this also gets tracked
    cors = ReduceSum (labelSequence .* Hardmax (z), tag='evaluation')
    errs = Pass (Constants.One - cors, tag='evaluation')

    # score output for decoding
    scoreSequence = Pass (z)
])

#######################################
# shared reader definition            #
#######################################

reader = [
    readerType = "CNTKTextFormatReader" 
    file = "$DataDir$/$trainFile$"
    #randomize="none"
    randomize="auto"
    skipSequenceIds = "false"
    maxErrors = 100
    traceLevel = 2
    chunkSizeInBytes = 30000000 # large enough for entire data set
 
    input = [
        src  = [
            alias = "s"
            dim = $inputVocabSize$
            format = "sparse"
        ]
        tgt = [
            alias = "t"
            dim = $labelVocabSize$
            format = "sparse"
        ]
    ]
]

cvReader = [
    readerType = "CNTKTextFormatReader" 
    file = "$DataDir$/$validFile$"
    randomize="none"
    skipSequenceIds = "false"
    maxErrors = 100
    traceLevel = 2
 
    input = [
        src  = [
            alias = "s"
            dim = $inputVocabSize$
            format = "sparse"
        ]
        tgt = [
            alias = "t"
            dim = $labelVocabSize$     # 39 phonemes (~AX missing), start and end symbol (in index 0)
            format = "sparse"
        ]
    ]
]

lmreader = [
    file = "$DataDir$/$trainFile$"
    #randomize = "auto" # gets ignored

    readerType = LMSequenceReader
    mode = "softmax"                    # TODO: find out what this means
    nbruttsineachrecurrentiter = 0      # 0 means auto-fill given minibatch size
    cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once

    # word class info
    wordclass = "$ModelDir$/vocab.txt"

    #### write definition
    # if writerType is set, we will cache to a binary file
    # if the binary file exists, we will use it instead of parsing this file
    #writerType = BinaryReader
    wfile = $CacheDir$\sequenceSentence.bin
    # if calculated size would be bigger, that is used instead
    wsize = 256
    #wrecords - number of records we should allocate space for in the file
    # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
    wrecords = 1000
    #windowSize - number of records we should include in BinaryWriter window
    windowSize = 10000

    # additional features sections
    # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
    input = [
        dim = 0     # no (explicit) labels   ...labelDim correct??
        ### write definition
        sectionType = "data"
    ]
    # labels sections
    # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
    # labels sections  --this is required, but our labels are extracted from the inLabels
    inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
        dim = 1

        # vocabulary size
        labelType = "category"
        labelDim = "$inputVocabSize$"
        labelMappingFile = "$ModelDir$/vocab.wl"
        beginSequence = "$startSymbol$" # "</s>"
        endSequence   = "</s>"

        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping = [
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=11
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category = [
          dim=11
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
    ]
    outputDummy = [
        labelType = "none"
    ]
]

lmcvReader = [
    file = "$DataDir$/$validFile$"
    #randomize = "none" # gets ignored

    # everything below here is duplicated from 'reader'
    readerType = LMSequenceReader
    mode = "softmax"
    nbruttsineachrecurrentiter = 0      # 0 means auto-fill given minibatch size
    cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once

    # word class info
    wordclass = "$ModelDir$/vocab.txt"

    #### write definition
    # if writerType is set, we will cache to a binary file
    # if the binary file exists, we will use it instead of parsing this file
    #writerType = BinaryReader
    wfile = $CacheDir$\sequenceSentence.bin
    # if calculated size would be bigger, that is used instead
    wsize = 256
    #wrecords - number of records we should allocate space for in the file
    # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
    wrecords = 1000
    #windowSize - number of records we should include in BinaryWriter window
    windowSize = 10000

    # additional features sections
    # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
    input = [
        dim = 0     # no (explicit) labels   ...labelDim correct??
        ### write definition
        sectionType = "data"
    ]
    # labels sections
    # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
    # labels sections  --this is required, but our labels are extracted from the inLabels
    inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
        dim = 1

        # vocabulary size
        labelType = "category"
        labelDim = "$inputVocabSize$"
        labelMappingFile = "$ModelDir$/vocab.wl"
        beginSequence = "</s>"
        endSequence   = "</s>"

        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping = [
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=11
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category = [
          dim=11
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
    ]
    outputDummy = [
        labelType = "none"
    ]
]

#######################################
#  PREPARATION CONFIG                 #
#######################################

writeWordAndClassInfo = [
    action = "writeWordAndClass"
    inputFile = "$DataDir$/$trainFile$"
    beginSequence = "$startSymbol$" # "</s>"
    endSequence   = "</s>"
    outputMappingFile = "$ModelDir$/vocab.wl"
    outputVocabFile = "$ModelDir$/vocab.txt"
    outputWord2Cls  = "$ModelDir$/word2cls.txt"
    outputCls2Index = "$ModelDir$/cls2idx.txt"
    vocabSize = "$inputVocabSize$"
    nbrClass = "$confClassSize$"
    cutoff = 0
    printValues = true
]

#######################################
#  TRAINING CONFIG                    #
#######################################

train = [
    action = "train"
    traceLevel = 1
    epochSize = 0               # (for quick tests, this can be overridden with something small)

    # BrainScriptNetworkBuilder is defined in outer scope

    SGD = [
        #minibatchSize = 128:128:256:512
        #minibatchSize = 64:64:128:256
        minibatchSize = 70:70:70:140:280
        learningRatesPerSample = 0.0035*2:0.002 #0.01 #0.005 # 0.01
        momentumAsTimeConstant = 1500 #2500
        gradientClippingWithTruncation = true   # TODO: clip and truncate? What is the difference?
        clippingThresholdPerSample = 1   #15.0 # 1#visibly impacts objectives, but not final result, so keep it for safety
        maxEpochs = 50
        numMBsToShowResult = 100
        firstMBsToShowResult = 10
        gradUpdateType = "none" # FSAdaGrad?
        loadBestModel = false   # true # broken for some models (rereading overwrites something that got set by validation)

        # tracing (enable these for debugging)
        #traceNodeNamesReal = labelsEmbedded:decoderInput:"decoder[0].lstmState._privateInnards.ht":z.Plus_left.Times_right.result:z:ce
        #traceNodeNamesReal = labelsEmbedded:decoderInput:z:ce
        #traceNodeNamesReal = thoughtVectorEverywhere.result:zMask:z:ce:wer:indexTestVals:index:packedIndex:filtered:unfiltered:isTraining
        #traceNodeNamesCategory = inputSequence.out:labelSequence

        dropoutRate = 0.0

        # settings for Auto Adjust Learning Rate
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
]

#######################################
#  DUMP CONFIG                        #
#######################################

# dumps the model, specifically the learnable parameters

dump = [
    action = "dumpnode"
    modelPath = "$dumpModelPath$"
    outputFile = "$dumpModelPath$.txt"
]

#######################################
#  TEST CONFIG                        #
#######################################

test = [
    action = "eval"

    # correspond to the number of words/characteres to train in a minibatch
    minibatchSize = 8192                # choose as large as memory allows for maximum GPU concurrency
    # need to be small since models are updated for each minibatch
    traceLevel = 1
    epochSize = 0

    reader = [
        file = "$DataDir$/$testFile$"
        #randomize = "none" # gets ignored
    
        # everything below here is duplicated from 'reader'
        readerType = LMSequenceReader
        mode = "softmax"
        nbruttsineachrecurrentiter = 0      # 0 means auto-fill given minibatch size
        cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once
    
        # word class info
        wordclass = "$ModelDir$/vocab.txt"
    
        #### write definition
        # if writerType is set, we will cache to a binary file
        # if the binary file exists, we will use it instead of parsing this file
        #writerType = BinaryReader
        wfile = $CacheDir$\sequenceSentence.bin
        # if calculated size would be bigger, that is used instead
        wsize = 256
        #wrecords - number of records we should allocate space for in the file
        # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
        wrecords = 1000
        #windowSize - number of records we should include in BinaryWriter window
        windowSize = 10000
    
        # additional features sections
        # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
        input = [
            dim = 0     # no (explicit) labels   ...labelDim correct??
            ### write definition
            sectionType = "data"
        ]
        # labels sections
        # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
        # labels sections  --this is required, but our labels are extracted from the inLabels
        inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
            dim = 1
    
            # vocabulary size
            labelType = "category"
            labelDim = "$inputVocabSize$"
            labelMappingFile = "$ModelDir$/vocab.wl"
            beginSequence = "$startSymbol$" # "</s>"
            endSequence   = "</s>"
    
            #### Write definition ####
            # sizeof(unsigned) which is the label index type
            elementSize=4
            sectionType=labels
            mapping = [
              #redefine number of records for this section, since we don't need to save it for each data record
              wrecords=11
              #variable size so use an average string size
              elementSize=10
              sectionType=labelMapping
            ]
            category = [
              dim=11
              #elementSize=sizeof(ElemType) is default
              sectionType=categoryLabels
            ]
        ]
        outputDummy = [
            labelType = "none"
        ]
    ]
]

#######################################
#  WRITE CONFIG                       #
#######################################

# This will write out the log sentence probabilities
#   log P(W) = sum_i P(w_n | w_1..w_n-1)
# of all test sentences in the form log P(W)=<value>, one line per test
# sentence.
#
# This is accomplished by writing out the value of the CE criterion, which
# is an aggregate over all words in a minibatch. By presenting each sentence
# as a separate minibatch, the CE criterion is equal to the log sentence prob.
#
# This can be used for N-best rescoring if you prepare your N-best hypotheses
# as an input file with one line of text per hypothesis, where the output is
# the corresponding log probabilities, one value per line, in the same order.

write = [
    action = "write"

    # these two lines select the decoder
    BrainScriptNetworkBuilder = (if $beamDepth$ == 1
                                 then BS.Seq2Seq.GreedySequenceDecoderFrom (BS.Network.Load ("$decodeModelPath$"))
                                 else BS.Seq2Seq.BeamSearchSequenceDecoderFrom (BS.Network.Load ("$decodeModelPath$"), $beamDepth$))

    # --- BEGIN IGNORE ---

    # (old version, needed for some old setups; please ignore)
    xBrainScriptNetworkBuilder = ([

        enableTracing = true
        traceFrequency = 1000
        tracingLabelMappingFile = "$ModelDir$/vocab.wl"
        include "S2SLib.bs"

        beamDepth = $beamDepth$ // 0=predict; 1=greedy; >1=beam

        # import some names
        Constants = BS.Constants
        Boolean = BS.Boolean
        Loop = BS.Loop
        Previous = Loop.Previous
        IsFirst = Loop.IsFirst
        If = Boolean.If
        OnesTensor = Constants.OnesTensor

        modelAsTrained = BS.Network.Load ("$decodeModelPath$")

        #useNYUStyle = false     # TODO: we should be able to infer this from some dimensions
        hasEmbeddings = false   # TODO: infer this

        top1DecodingModel(model) = new ComputationNetwork [
            # compute top-N from output
            logP = LogSoftmax (model.z)

            offset = Constant (10000)
            top1b = Hardmax (logP)  .* (logP + offset)/*for tracing*/
            top1 = TraceSparse (top1b, 'logP') # TODO: get the accumulated logP out, it's a little more involved

            topN = 10
            topPaths = BS.Seq2Seq.GetTopNTensor (topN, logP) # [V x 1] -> [V x 1 x topN]
            topPathScores = topPaths .* logP   # [V x 1 x topN]
            # reduce back to a single column
            topHyps = TraceSparse (topPathScores * OnesTensor (1 : topN), 'topHyps')

            inputsOut = Pass (model.streams_out_input/*inputSequence*/)
            labelsOut = Pass (TraceOneHot (model.labelSequence, 'labels'))
            decodeOut = Pass (TraceOneHot (top1, 'out'))
            topNOut   = Pass (topHyps)
        ]

        # replace old decoderFeedback node by newDecoderFeedback
        EmbedLabels (x) = if hasEmbeddings then TransposeTimes (modelAsTrained.labelsEmbedded.TransposeTimesArgs[0], x) else x
        decoderFeedback = EmbedLabels (Hardmax (modelAsTrained.z))  # in training, this is decoderFeedback = labelsEmbedded

        # TODO: fold this in
        PreviousOrDefault1 (x, defaultValue=Constant (0)) =   # a delay node with initial value  --TODO: merge the two, then do in C++
        [
            flags = IsFirst (defaultValue/*x*/)
            out = BS.Boolean.If (flags,
                        /*then*/ defaultValue,
                        /*else*/ Previous (x))
        ].out

        # old model structure:
        labelSentenceStart = modelAsTrained.labelSentenceStart_out # _ is a hack
        labelsToUse             = if hasEmbeddings then modelAsTrained.labelsEmbedded             else modelAsTrained.labelSequence
        labelSentenceStartToUse = if hasEmbeddings then modelAsTrained.labelSentenceStartEmbedded else labelSentenceStart
        labelSentenceStartEmbeddedScattered = TraceDense (BS.Sequences.Scatter (IsFirst (labelsToUse), labelSentenceStartToUse), 'sest')

        # new model structure
        #labelSentenceStartEmbeddedScattered = modelAsTrained.labelSentenceStartEmbeddedScattered

        delayedDecoderFeedback = TraceDense (/*Loop.*/PreviousOrDefault1 (defaultValue=labelSentenceStartEmbeddedScattered, TraceDense (decoderFeedback, 'lemb'))   , 'prev lemb')

        greedyDecodingModel = BS.Network.Edit (modelAsTrained,
                                               BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.decoderInput/*delayedDecoderFeedback*/, delayedDecoderFeedback),
                                               #BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.decoderHistoryHook, modelAsTrained.decoderHistoryFromOutput),
                                               modelAsTrained.z/*dummy for now since cannot pass empty set*/)

        # beam search of width 'beamDepth'
        beamDecodingModel = [
            # this comes out of modelAsTrained:
            #  decoder[0].prevState.h = PastValue (decoder[0].lstmState._privateInnards.ht) : [200 x 1 {1,200} x *] -> [200 x 1 {1,200} x *]
            #  decoder[0].prevState.c = PastValue (decoder[0].lstmState._privateInnards.ct) : [200 x 1 {1,200} x *] -> [200 x 1 {1,200} x *]
            #  decoderInput.inputs[1] = PastValue (labelsEmbedded) : [300 x 1 {1,300} x *] -> [300 x 1 {1,300} x *]
            #  decoder[0].prevState.h.elseVal = PastValue (decoder[0].lstmState._privateInnards.ht) : [512 x 1 x labelSequence.h.out.h.indexSequence.h.indexSequence.h] -> [512 x 1 x labelSequence.h.out.h.indexSequence.h.indexSequence.h]
            #  decoder[0].prevState.c.elseVal = PastValue (decoder[0].lstmState._privateInnards.ct) : [512 x 1 x labelSequence.h.out.h.indexSequence.h.indexSequence.h] -> [512 x 1 x labelSequence.h.out.h.indexSequence.h.indexSequence.h]

            #hiddenDim    = modelAsTrained.decoderFeedback.dim
            #embeddingDim = modelAsTrained.decoderOutputEmbedded.dim
            vocabSize    = modelAsTrained.z.dim

            # replace every reference of these by PropagateTopN(of these)

            # turning a regular LSTM to a top-N beam-search decoder:
            #  - add a depth axis of dimension N to all nodes inside the decoder loop
            #     - only needs the init signal for PastValue to be that
            #  - h and c must be shuffled versions of their PastValue
            #     - since what are the top N in one time step is not the top N in the next
            #     - reshufling and adding depth to the init signal can be done at the same place
            #  - decoder output must determine the top N and a reshuffling matrix for h and c
            #     - the current Hardmax needs to be replaced by something that outputs these (output depth N)
            #     - we get a N^2 depth: [V x (input set) x (top N output hypos)]
            #     - reshuffling matrix is reduction over V (multiply with row of V ones) plus possibly a transposition
            #  - we need an accumulated path score
            #     - start value constructed by stacking a 0 and N-1 -INF
            #  - for testing, we can output the current best in each step
            #     - that's a Slice()
            #  - traceback is a right-to-left recurrence
            #     - output best hypo conditioned on the path (it is already known)

            # attention:
            # tanHOut = Tanh (TraceDense(   projectedAttentionWindowBroadcast.projectedValue,   'hencp') + TraceDense (   projectedH,   'hdecp')) # [attentionDim x attentionSpan]
            #   decoder[0].tanHOut.z = Plus (decoder[0].tanHOut.z.PlusArgs[0], decoder[0].tanHOut.z.PlusArgs[1]) : [128 x 20 x WhereNodeAxis1], [128] -> [128 x 20 x WhereNodeAxis1]
            #   patch PlusArgs[0]
            # uValid = u + Log (projectedAttentionWindowBroadcast.valid) # [1 x attentionSpan]
            #   decoder[0].uValid = Plus (decoder[0].u, decoder[0].uValid.PlusArgs[1]) : [1 x 20 x WhereNodeAxis1], [1 x 20 x WhereNodeAxis1] -> [1 x 20 x WhereNodeAxis1]
            #   patch PlusArgs[1]
            # weightedAttentionWindow = projectedAttentionWindowBroadcast.value .* attentionWeights # [encoderHiddenDim x attentionSpan]
            #   decoder[0].weightedAttentionWindow = ElementTimes (projectedAttentionWindowBroadcast.value.out, decoder[0].attentionWeights) : [512 x 20 x WhereNodeAxis1], [1 x 20 x WhereNodeAxis1] -> [512 x 20 x WhereNodeAxis1]
            #   patch ElementTimesArgs[0]
            # each:
            #   node -> SplitDimension (node, /*axis=*/, 1 /*->0:1*/)
            #   e.g.
            #   [512 x 20 x *] -> [(0:1) x 20 x *hereNodeAxis13] -> [512 x 1 x 20 x *]
            # decoder[0].weightedAttentionAverage = Times (decoder[0].weightedAttentionWindow, decoder[0].weightedAttentionAverage.TimesArgs[1]) : [512 x 1 x 20 x WhereNodeAxis11], [20] -> [512] FAILED
            #   change to outputRank=2
            # attentionWeights = TraceDense(  Softmax (uValid)    ,'weights')                    # [1 x attentionSpan]
            #   decoder[0].attentionWeights.h = Softmax (decoder[0].uValid) : [1 x 3 x 20 x WhereNodeAxis21] -> [1 x 3 x 20 x WhereNodeAxis21]
            #   path SoftmaxArgs[0] to be column-wise over axis 3

            ColumnwiseSoftmax = Softmax
            #ColumnwiseSoftmax (axis=1, z) = [ n = TraceDense(   Softmax (z),    'smz') ; axis1 = axis ; d = TraceDense(    ReduceSum (axis=axis1, n),    'denom') ; p = TraceDense(    n .* Reciprocal (d),    'p') ].p

            #Columnwise (f, beamDepth, z) = # TODO: Takes LogSoftmax over axis=1. it is more tricky to do this over arbitrary axes
            #[
            #    cols[d:0..beamDepth-1] = f (Slice (d, d+1, z, axis=2) /*[:,d]*/ )
            #    out = Splice (cols, axis=2)
            #].out

            #InjectDepth (node) = SplitDimension (node, /*axis=*/1, 1 /*->0:1*/)
            InjectDepth (node) = node     # use this with newer models (16+) that do this in the model already

            # decoderFeedback must be updated to take actual decoder output

            propagationEdits[i:0..14] = // TODO: implement and use { } syntax  TODO: VV elseVal only for non-NYU?
                # non-NYU:
                if      i ==  0 then (node => if node.name == 'decoder[0].prevState.h.elseVal' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node) # inject reshuffling of hypotheses
                else if i ==  1 then (node => if node.name == 'decoder[0].prevState.c.elseVal' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node)
                # NYU:
                else if i ==  2 then (node => if node.name == 'decoder[0].prevState.h' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node) # inject reshuffling of hypotheses
                else if i ==  3 then (node => if node.name == 'decoder[0].prevState.c' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node)
                # all:
                else if i ==  4 then (node => if node.name == 'decoder[1].prevState.h' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node) # inject reshuffling of hypotheses
                else if i ==  5 then (node => if node.name == 'decoder[1].prevState.c' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node)
                else if i ==  6 then (node => if node.name == 'decoder[2].prevState.h' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node) # inject reshuffling of hypotheses
                else if i ==  7 then (node => if node.name == 'decoder[2].prevState.c' then TraceState (Previous (ReorderTopN (node.PastValueArgs[0])), 'propagated') else node)
                # attention:
                else if i ==  8 then (node => if node.name != 'decoder[0].tanHOut.z'                then node else InjectDepth (node.PlusArgs[0])          +              node.PlusArgs[1])
                else if i ==  9 then (node => if node.name != 'decoder[0].uValid'                   then node else              node.PlusArgs[0]           + InjectDepth (node.PlusArgs[1]))
                else if i == 10 then (node => if node.name != 'decoder[0].weightedAttentionWindow'  then node else InjectDepth (node.ElementTimesArgs[0]) .*              node.ElementTimesArgs[1])
                else if i == 11 then (node => if node.name != 'decoder[0].weightedAttentionAverage' then node else Times (node.TimesArgs[0], node.TimesArgs[1], outputRank=2))
                else if i == 12 then (node => if node.name != 'decoder[0].attentionWeights'         then node else ColumnwiseSoftmax (axis=3, node.SoftmaxArgs[0]))

                else if i == 13 then (node => if node.name != 'beamSearchReorderHook'               then node else tokens.from)
                #else                BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.decoderHistoryHook, decoderFeedback)
                else                BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.decoderInput/*delayedDecoderFeedback*/, delayedDecoderFeedback)

            # decoderHistoryHook --> decoderFeedback

            m2 = BS.Network.Edit (modelAsTrained,
                                  propagationEdits,
                                  (inputsOut : labelsOut : decodeOut)) # additional roots

            # TODO: use ReduceSum
            ReduceAxis (axisDim, x, axis=1) =   # unfortunately, we must feed in the dimension of the axis, it can't be inferred
                if      axis == 1 then Times (OnesTensor (axisDim), x, outputRank = 0)
                else if axis == 2 then ReduceAxis (axisDim, TransposeDimensions (x, 1, 2), axis=1)
                else Fail("ReduceAxis: Only supports axes 1 and 2.")

            # === BEGIN DECODER ===

            # constants for initial score and final traceback
            initialPathScores = FirstAndOther (0, LOGZERO, beamDepth, axis = 2)  # [1 x D]: [ 0, -INF, -INF, -INF, ... ]
            finalHyp          = FirstAndOther (1, 0,       beamDepth, axis = 1)  # [D] the final token is the top-scoring hypothesis, that is, hyp[0]

            # path expansion of the D hypotheses that were best in previous time step (ordered as in previous time step)
            logLLs = Columnwise (LogSoftmax, beamDepth, modelAsTrained.z)                                   # [V x Dprev]     log  P(w|hist)
            expandedPathScores = logLLs + If (IsFirst (logLLs), initialPathScores, Previous (tokens.score)) # [V x Dprev] log (P(w|hist) * P(hist)) for all top D hypotheses

            # determine top D of expanded paths
            topPaths      = TraceSparse (BS.Seq2Seq.GetTopNTensor (beamDepth, expandedPathScores), 'topPaths') # [V x Dprev] -> [V x Dprev x Dnew]
            topPathScores = TraceSparse (topPaths .* expandedPathScores, 'topPathScores')           #                [V x Dprev x Dnew]

            # form new decoding token, by reducing topPaths(Scores) along relevant dimensions
            tokens = [                                    # [. x Dnew]
                from  = ReduceAxis (axis=1, vocabSize, topPaths) # [Dprev x Dnew], reduced over V
                word  = ReduceAxis (axis=2, beamDepth, topPaths) # [V x Dnew], reduced over Dprev
                score = TraceDense (OnesTensor (1/*output dim*/ : /*reduction dims: */vocabSize : beamDepth/*Dprev*/) * topPathScores, 'tokens.score')  # [1 x Dnew], reduced over [V x Dprev] and inserted a '1'
            ]

            # network feedback for next time step
            decoderFeedback = TraceState (EmbedLabels (TraceSparse (tokens.word, 'tokens.word')), 'feedback') # [embeddingDim x Dnew]
            delayedDecoderFeedback = If (IsFirst (labelSentenceStartEmbeddedScattered), labelSentenceStartEmbeddedScattered, Loop.Previous (decoderFeedback))

            # network state for next step. We must reorder the network state for use in next time step: Apply this lambda to all decoder LSTMs' h and c.
            ReorderTopN (past_h_or_c) = Times (TraceState (past_h_or_c, 'past'), TraceDense (tokens.from, 'backp'))

            # final traceback
            traceback = TraceDense (If (Loop.IsLast (labelSentenceStartEmbeddedScattered/*tokens.from*/), finalHyp, Loop.Next (tokens.from * traceback)), 'traceback')    # [D] one-hot, multiplying tokens.from from the left will select another one-hot row of tokens.from
            decodeHyp = Times (topPaths, traceback, outputRank = 2)          # [V x Dprev] 2D one-hot, selected the best hyp according to traceback
            decode = TraceOneHot (decodeHyp * OnesTensor (beamDepth), 'out') # [V] reduces over Dprev -> 1D one-hot
            # TODO: Can this be done in one ^^ go?

            # === END DECODER ===

            # propagate LSTM state to the right top-N rank given where that rank came from in the previous time step

            #PropagateTopN (past_h_or_c) = Times (TraceState (past_h_or_c, 'past1'), eye3) # hack version that does no shuffling
            eye3 = ConstantFromString ("1 1 1
                                        0 0 0
                                        0 0 0")

            # PropagateTopN:
            # tokens.from: [Dprev, Dnew]
            #   v--------- best came from input hyp[1]
            #     v------- second best came from input hyp[0]
            #       v----- third best came from input hyp[2]
            #   0 1 0
            #   1 0 0
            #   0 0 1
            # tokens.from[:,n] one-hot encodes the best predecessor at top-N rank n
            # each column is a one-hot vector
            # multiplying with such a column from the right will select the column represented by the one-hot value

            # logLLs: get decoder log likelihoods

            Columnwise (f, beamDepth, z) = # TODO: Takes LogSoftmax over axis=1. it is more tricky to do this over arbitrary axes
            [
                cols[d:0..beamDepth-1] = f (Slice (d, d+1, z, axis=2) /*[:,d]*/ )
                out = Splice (cols, axis=2)
            ].out

            # initialPathScores: decoder start token: 0 for first hyp, -INF for the others
            LOGZERO = -1e30

            # expandedPathScores: path expansion, [V x 1] + [1 x D] -> [V x D]

            # topPaths:
            #   +-----+
            #   |0 0 0|
            #   |0 0 0|-+
            #   |0 1 0|0|     means word[2] in input hyp[1] was the best
            #   |0 0 0|0|-+
            #   +-----+0|0|
            #     |1 0 0|0|   means word[3] in input hyp[0] was the second best
            #     +-----+1|   means word[2] in input hyp[2] was the third best
            #       |0 0 0|
            #       +-----+

            # tokens.word:
            #tokens.word = ReduceSum (axis=2, topPaths) # TODO: add an axis parameter to SumColumnElements()
            #   +-+
            #   |0|
            #   |0|-+
            #   |1|0|     means word[2] in input hyp[1] was the best
            #   |0|0|-+
            #   +-+0|0|
            #     |1|0|   means word[3] in input hyp[0] was the second best
            #     +-+1|   means word[2] in input hyp[2] was the third best
            #       |0|
            #       +-+

            # tokens.from:
            # before dropping the first dimension: [V x Dprev x Dnew]
            #   +-----+
            #   |0 1 0|       means input hyp[1] gave rise to the best    
            #   +-----+-+  
            #     |1 0 0|     means input hyp[0] gave rise to second best
            #     +-----+-+
            #       |0 0 1|   means input hyp[2] gave rise to third best
            #       +-----+
            # after: [Dprev x Dnew]        e.g. "0 1 0" goes into first column, vertically
            #   v--------- best came from input hyp[1]
            #     v------- second best came from input hyp[0]
            #       v----- third best came from input hyp[2]
            #   0 1 0
            #   1 0 0
            #   0 0 1
            # tokens.from[:,n] one-hot encodes the best predecessor at top-N rank n

            # topPathScores:
            #   +-----+
            #   |0 0 0|
            #   |0 0 0|-+
            #   |0 x 0|0|     x denotes the accumulated path score max_w P(w|hyp[1])
            #   |0 0 0|0|-+
            #   +-----+0|0|
            #     |y 0 0|0|   y denotes the accumulated path score max_w P(w|hyp[0])
            #     +-----+z|   z denotes the accumulated path score max_w P(w|hyp[2])
            #       |0 0 0|
            #       +-----+

            # traceback:
            # last state: take Hardmax over tokens.score
            # previous states: multiply wth respective tokens.from matrix
            # -> hyp index for every time step
            # then finally use that to select the actual output   TODO: That's a sample-wise matrix product between two sequences!!!
            # TODO: condition must be 1-dim, not 2-dim tensor, so we use labelSentenceStartEmbeddedScattered instead of tokens.from
            # +-+
            # |0|
            # |1|  means at this time step, hyp[1] was the best globally
            # |0|
            # +-+

            # decode: and the actual decoding output
            # This is the one to output (top sentence-level hypothesis after traceback).

            # traceback : [Dnew]
            # topPaths : [V x Dprev x Dnew]
            #   +-----+
            #   |0 0 0|
            #   |0 0 0|-+
            #   |0 1 0|0|     means word[2] in input hyp[1] was the best
            #   |0 0 0|0|-+
            #   +-----+0|0|
            #     |1 0 0|0|   means word[3] in input hyp[0] was the second best
            #     +-----+1|   means word[2] in input hyp[2] was the third best
            #       |0 0 0|
            #       +-----+

            # helper macros  --> move to BS.core.bs

            FirstAndOther (firstVal, otherVals, N, axis = 1) = if N == 1 then ConstantTensor (firstVal, (1)) else [
                axis1 = axis  # TODO: Is this really necessary? Why? Then we need the syntax   axis = ^.axis or ^axis
                out = if axis == 1  # maybe this can be unified or pushed into Splice?
                      then RowStack (ConstantTensor (firstVal, (1)) : ConstantTensor (otherVals, (N -1)))                                # col vector: [ 1; 0; 0; 0 ... ]
                      else Splice   (Constant       (firstVal)      : ConstantTensor (otherVals, (1 : N -1)), axis = axis1 /*, axis*/)   # row vector: [ 0, -INF, -INF, -INF, ... ]
            ].out

            inputsOut = Pass (modelAsTrained.inputSequence)
            labelsOut = Pass (modelAsTrained.labelSequence)
            decodeOut = Pass (decode)
            #topNOut   = Pass (topHyps)
        ].m2

        model = if beamDepth == 0 then top1DecodingModel (modelAsTrained)
           else if beamDepth == 1 then top1DecodingModel (greedyDecodingModel)
           else                        beamDecodingModel
           #else                        BS.Seq2Seq.BeamSearchSequenceDecoderFrom (modelAsTrained, beamDepth)

    ].model)

    # --- END IGNORE ---

    outputPath = $decodeOutputPath$
    #outputPath = "-"                    # "-" will write to stdout; useful for debugging

    outputNodeNames = inputsOut:labelsOut:decodeOut:network.beamDecodingModel.inputsOut:network.beamDecodingModel.labelsOut:network.beamDecodingModel.decodeOut

    format = [
        type = "category" #"sparse"
        transpose = false
        labelMappingFile = "$ModelDir$/vocab.wl"
        #precisionFormat = "10"
        #sequenceEpilogue = "\t// %s\n"
    ]

    minibatchSize = 8192                # choose this to be big enough for the longest sentence
    traceLevel = 1
    epochSize = 0

    reader = [
        file = "$DataDir$/$testFile$"
        #randomize = "none" # gets ignored
    
        # everything below here is duplicated from 'reader'
        readerType = LMSequenceReader
        mode = "softmax"
        nbruttsineachrecurrentiter = 1      # 1 means one per minibatch
        cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once

        # word class info
        wordclass = "$ModelDir$/vocab.txt"

        #### write definition
        # if writerType is set, we will cache to a binary file
        # if the binary file exists, we will use it instead of parsing this file
        #writerType = BinaryReader
        wfile = $CacheDir$\sequenceSentence.bin
        # if calculated size would be bigger, that is used instead
        wsize = 256
        #wrecords - number of records we should allocate space for in the file
        # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
        wrecords = 1000
        #windowSize - number of records we should include in BinaryWriter window
        windowSize = 10000
    
        # additional features sections
        # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
        input = [
            dim = 0     # no (explicit) labels   ...labelDim correct??
            ### write definition
            sectionType = "data"
        ]
        # labels sections
        # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
        # labels sections  --this is required, but our labels are extracted from the inLabels
        inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
            dim = 1

            # vocabulary size
            labelType = "category"
            labelDim = "$inputVocabSize$"
            labelMappingFile = "$ModelDir$/vocab.wl"
            beginSequence = "$startSymbol$" # "</s>"
            endSequence   = "</s>"

            #### Write definition ####
            # sizeof(unsigned) which is the label index type
            elementSize=4
            sectionType=labels
            mapping = [
              #redefine number of records for this section, since we don't need to save it for each data record
              wrecords=11
              #variable size so use an average string size
              elementSize=10
              sectionType=labelMapping
            ]
            category = [
              dim=11
              #elementSize=sizeof(ElemType) is default
              sectionType=categoryLabels
            ]
        ]
        outputDummy = [
            labelType = "none"
        ]
    ]
]

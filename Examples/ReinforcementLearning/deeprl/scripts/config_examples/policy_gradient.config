# See cntk.contrib.deeprl.agent.shared.policy_gradient_parameters for detailed
# explanation of each parameter.

[General]
Agent = actor_critic
Gamma = 0.99
# PreProcessing = cntk.contrib.deeprl.agent.shared.preprocessing.AtariPreprocessing
# PreProcessingArgs = (4,)

[PolicyGradient]
SharedRepresentation = False
# PolicyRepresentation/ValueFunctionRepresentation can be nn, or some
# customized model defined as module_name.method_name, e.g.
# PolicyRepresentation = cntk.contrib.deeprl.agent.shared.customized_models.conv_dqn
PolicyRepresentation = nn
InitialPolicy =
# ValueFunctionRepresentation is ignored when SharedRepresentation is true
ValueFunctionRepresentation = nn
UpdateFrequency = 32
RelativeStepSize = 0.5
RegularizationWeight = 0.001

[NetworkModel]
# Use (a list of integers) when PolicyRepresentation is nn
PolicyNetworkHiddenLayerNodes = [20]

# Use (a list of integers) when ValueFunctionRepresentation is nn, ignored when
# SharedRepresentation is true
ValueNetworkHiddenLayerNodes = [20]

[Optimization]
Momentum = 0.95
InitialEta = 0.01
EtaDecayStepCount = 10000
EtaMinimum = 0.01

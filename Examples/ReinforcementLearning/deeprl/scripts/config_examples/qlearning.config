# See cntk.contrib.deeprl.agent.shared.qlearning_parameters for detailed
# explanation of each parameter.

[General]
Agent = qlearning
Gamma = 0.99
# PreProcessing = cntk.contrib.deeprl.agent.shared.preprocessing.AtariPreprocessing
# PreProcessingArgs = (4,)

[QLearningAlgo]
InitialEpsilon = 1.0
EpsilonDecayStepCount = 10000
EpsilonMinimum = 0.01
InitialQ = 0.0
TargetQUpdateFrequency = 100
QUpdateFrequency = 4
MinibatchSize = 32
# QRepresentation can be 'dqn', 'dueling-dqn', or some customized model defined as
# module_name.method_name, e.g.
# QRepresentation = cntk.contrib.deeprl.agent.shared.customized_models.conv_dqn
QRepresentation = dqn
ErrorClipping = False
ReplaysPerUpdate = 1

[ExperienceReplay]
Capacity = 500
StartSize = 100
Prioritized = True
PriorityAlpha = 0.7
PriorityBeta = 1
PriorityEpsilon = 0.0001

[NetworkModel]
# Use (a list of integers) when QRepresentation is 'dqn'
HiddenLayerNodes = [20]

# Or use (a list of integers followed by two lists of integers) when
# QRepresentation is 'dueling-dqn'
; HiddenLayerNodes = [10, [5], [5]]

[Optimization]
Momentum = 0.9
InitialEta = 0.01
EtaDecayStepCount = 10000
EtaMinimum = 0.0001
GradientClippingThreshold = 10

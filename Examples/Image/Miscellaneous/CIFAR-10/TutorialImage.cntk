# Simple CIFAR-10 convnet, without and with BatchNormalization.

command = TrainConvNet:Eval
#command = TrainConvNetWithBN:Eval

makeMode = false ; traceLevel = 0 ; deviceId = "auto"

RootDir = "." ; DataDir  = "$RootDir$" ; ModelDir = "$RootDir$/Output/Models"

modelPath = "$ModelDir$/cifar10.cmf"

# Training without BN
TrainConvNet = {
    action = "train"

    BrainScriptNetworkBuilder = {
        imageShape = 32:32:3
        labelDim = 10

        # basic model
        model_basic (features) =
        {
            featNorm = features - Constant (128)
            l1 = ConvolutionalLayer {32, (5:5), pad = true, activation = ReLU,
                                     init = "gaussian", initValueScale = 0.0043} (featNorm)
            p1 = MaxPoolingLayer {(3:3), stride = (2:2)} (l1)
            l2 = ConvolutionalLayer {32, (5:5), pad = true, activation = ReLU,
                                     init = "gaussian", initValueScale = 1.414} (p1)
            p2 = MaxPoolingLayer {(3:3), stride = (2:2)} (l2)
            l3 = ConvolutionalLayer {64, (5:5), pad = true, activation = ReLU,
                                     init = "gaussian", initValueScale = 1.414} (p2)
            p3 = MaxPoolingLayer {(3:3), stride = (2:2)} (l3)
            d1 = DenseLayer {64, activation = ReLU, init = "gaussian", initValueScale = 12} (p3)
            z  = LinearLayer {10, init = "gaussian", initValueScale = 1.5} (d1)
        }.z

        # with self-defined layer
        MyConvReLUPoolLayer {dim, initValueScale} =
        {
            C = ConvolutionalLayer {dim, (5:5), pad = true, activation = ReLU, init = "gaussian", initValueScale = initValueScale}
            P = MaxPoolingLayer {(3:3), stride = (2:2)}
            apply (x) = P(C(x))
        }.apply
        model_layers (features) =
        {
            featNorm = features - Constant (128)
            h1 = MyConvReLUPoolLayer {32, 0.0043} (featNorm)
            h2 = MyConvReLUPoolLayer {32, 1.414} (h1)
            h3 = MyConvReLUPoolLayer {64, 1.414} (h2)
            d1 = DenseLayer {64, activation = ReLU, init = "gaussian", initValueScale = 12} (h3)
            z = LinearLayer {labelDim, init = "gaussian", initValueScale = 1.5} (d1)
        }.z

        # model-composition style
        # ...TODO: test this again; last run was a little worse
        Subtract128 (x) = x - Constant (128)
        model_compositionStyle = Sequential (
            Subtract128 :
            MyConvReLUPoolLayer {32, 0.0043} :
            MyConvReLUPoolLayer {32, 1.414}  :
            MyConvReLUPoolLayer {64, 1.414}  :
            DenseLayer {64, activation = ReLU, init = "gaussian", initValueScale = 12} :
            LinearLayer {labelDim, init = "gaussian", initValueScale = 1.5}
        )
        #model_compositionStyle =
        #    Subtract128 >>
        #    LayerStack {3, i => MyConvReLUPoolLayer {dims[i], initValueScales[i]} } >>
        #    MyConvReLUPoolLayer {32, 0.0043} >>
        #    MyConvReLUPoolLayer {32, 1.414}  >>
        #    MyConvReLUPoolLayer {64, 1.414}  >>
        #    DenseLayer {64, activation = ReLU, init = "gaussian", initValueScale = 12} >>
        #    LinearLayer {labelDim, init = "gaussian", initValueScale = 1.5}

        // --- with BatchNorm
        MyConvBNReLUPoolLayer {dim, initValueScale} =
        {
            C = ConvolutionalLayer {dim, (5:5), pad = true, bias = false, init = "gaussian", initValueScale = initValueScale}
            B = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096}
            P = MaxPoolingLayer {(3:3), stride = (2:2)}
            apply (x) = P(ReLU(B(C(x))))
        }.apply
        MyDenseBNReLULayer {dim, initValueScale} =
        {
            D = DenseLayer {dim, bias = false, init = "gaussian", initValueScale = initValueScale}
            B = BatchNormalizationLayer {normalizationTimeConstant = 4096}
            apply (x) = ReLU(B(D(x)))
        }.apply
        model_withBatchNorm (features) =
        {
            featNorm = features - Constant (128)
            h1 = MyConvBNReLUPoolLayer {32, 0.0043} (featNorm)
            h2 = MyConvBNReLUPoolLayer {32, 1.414} (h1)
            h3 = MyConvBNReLUPoolLayer {64, 1.414} (h2)
            d1 = MyDenseBNReLULayer {64, 12} (h3)
            z = LinearLayer {labelDim, init = "gaussian", initValueScale = 1.5} (d1)
        }.z

        // --- ResNet
        MyConvBNLayer {dim, initValueScale, stride} =
        {
            # note: (3:3), while the macro above is (5:5)
            C = ConvolutionalLayer {dim, (3:3), pad = true, stride = (stride:stride), bias = false, init = "gaussian", initValueScale = initValueScale}
            B = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096}
            apply (x) = B(C(x))
        }.apply
        ResNetNode {dim, initValueScale} =
        {
            C1 = MyConvBNLayer {dim, initValueScale, 1}  # first convolution layer
            C2 = MyConvBNLayer {dim, initValueScale, 1}  # second convolution layer
            #B = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096}
            # ^^ Note: Adding an exra BN to 'x' trains slightly better.
            apply (x) = ReLU (x + C2(ReLU(C1(x))))  # ReLU between C1 and C2 and after summation
        }.apply
        ResNetIncNode {dim, initValueScale} =
        {
            # first branch. This doubles the #channels but halves the image size
            C1 = MyConvBNLayer {dim, initValueScale, 2}  # first convolution layer, stride = 2
            C2 = MyConvBNLayer {dim, initValueScale, 1}  # second convolution layer

            # second branch:
            # sub-sample spatially by a factor of 2
            DownSamplingLayer {stride} = MaxPoolingLayer {(1:1), stride = stride}
            # append dim/2 zero output channels
            pad = ConstantTensor (0, (1:1:dim/2))  # the 1s will broadcast to image size
            P(x) = Splice ((DownSamplingLayer {(2:2)} (x) : pad), axis = 3)
            B = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096}

            # layer sums both branches and rectifies the result
            apply (x) = ReLU (B(P(x)) + C2(ReLU(C1(x))))  # ReLU between C1 and C2 and after summation
        }.apply
        model_resNet (features) =
        {
            conv1 = MyConvBNLayer {16, 0.26, 1} (features)
            rl1   = ReLU (conv1)
            rn1   = LayerStack {3, _ => ResNetNode {16, 7.07}} (rl1)

            rn2_1 = ResNetIncNode {32, 7.07} (rn1)
            rn2   = LayerStack {2, _ => ResNetNode {32, 7.07}} (rn2_1)

            rn3_1 = ResNetIncNode {64, 7.07} (rn2)
            rn3   = LayerStack {2, _ => ResNetNode {64, 7.07}} (rn3_1)

            pool = AveragePoolingLayer {(8:8)} (rn3)

            z = LinearLayer {labelDim, init = "gaussian", initValueScale = 0.4} (pool)
        }.z

        # inputs
        features = Input {imageShape}
        labels   = Input {labelDim}

        # apply model to features
        z = model_basic (features)

        # connect to system
        ce       = CrossEntropyWithSoftmax (labels, z)
        errs     = ErrorPrediction         (labels, z)
        top5Errs = ErrorPrediction         (labels, z, topN=5)  # only used in Eval action

        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)  # top5Errs only used in Eval
        outputNodes     = (z)
    }

    SGD = {
        epochSize = 50000   # 49984 --TODO: why 16 less?

        # without BatchNormalization:
        maxEpochs = 30 ; minibatchSize = 64
        learningRatesPerSample = 0.00015625*10:0.000046875*10:0.000015625
        momentumAsTimeConstant = 600*20:6400
        L2RegWeight = 0.03

        # with BatchNormalization:
        #maxEpochs = 30 ; minibatchSize = 64
        #learningRatesPerSample = 0.00046875*7:0.00015625
        #momentumAsTimeConstant = 0
        #L2RegWeight = 0

        # ResNet
        #maxEpochs = 160 ; minibatchSize = 128
        #learningRatesPerSample = 0.0078125*80:0.00078125*40:0.000078125
        #momentumAsTimeConstant = 1200
        #L2RegWeight = 0.0001

        firstMBsToShowResult = 10 ; numMBsToShowResult = 500
    }

    reader = {
        verbosity = 0
        randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$DataDir$/cifar-10-batches-py/train_map.txt"
            input = {
                features = { transforms = (
                    { type = "Crop" ; cropType = "random" ; cropRatio = 0.8 ; jitterType = "uniRatio" } :
                    { type = "Scale" ; width = 32 ; height = 32 ; channels = 3 ; interpolations = "linear" } :
                    { type = "Transpose" }
                )}
                labels =   { labelDim = 10 }
            }
        })
    }
}

# Eval action
Eval = {
    action = "eval"
    minibatchSize = 16
    evalNodeNames = errs:top5Errs  # also test top-5 error rate
    reader = {
        verbosity = 0
        randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$DataDir$/cifar-10-batches-py/test_map.txt"
            input = {
                features = { transforms = (
                   { type = "Scale" ; width = 32 ; height = 32 ; channels = 3 ; interpolations = "linear" } :
                   { type = "Transpose" }
                )}
                labels =   { labelDim = 10 }
            }
        })
    }
}

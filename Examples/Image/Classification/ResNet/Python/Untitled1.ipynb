{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "# Licensed under the MIT license. See LICENSE.md file in the project root\n",
    "# for full license information.\n",
    "# ==============================================================================\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from cntk import input, cross_entropy_with_softmax, classification_error, reduce_mean\n",
    "from cntk.io import MinibatchSource, ImageDeserializer, StreamDef, StreamDefs\n",
    "import cntk.io.transforms as xforms\n",
    "from cntk import Trainer, cntk_py\n",
    "from cntk.learners import adam, nesterov, sgd, momentum_sgd, learning_rate_schedule, momentum_as_time_constant_schedule, UnitType\n",
    "from cntk.debugging import set_computation_network_trace_level\n",
    "from cntk.logging import *\n",
    "from cntk.debugging import *\n",
    "from resnet_models import *\n",
    "import hessianfree as hf\n",
    "\n",
    "# Paths relative to current python file.\n",
    "abs_path   = os.path.dirname(os.path.abspath(__file__))\n",
    "data_path  = os.path.join(abs_path, \"..\", \"..\", \"..\", \"DataSets\", \"CIFAR-10\")\n",
    "\n",
    "# model dimensions\n",
    "image_height = 32\n",
    "image_width  = 32\n",
    "num_channels = 3  # RGB\n",
    "num_classes  = 10\n",
    "\n",
    "# Define the reader for both training and evaluation action.\n",
    "def create_reader(map_file, mean_file, train):\n",
    "    if not os.path.exists(map_file) or not os.path.exists(mean_file):\n",
    "        raise RuntimeError(\"File '%s' or '%s' does not exist. Please run install_cifar10.py from DataSets/CIFAR-10 to fetch them\" %\n",
    "                           (map_file, mean_file))\n",
    "\n",
    "    # transformation pipeline for the features has jitter/crop only when training\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms += [\n",
    "            xforms.crop(crop_type='randomside', side_ratio=0.8, jitter_type='uniratio') # train uses jitter\n",
    "        ]\n",
    "    transforms += [\n",
    "        xforms.scale(width=image_width, height=image_height, channels=num_channels, interpolations='linear'),\n",
    "        xforms.mean(mean_file)\n",
    "    ]\n",
    "    # deserializer\n",
    "    return MinibatchSource(ImageDeserializer(map_file, StreamDefs(\n",
    "        features = StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels   = StreamDef(field='label', shape=num_classes))))   # and second as 'label'\n",
    "\n",
    "\n",
    "# Train and evaluate the network.\n",
    "def train_and_evaluate(reader_train, reader_test, network_name, epoch_size, max_epochs, profiler_dir=None,\n",
    "                       model_dir=None, tensorboard_logdir=None):\n",
    "\n",
    "    set_computation_network_trace_level(0)\n",
    "\n",
    "    # Input variables denoting the features and label data\n",
    "    input_var = input((num_channels, image_height, image_width))\n",
    "    label_var = input((num_classes))\n",
    "\n",
    "    # create model, and configure learning parameters\n",
    "    if network_name == 'resnet20':\n",
    "        z = create_cifar10_model(input_var, 3, num_classes)\n",
    "        lr_per_mb = [1.0]*80+[0.1]*40+[0.01]\n",
    "    elif network_name == 'resnet110':\n",
    "        z = create_cifar10_model(input_var, 18, num_classes)\n",
    "        lr_per_mb = [0.1]*1+[1.0]*80+[0.1]*40+[0.01]\n",
    "    else:\n",
    "        return RuntimeError(\"Unknown model name!\")\n",
    "\n",
    "    # loss and metric\n",
    "    ce = cross_entropy_with_softmax(z, label_var)\n",
    "    pe = classification_error(z, label_var)\n",
    "\n",
    "    # shared training parameters\n",
    "    minibatch_size = 128\n",
    "    momentum_time_constant = -minibatch_size/np.log(0.9)\n",
    "    l2_reg_weight = 0.0001\n",
    "\n",
    "    # Set learning parameters\n",
    "    lr_per_sample = [lr/minibatch_size for lr in lr_per_mb]\n",
    "    lr_schedule = learning_rate_schedule(lr_per_sample, epoch_size=epoch_size, unit=UnitType.sample)\n",
    "    mm_schedule = momentum_as_time_constant_schedule(momentum_time_constant)\n",
    "\n",
    "    # progress writers\n",
    "    # progress_writers = [ProgressPrinter(tag='Training', num_epochs=max_epochs, log_to_file = 'momentum_sgd.log')]\n",
    "    # progress_writers = [ProgressPrinter(tag='Training', num_epochs=max_epochs, log_to_file = 'sgd.log')]\n",
    "    # progress_writers = [ProgressPrinter(tag='Training', num_epochs=max_epochs, log_to_file = 'nesterov.log')]\n",
    "    progress_writers = [ProgressPrinter(tag='Training', num_epochs=max_epochs, log_to_file='adam.log')]\n",
    "    \n",
    "    tensorboard_writer = None\n",
    "    if tensorboard_logdir is not None:\n",
    "        tensorboard_writer = TensorBoardProgressWriter(freq=10, log_dir=tensorboard_logdir, model=z)\n",
    "        progress_writers.append(tensorboard_writer)\n",
    "\n",
    "    # trainer object\n",
    "    # learner = momentum_sgd(z.parameters, lr_schedule, mm_schedule,\n",
    "    #                        l2_regularization_weight = l2_reg_weight)\n",
    "    # learner = sgd(z.parameters, lr_schedule, mm_schedule,\n",
    "    #                        l2_regularization_weight = l2_reg_weight)\n",
    "    # learner = nesterov(z.parameters, lr_schedule, mm_schedule,\n",
    "    #                        l2_regularization_weight = l2_reg_weight)\n",
    "    learner = adam(z.parameters, lr_schedule, mm_schedule,\n",
    "                           l2_regularization_weight = l2_reg_weight)\n",
    "    \n",
    "    trainer = Trainer(z, (ce, pe), learner, progress_writers)\n",
    "\n",
    "    # define mapping from reader streams to network inputs\n",
    "    input_map = {\n",
    "        input_var: reader_train.streams.features,\n",
    "        label_var: reader_train.streams.labels\n",
    "    }\n",
    "\n",
    "    log_number_of_parameters(z) ; print()\n",
    "\n",
    "    # perform model training\n",
    "    if profiler_dir:\n",
    "        start_profiler(profiler_dir, True)\n",
    "\n",
    "    for epoch in range(max_epochs):       # loop over epochs\n",
    "        sample_count = 0\n",
    "        while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "            data = reader_train.next_minibatch(min(minibatch_size, epoch_size-sample_count), input_map=input_map) # fetch minibatch.\n",
    "            trainer.train_minibatch(data)                                   # update model with it\n",
    "            sample_count += trainer.previous_minibatch_sample_count         # count samples processed so far\n",
    "\n",
    "        trainer.summarize_training_progress()\n",
    "\n",
    "        # Log mean of each parameter tensor, so that we can confirm that the parameters change indeed.\n",
    "        if tensorboard_writer:\n",
    "            for parameter in z.parameters:\n",
    "                tensorboard_writer.write_value(parameter.uid + \"/mean\", reduce_mean(parameter).eval(), epoch)\n",
    "\n",
    "        if model_dir:\n",
    "            z.save(os.path.join(model_dir, network_name + \"_{}.dnn\".format(epoch)))\n",
    "        enable_profiler() # begin to collect profiler data after first epoch\n",
    "\n",
    "    if profiler_dir:\n",
    "        stop_profiler()\n",
    "\n",
    "    # Evaluation parameters\n",
    "    test_epoch_size     = 10000\n",
    "    minibatch_size = 16\n",
    "\n",
    "    # process minibatches and evaluate the model\n",
    "    metric_numer    = 0\n",
    "    metric_denom    = 0\n",
    "    sample_count    = 0\n",
    "\n",
    "    while sample_count < test_epoch_size:\n",
    "        current_minibatch = min(minibatch_size, test_epoch_size - sample_count)\n",
    "        # Fetch next test min batch.\n",
    "        data = reader_test.next_minibatch(current_minibatch, input_map=input_map)\n",
    "        # minibatch data to be trained with\n",
    "        metric_numer += trainer.test_minibatch(data) * current_minibatch\n",
    "        metric_denom += current_minibatch\n",
    "        # Keep track of the number of samples processed so far.\n",
    "        sample_count += data[label_var].num_samples\n",
    "\n",
    "    print(\"\")\n",
    "    trainer.summarize_test_progress()\n",
    "    print(\"\")\n",
    "\n",
    "    return metric_numer/metric_denom\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--network', help='network type, resnet20 or resnet110', required=False, default='resnet20')\n",
    "    parser.add_argument('-e', '--epochs', help='total epochs', required=False, default='160')\n",
    "    parser.add_argument('-p', '--profiler_dir', help='directory for saving profiler output', required=False, default=None)\n",
    "    parser.add_argument('-m', '--model_dir', help='directory for saving model', required=False, default=None)\n",
    "    parser.add_argument('-tensorboard_logdir', '--tensorboard_logdir', help='Directory where TensorBoard logs should be created', required=False, default=None)\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "    epochs = int(args['epochs'])\n",
    "    network_name = args['network']\n",
    "\n",
    "    model_dir = args['model_dir']\n",
    "    if not model_dir:\n",
    "        model_dir = os.path.join(abs_path, \"Models\")\n",
    "\n",
    "    reader_train = create_reader(os.path.join(data_path, 'train_map.txt'), os.path.join(data_path, 'CIFAR-10_mean.xml'), True)\n",
    "    reader_test  = create_reader(os.path.join(data_path, 'test_map.txt'), os.path.join(data_path, 'CIFAR-10_mean.xml'), False)\n",
    "\n",
    "    epoch_size = 50000\n",
    "    train_and_evaluate(reader_train, reader_test, network_name, epoch_size, epochs, args['profiler_dir'], model_dir,\n",
    "                       args['tensorboard_logdir'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

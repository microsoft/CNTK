//
// Copyright (c) Microsoft. All rights reserved.
// Licensed under the MIT license. See LICENSE.md file in the project root for full license information.
//
// CNTK.core.bs -- core BrainScript library including both general and CNTK-specific definitions
//

##############################################################################
# standard functions
##############################################################################

Print(value, format='') = new PrintAction [ what = value /*; how = format*/ ] 
Fail(what) = new FailAction [ /*what*/ ]
Format(value, format) = new StringFunction [ what = 'Format' ; arg = value ; how = format ] 
Replace(s, from, to) = new StringFunction [ what = 'Replace' ; arg = s ; replacewhat = from ; withwhat = to ] 
Substr(s, begin, num) = new StringFunction [ what = 'Substr' ; arg = s ; pos = begin ; chars = num ] 
Chr(c) = new StringFunction [ what = 'Chr' ;  arg = c ] 
Floor(x)  = new NumericFunction [ what = 'Floor' ;  arg = x ] 
Length(x) = new NumericFunction [ what = 'Length' ; arg = x ] 
Ceil(x) = -Floor(-x) 
Round(x) = Floor(x+0.5)
Sign(x) = if x > 0 then 1 else if x < 0 then -1 else 0 
Min(a,b) = if a < b then a else b 
Max(a,b) = if a > b then a else b 
Fac(n) = if n > 1 then Fac(n-1) * n else 1 
IsSameObject(a,b) = new CompareFunction [ what = 'IsSameObject' ; args = (a : b) ]

##############################################################################
# ComputationNodes
##############################################################################

LearnableParameter(rows, cols, learningRateMultiplier = 1.0, init = 'uniform'/*|fixedValue|gaussian|fromFile*/, initValueScale = 1, value = 0, initFromFilePath = '', initFromLiteral = '', initOnCPUOnly=true, randomSeed=-1, tag='') = new ComputationNode [ operation = 'LearnableParameter' ; shape = new TensorShape [ dims = (rows : cols) ] /*plus the function args*/ ]
Parameter = LearnableParameter // deprecated 
# TODO: make Parameter take tensor dims?
ParameterTensor(dims, learningRateMultiplier = 1.0, init = 'uniform'/*|fixedValue|gaussian|fromFile*/, initValueScale = 1, value = 0, initFromFilePath = '', initFromLiteral = '', initOnCPUOnly=true, randomSeed=-1, tag='') = new ComputationNode [ operation = 'LearnableParameter' ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
ConstantFromString(literal, tag='') = ParameterTensor((0)/*dim, will be inferred*/, init = 'fromLiteral', initFromLiteral = literal, learningRateMultiplier = 0.0)
Input(dims, tag='feature') = new ComputationNode [ operation = 'InputValue' ; shape = new TensorShape [ /*dims*/ ] ; isImage = false /*plus the function args*/ ]
SparseInput(dims, tag='feature') = new ComputationNode [ operation = 'SparseInputValue' ; shape = new TensorShape [ /*dims*/ ] ; isImage = false /*plus the function args*/ ]
ImageInput(imageWidth, imageHeight, imageChannels, imageLayout='CHW', tag='feature') = new ComputationNode [ operation = 'InputValue' ; isImage = true /*plus the function args*/ ]
SparseImageInput(imageWidth, imageHeight, imageChannels, imageLayout='CHW', tag='feature') = new ComputationNode [ operation = 'SparseInputValue' ; isImage = true /*plus the function args*/ ]
EnvironmentInput(propertyName, tag='') = new ComputationNode [ operation = 'EnvironmentInput' /*plus the function args*/ ]
ConstantTensor(val, dims, tag='') = ParameterTensor(dims, learningRateMultiplier = 0, init = 'fixedValue', value = val) 
Constant(val, rows = 1, cols = 1, tag='') = Parameter(rows, cols, learningRateMultiplier = 0, init = 'fixedValue', value = val) 
PastValue(dims, input, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'PastValue' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
FutureValue(dims, input, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'FutureValue' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
Shift(input, fromOffset, boundaryValue, boundaryMode=-1/*context*/, dim=-1, tag='') = new ComputationNode [ operation = 'Shift' ; inputs = (input : boundaryValue) /*plus the function args*/ ]
RowSlice(beginIndex, numRows, input, tag='') = Slice(beginIndex, beginIndex + numRows, input, axis = 1)
RowRepeat(input, numRepeats, tag='') = new ComputationNode [ operation = 'RowRepeat' ; inputs = input /*plus the function args*/ ]
RowStack(inputs, tag='') = new ComputationNode [ operation = 'RowStack' /*plus the function args*/ ]
Splice (inputs, axis=1) = # TODO: This is a workaround. RowStack itself shall interpret 'axis' and be renamed to Splice().
    if axis < 1 then Fail('Splice does not yet implement splicing the time axis.')
    else if axis == 1 then RowStack (inputs)
    else [ # workaround: swap 'axis' to first position, RowStack, swap back
        ArrayTransposeDimensions (inputs, axis1, axis2) = [ # transpose each element of a BS array
            inputsT[i:0..Length(inputs)-1] = TransposeDimensions (inputs[i], axis1, axis2)
        ].inputsT
        out = TransposeDimensions (RowStack (ArrayTransposeDimensions (inputs, 1, axis)), 1, axis)
    ].out
Reshape(input, numRows, imageWidth = 0, imageHeight = 0, imageChannels = 0, tag='') = new ComputationNode [ operation = 'LegacyReshape' ; inputs = input /*plus the function args*/ ]
NewReshape(input, dims, beginAxis=0, endAxis=0, tag='') = new ComputationNode [ operation = 'Reshape' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
ReshapeDimension(x, axis, tensorShape) = NewReshape(x, tensorShape, beginAxis=axis, endAxis=axis + 1) 
FlattenDimensions(x, axis, num) = NewReshape(x, 0, beginAxis=axis, endAxis=axis + num) 
Slice(beginIndex, endIndex, input, axis=1, tag='') =
    if axis < 0 then [ # time axis: specify -1
        beginFlags = if beginIndex > 0 then BS.Boolean.Not (BS.Loop.IsFirstN (beginIndex, input)) else                 BS.Loop.IsLastN  (-beginIndex, input)
        endFlags   = if endIndex   > 0 then                 BS.Loop.IsFirstN (endIndex,   input)  else BS.Boolean.Not (BS.Loop.IsLastN  (-endIndex,   input))
        flags = if      beginIndex == 0 then endFlags
                else if endIndex   == 0 then beginFlags
                else                         BS.Boolean.And (beginFlags, endFlags)
        out = if beginIndex == 0 && endIndex == 0
              then input
              else BS.Sequences.Gather (flags, input)
    ].out
    else new ComputationNode [ operation = 'Slice' ; inputs = input /*plus the function args*/ ] # non-time axis
SplitDimension(x, axis, N) = ReshapeDimension(x, axis, 0:N) 
TransposeDimensions(input, axis1, axis2, tag='') = new ComputationNode [ operation = 'TransposeDimensions' ; inputs = input /*plus the function args*/ ]
# TODO: make input the last arg!
Transpose(x) = TransposeDimensions(x, 1, 2)
Times(A, B, outputRank=1, tag='') = new ComputationNode [ operation = 'Times' ; inputs = ( A : B ) /*plus the function args*/ ]
Logistic(label, probability, tag='') = new ComputationNode [ operation = 'Logistic' ; inputs = (label : probability) /*plus the function args*/ ]
WeightedLogistic(label, probability, instanceWeight, tag='') = new ComputationNode [ operation = 'Logistic' ; inputs = (label : probability : instanceWeight) /*plus the function args*/ ]
ReconcileMBLayout(dataInput, layoutInput, tag='') = new ComputationNode [ operation = 'ReconcileMBLayout' ; inputs = (dataInput : layoutInput) /*plus the function args*/ ]
CastAs (type, data) = ReconcileMBLayout (data, type) # read as CastAs<type>(data) where the cast may consist of rearranging the data w.r.t. MBLayout or broadcasting across sequence items
Convolution(weightNode, inputValueNode, kernelDims, mapDims = 1, stride = 1, sharing = true, autoPadding = true, lowerPad = 0, upperPad = 0, imageLayout='CHW', maxTempMemSizeInSamples = 0, tag='') = new ComputationNode [ operation = 'Convolution' ; inputs = (weightNode : inputValueNode); kernelShape = new TensorShape [ dims = kernelDims ] ; mapCount = new TensorShape [ dims = mapDims ] ; strideShape = new TensorShape [ dims = stride ] ; dimSharing = new BoolVector [ items = sharing ] ; dimPadding = new BoolVector [ items = autoPadding ] ; dimPadLower = new TensorShape [ dims = lowerPad ] ; dimPadUpper = new TensorShape [ dims = upperPad ] /*plus the function args*/ ]
Pooling(input, poolKind/*'max'|'average'*/, kernelDims, stride=1, autoPadding = true, lowerPad = 0, upperPad = 0, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'Pooling' ; inputs = (input); pool = poolKind ; kernelShape = new TensorShape [ dims = kernelDims ] ; strideShape = new TensorShape [ dims = stride ] ; dimPadding = new BoolVector [ items = autoPadding ] ; dimPadLower = new TensorShape [ dims = lowerPad ] ; dimPadUpper = new TensorShape [ dims = upperPad ] /*plus the function args*/ ]
MaxPooling(input, windowWidth, windowHeight, horizontalSubsample, verticalSubsample, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'MaxPooling' ; inputs = input /*plus the function args*/ ]
AveragePooling(input, windowWidth, windowHeight, horizontalSubsample, verticalSubsample, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'AveragePooling' ; inputs = input /*plus the function args*/ ]
ColumnwiseCrossProduct = KhatriRaoProduct // deprecated 
ClassificationError = ErrorPrediction 
Delay = PastValue 

BatchNormalization(input, scale, bias, runMean, runInvStdDev, spatial, normalizationTimeConstant = 0, blendTimeConstant = 0, epsilon = 0.00001, useCntkEngine = true, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'BatchNormalization' ; inputs = (input : scale : bias : runMean : runInvStdDev) /*plus the function args*/ ]
Abs(x, tag='') = new ComputationNode [ operation = 'Abs' ; inputs = x /*plus the function args*/ ]
ClassBasedCrossEntropyWithSoftmax(labelClassDescriptorVectorSequence, mainInputInfo, mainWeight, classLogProbsBeforeSoftmax, tag='') = new ComputationNode [ operation = 'ClassBasedCrossEntropyWithSoftmax' ; inputs = (labelClassDescriptorVectorSequence : mainInputInfo : mainWeight : classLogProbsBeforeSoftmax) /*plus the function args*/ ]
ColumnElementTimes(aVectorSequence, anotherVectorSequence, tag='') = new ComputationNode [ operation = 'ColumnElementTimes' ; inputs = (aVectorSequence : anotherVectorSequence) /*plus the function args*/ ]
// TODO: ColumnElementTimes = ElementTimes
CosDistance(aVectorSequence, anotherVectorSequence, tag='') = new ComputationNode [ operation = 'CosDistance' ; inputs = (aVectorSequence : anotherVectorSequence) /*plus the function args*/ ]
CosDistanceWithNegativeSamples(aVectorSequence, anotherVectorSequence, numShifts, numNegSamples, tag='') = new ComputationNode [ operation = 'CosDistanceWithNegativeSamples' ; inputs = (aVectorSequence : anotherVectorSequence : numShifts : numNegSamples) /*plus the function args*/ ]
Cosine(x, tag='') = new ComputationNode [ operation = 'Cosine' ; inputs = x /*plus the function args*/ ]
CrossEntropy(refProbVectorSequence, outProbVectorSequence, tag='') = new ComputationNode [ operation = 'CrossEntropy' ; inputs = (refProbVectorSequence : outProbVectorSequence) /*plus the function args*/ ]
CrossEntropyWithSoftmax(labelVectorSequence, outProbVectorSequence, tag='') = new ComputationNode [ operation = 'CrossEntropyWithSoftmax' ; inputs = (labelVectorSequence : outProbVectorSequence) /*plus the function args*/ ]
DiagTimes(diagonalMatrixAsColumnVector, matrix, tag='') = new ComputationNode [ operation = 'DiagTimes' ; inputs = (diagonalMatrixAsColumnVector : matrix) /*plus the function args*/ ]
// TODO: DiagTimes = ElementTimes
Dropout(activationVectorSequence, tag='') = new ComputationNode [ operation = 'Dropout' ; inputs = activationVectorSequence /*plus the function args*/ ]
ElementTimes(aMatrix, anotherMatrix, tag='') = new ComputationNode [ operation = 'ElementTimes' ; inputs = (aMatrix : anotherMatrix) /*plus the function args*/ ]
ElementDivide(aMatrix, anotherMatrix, tag='') = ElementTimes(aMatrix, Reciprocal(anotherMatrix))
ErrorPrediction(labelVectorSequence, outVectorSequence, tag='') = new ComputationNode [ operation = 'ErrorPrediction' ; inputs = (labelVectorSequence : outVectorSequence) /*plus the function args*/ ]
Exp(x, tag='') = new ComputationNode [ operation = 'Exp' ; inputs = x /*plus the function args*/ ]
GatherPacked(indexSequence, sourceData, tag='') = new ComputationNode [ operation = 'GatherPacked' ; inputs = (indexSequence : sourceData) /*plus the function args*/ ]
GMMLogLikelihood(unnormalizedPriorVector, meansAsRows, logStdDevAsRows, dataVectorSequence, tag='') = new ComputationNode [ operation = 'GMMLogLikelihood' ; inputs = (unnormalizedPriorVector : meansAsRows : logStdDevAsRows : dataVectorSequence) /*plus the function args*/ ]
InvStdDev(dataVectorSequence, tag='') = new ComputationNode [ operation = 'InvStdDev' ; inputs = dataVectorSequence /*plus the function args*/ ]
KhatriRaoProduct(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'KhatriRaoProduct' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Log(x, tag='') = new ComputationNode [ operation = 'Log' ; inputs = x /*plus the function args*/ ]
LogSoftmax(z, tag='') = new ComputationNode [ operation = 'LogSoftmax' ; inputs = z /*plus the function args*/ ]
MatrixL1Reg(matrix, tag='') = new ComputationNode [ operation = 'MatrixL1Reg' ; inputs = matrix /*plus the function args*/ ]
MatrixL2Reg(matrix, tag='') = new ComputationNode [ operation = 'MatrixL2Reg' ; inputs = matrix /*plus the function args*/ ]
Mean(dataVectorSequence, tag='') = new ComputationNode [ operation = 'Mean' ; inputs = dataVectorSequence /*plus the function args*/ ]
Minus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'Minus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Negate(input, tag='') = new ComputationNode [ operation = 'Negate' ; inputs = input /*plus the function args*/ ]
PackedIndex(targetObject, indexSequence, tag='') = new ComputationNode [ operation = 'PackedIndex' ; inputs = (targetObject : indexSequence) /*plus the function args*/ ]
Pass(x, tag='') = new ComputationNode [ operation = 'Pass' ; inputs = x /*plus the function args*/ ]
PerDimMeanVarDeNormalization(dataVectorSequence, meanVector, invStdDevVector, tag='') = new ComputationNode [ operation = 'PerDimMeanVarDeNormalization' ; inputs = (dataVectorSequence : meanVector : invStdDevVector) /*plus the function args*/ ]
PerDimMeanVarNormalization(dataVectorSequence, meanVector, invStdDevVector, tag='') = new ComputationNode [ operation = 'PerDimMeanVarNormalization' ; inputs = (dataVectorSequence : meanVector : invStdDevVector) /*plus the function args*/ ]
Plus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'Plus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Reciprocal(z, tag='') = new ComputationNode [ operation = 'Reciprocal' ; inputs = z /*plus the function args*/ ]
RectifiedLinear(z, tag='') = new ComputationNode [ operation = 'RectifiedLinear' ; inputs = z /*plus the function args*/ ]
Scale(scalarScalingFactor, matrix, tag='') = new ComputationNode [ operation = 'Scale' ; inputs = (scalarScalingFactor : matrix) /*plus the function args*/ ]
// TODO: Scale = ElementTimes
ScatterPacked(cond, indexSequence, sourceData, tag='') = new ComputationNode [ operation = 'ScatterPacked' ; inputs = (cond : indexSequence : sourceData) /*plus the function args*/ ]
Sigmoid(z, tag='') = new ComputationNode [ operation = 'Sigmoid' ; inputs = z /*plus the function args*/ ]
Softmax(z, tag='') = new ComputationNode [ operation = 'Softmax' ; inputs = z /*plus the function args*/ ]
Hardmax(z, tag='') = new ComputationNode [ operation = 'Hardmax' ; inputs = z /*plus the function args*/ ]
Sqrt(z, tag='') = new ComputationNode [ operation = 'Sqrt' ; inputs = z /*plus the function args*/ ]
SquareError(aMatrix, anotherMatrix, tag='') = new ComputationNode [ operation = 'SquareError' ; inputs = (aMatrix : anotherMatrix) /*plus the function args*/ ]
SumColumnElements(z, tag='') = new ComputationNode [ operation = 'SumColumnElements' ; inputs = z /*plus the function args*/ ]
# ^^ TODO: Rename to SumElements? ReduceSum without axis?
SumElements(matrix, tag='') = new ComputationNode [ operation = 'SumElements' ; inputs = matrix /*plus the function args*/ ]
# ^^ TODO: Rename to ReduceSumMB?
Tanh(z, tag='') = new ComputationNode [ operation = 'Tanh' ; inputs = z /*plus the function args*/ ]
TimeReverse(vectorSequence, tag='') = new ComputationNode [ operation = 'TimeReverse' ; inputs = vectorSequence /*plus the function args*/ ]
TransposeTimes(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'TransposeTimes' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Where(cond, tag='') = new ComputationNode [ operation = 'Where' ; inputs = cond /*plus the function args*/ ]

##############################################################################
# common macros
##############################################################################

BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols) ; z = W*in+B ] 
SBFF(in, rows, cols) = [ Eh = Sigmoid(BFF(in, rows, cols).z) ] 
MeanVarNorm(feat) = PerDimMeanVarNormalization(feat, Mean(feat), InvStdDev(feat)) 
LogPrior(labels) = Log(Mean(labels)) 

##############################################################################
# the more specific standard things are in a namespace called 'BS'
# You can create shorthands for accessing these, e.g. saying B = BS.Boolean.
# Note: Identifiers beginning with _ should be considered for library use only.
##############################################################################

BS = [

##############################################################################
# Basic constants
##############################################################################

Constants = [
    Zero = ConstantTensor (0, (1))
    One  = ConstantTensor (1, (1))
    # BUGBUG: ZeroesLike() would recreate the full dimension of x. Well, no need if it considers broadcasting. But still wrong if we want to broadcast a vector of different tensor dim.
    #ZeroesLike (x) = CastAs (x, Zero) // read: Cast<x>(Zero)
    #OnesLike (x)   = CastAs (x, One)
    # CastAs() does not implement broadcasting
    ZeroesLike (x) = RowSlice (0, 1, x) .* Zero  // hack: get one row of input and multiply with zero
    OnesLike (x) = ZeroesLike (x) + One
    # is this like Sequences.Repeat?
    True  = 1
    False = 0
]

##############################################################################
# Boolean operations
# These operations will have undefined behavior for input values != 0 or 1.
##############################################################################

# boolean helpers
Boolean = [
    True  = 1
    False = 0

    # basic logical operations
    And (a,b) =         a .* b
    Or  (a,b) = a + b - a .* b
    Xor (a,b) = a + b - a .* b * Constant (2)
    Not (x)   = Constants.One - x

    # on each time step where clk 1, this toggles its value
    Toggle (clk, initialValue=False) = [
        state = Xor (PastValue (1, state, defaultHiddenActivation=initialValue), clk)
    ].state

    # select a value
    # Note: This will be replaced by BrainScript 'if cond then thenVal else elseVal' and SwitchNode
    If (cond, thenVal, elseVal) = cond .* thenVal + Not (cond) .* elseVal
]

##############################################################################
# sequence operations
# These mimic LINQ operations.
##############################################################################

Sequences = [
    # fold left/right: Reduce entire sequence by applying binaryOp, e.g. FoldL (Plus, 0, input)
    # LINQ calls this Aggregate; and may or may not specify the seed value; and allows a predicate
    FoldL (binaryOp, x0, x) = _Fold (PastValue,   binaryOp, x0, x)
    FoldR (binaryOp, x0, x) = _Fold (FutureValue, binaryOp, x0, x)
    _Fold (binaryOp, x0, x) = [
        acc = binaryOp (x, if Loop.IsFirst (x) then x0 else PastValue (acc))
        out = Last (acc)
    ].out
    # TODO: need a version that does not require an initial value--what would that be called?

    # LINQ-like operators
    Map (lambda, x) = lambda (x)     // that one's easy
    # Reverse (x) is a C++ node currently called TimeReverse

    # Gather and Scatter
    # We go through 3 nodes each to take advantage of x
    Gather  (cond, x) =  GatherPacked (      PackedIndex (x, Where (cond)), x) # 'cond' matches 'x'
    Scatter (cond, y) = ScatterPacked (cond, PackedIndex (y, Where (cond)), y) # 'cond' matches the result

    # sequence-altering LINQ-like operators
    # These generate new data packing (MBLayouts)

    # TakeWhile and DropWhile
    TakeWhile (predicate, x) = Filter ( _WhilePredicate (PastValue, predicate), x)
    SkipWhile (predicate, x) = Filter (!_WhilePredicate (PastValue, predicate), x)
    _WhilePredicate (DelayFn, predicate, input) =
    [
        whilePredicateRec = Boolean.And (DelayFn (whilePredicateRec, defaultHiddenActivation=Boolean.True), predicate)
    ].whilePredicateRec
    # TODO: do we need operations from the back?

    # First and Take
    # LINQ allows predicates as well.
    First (x)   =  Take (1, x)
    Take (N, x) = _Take (PastValue, N, x)
    _Take (DelayFn, N, x) = [
        selected = Loop._IsWithin (DelayFn, N, x)
        out = Gather (selected, x)
    ].out
    Skip (N, x) = if N > 0 then _Skip (PastValue, N, x) else x
    _Skip (DelayFn, N, x) = [ // TODO: merge with _Take
        selected = Loop._IsWithin (DelayFn, N, x)
        out = Gather (Boolean.Not (selected), x)
    ].out
    ElementAt (n, x) = [ // not efficient, as it filters twice. Better AND the predicates. TODO: what if n is out of range? ElementAtOrDefault
        startMask = Skip (n, x)                     // ...000111...
        mask = startMask - PastValue (0, startMask) // ...000100...
        out = Gather (mask, x)
    ]
    Single (predicate, x) = x

    #FirstOrDefault (x) = ? // can empty sequences exist or even be represented by CNTK?

    Average (x) = Sum (x) / Loop.Count(x)  // TODO: patch opQuotient to check 0/0 = 0
    Sum (x)    = FoldL (Plus,    0, x)
    LogSum (x) = FoldL (LogPlus, 0, x)
    #Max (x) = FoldL (^.Max, ?, x) // TODO: name clash; need to implement ^.
    #Min (x) = FoldL (^.Min, ?, x) // TODO: what's the init value?
    All (x) = FoldL (Boolean.And,  OnesLike (x), x)
    Any (x) = FoldL (Boolean.Or, ZeroesLike (x), x)

    # Join to create 2D fields for s2s attention?

    # Concat  (a Zip but in sequence dimension)
]

##############################################################################
# index operations
# These refer to the loop iteration itself.
##############################################################################

Loop = [
    # get the current iteration index w.r.t a node in a loop, such as a Delay node
    Iteration (x) = [
      agg = OnesLike (x) + PastValue(agg, defaultHiddenActivation=0) // a recurrence that sums up ones
    ].agg

    # get the total length of a sequence
    # TODO: in LINQ, this is an aggregation operation, so it would be long into Sequences
    Count(x) = Sequences.Last (1, Iteration (x)) // take last item of recurrence that sums up ones

    # is the current iteration the first/last of the loop?
    IsFirst (x) = _IsWithin (PastValue,   1, x)
    IsLast (x)  = _IsWithin (FutureValue, 1, x)

    IsFirstN (N, x) = _IsWithin (PastValue,   N, x)
    IsLastN  (N, x) = _IsWithin (FutureValue, N, x)

    # private helpers
    # flag whether a frame is within the first or last N frames
    _IsWithin (DelayFn/*PastValue or FutureValue*/, N, x) = DelayFn (0, Constants.ZeroesLike (x)/*false*/, timeStep=N, defaultHiddenActivation=Constants.True)

    # opposite of Id's "next"
    Previous (x) = PastValue (0, x, timeStep=1)
    Next (x) = FutureValue (0, x, timeStep=1)
]

##############################################################################
# parameter definitions
##############################################################################

Parameters =
[
    WeightParam (outputDim, inputDim) = Parameter (outputDim, inputDim, init='uniform', initValueScale=1, initOnCPUOnly=true, randomSeed=1)
    BiasParam (dim)                   = ParameterTensor (dim, init='fixedValue', value=0.0)
    ScalarParam()                     = Parameter (1, 1, init='fixedValue', value=0.0)

    # route input through an extra scalar weight, for stabilization
    Stabilize (x, enabled=true) =
        if enabled
        then [
                 beta = Exp (ScalarParam())
                 result = Scale (beta, x)
             ].result
        else x
]

##############################################################################
# recurrent networks
##############################################################################

RNNs =
[
    # LSTMP -- LSTM function with projection and self-stabilization
    # Projection it enabled by passing different values for outputDim and cellDim.
    # This is the stateless version that takes the previous state as an input.
    # It returns a dictionary with two members: h and c. prevState must be in the same format.
    // TODO: Standardize on one parameter order. Is first dimension the output (like in math, strcpy, or functional style) or the input (listing inputs first)?
    //       If we change this, we'd need to fix the LSTM end-to-end test.
    LSTMP (inputDim, outputDim, cellDim, x, prevState, enableSelfStabilization=false) =
    [
        _privateInnards = [       // encapsulate the privateInnards workings
            dh = prevState.h // previous values
            dc = prevState.c

            // parameter macros--these carry their own weight matrices
            B() = Parameters.BiasParam(cellDim)

            W(v) = Parameters.WeightParam (cellDim, inputDim)  * Parameters.Stabilize (v, enabled=enableSelfStabilization) // input-to-hidden
            H(h) = Parameters.WeightParam (cellDim, outputDim) * Parameters.Stabilize (h, enabled=enableSelfStabilization) // hidden-to-hidden
            C(c) = Parameters.WeightParam (cellDim, 1)        .* Parameters.Stabilize (c, enabled=enableSelfStabilization) // cell-to-hiddden (note: applied elementwise)

            // note: the W(x) here are all different, they all come with their own set of weights; same for H(dh), C(dc), and B()
            it = Sigmoid (W(x) + B() + H(dh) + C(dc))          // input gate(t)
            bit = it .* Tanh (W(x) + (H(dh) + B()))            // applied to tanh of input network

            ft = Sigmoid (W(x) + B() + H(dh) + C(dc))          // forget-me-not gate(t)
            bft = ft .* dc                                     // applied to cell(t-1)

            ct = bft + bit                                     // c(t) is sum of both

            ot = Sigmoid (W(x) + B() + H(dh) + C(ct))          // output gate(t)
            ht = ot .* Tanh (ct)                               // applied to tanh(cell(t))
        ]

        // LSTM cell
        # TODO: This is temporary test code for the new ShiftNode (until we switch PastValue() itself over)
        #PastValueShift(dimDummy, input) = Shift(input, /*fromOffsets=*/-1, /*boundaryValue=*/Constant(0.1), dim=-1)
        #PastValue1 = PastValue
        #PastValue1 = PastValueShift

        # our return values
        c = _privateInnards.ct          // cell value
        h = if outputDim != cellDim     // output/hidden state
            then [                      // project
                Wmr = Parameters.WeightParam(outputDim, cellDim);
                htp = Wmr * Parameters.Stabilize (_privateInnards.ht, enabled=enableSelfStabilization)
            ].htp         // TODO: ^^ extend BS syntax to allow to say: then [ Wmr = WeightParam(outputDim, cellDim) ] in Wmr * Stabilize (...)
            else _privateInnards.ht     // no projection
    ]

    # this implements a recurrent (stateful) LSTM with projection and self-stabilization
    RecurrentLSTMP (inputDim, outputDim, cellDim, x, enableSelfStabilization=false) =
    [
        prevState =
        [
            h = Loop.Previous (lstmState.h)             // hidden state(t-1)
            c = Loop.Previous (lstmState.c)             // cell(t-1)
        ]
        enableSelfStabilization1 = enableSelfStabilization // TODO: BS syntax needs to allow to say ^.enableSelfStabilization
        lstmState = LSTMP (inputDim, outputDim, cellDim, x, prevState, enableSelfStabilization=enableSelfStabilization1)
    ].lstmState.h // that's the value we return
]

##############################################################################
# Network operations
# These operations will have undefined behavior for input values != 0 or 1.
##############################################################################

Network = [
    Load(pathName) = new ComputationNetworkFromFile [ /*pathName; also needs 'precision' somewhere*/ ]
    Edit(inputModel, editFunctions, additionalRoots) = new ComputationNetworkWithEdits [ /*inputModel, editFunctions, additionalRoots*/ ]

    Editing = [
        // Create a lambda that returns its argument unless that argument == 'old', then it will return 'replacement'
        ReplaceLinksToNode (old, replacement) = (node => if IsSameObject (node, old) then replacement else node)
        ReplaceLinksToNamedNode (name, replacement) = (node => if node.name == name then replacement else node)
    ]
]

] # end of BS namespace

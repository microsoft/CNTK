//
// Copyright (c) Microsoft. All rights reserved.
// Licensed under the MIT license. See LICENSE.md file in the project root for full license information.
//
// CNTK.core.bs -- core BrainScript library including both general and CNTK-specific definitions
//

##############################################################################
# standard functions
##############################################################################

Print(value, format='') = new PrintAction [ what = value /*; how = format*/ ] 
Fail(what) = new FailAction [ /*what*/ ]
Format(value, format) = new StringFunction [ what = 'Format' ; arg = value ; how = format ] 
Replace(s, from, to) = new StringFunction [ what = 'Replace' ; arg = s ; replacewhat = from ; withwhat = to ] 
Substr(s, begin, num) = new StringFunction [ what = 'Substr' ; arg = s ; pos = begin ; chars = num ] 
Chr(c) = new StringFunction [ what = 'Chr' ;  arg = c ] 
Length(x) = new NumericFunction [ what = 'Length' ; arg = x ] 
Sign(x) = if x > 0 then 1 else if x < 0 then -1 else 0 
Min(a,b) = if a < b then a else b 
Max(a,b) = if a > b then a else b 
Fac(n) = if n > 1 then Fac(n-1) * n else 1 
IsSameObject(a,b) = new CompareFunction [ what = 'IsSameObject' ; args = (a : b) ]
Mod(x, y)  = new NumericFunction [ what = 'Mod' ;  args = (x:y) ] 
IntDiv(x, y)  = new NumericFunction [ what = 'IntDiv' ;  args = (x:y) ] 


##############################################################################
# comparison functions
##############################################################################

Less         = CNTK2.Less
Equal        = CNTK2.Equal
Greater      = CNTK2.Greater
GreaterEqual = CNTK2.GreaterEqual
NotEqual     = CNTK2.NotEqual
LessEqual    = CNTK2.LessEqual

##############################################################################
# ComputationNodes
##############################################################################

##############################################################################
# "Stable API" with the purpose of staying compatible towards 2.0. 
# - Use only tensors as concept. Move away from matrices.
# - Main input goes first
# - Main input is called "_" (becomes either self, or is moved to the end, 
#   depending on language binding)
# - tensor shape is called "shape"
# - output shape before input shape
# Operator list is sorted alphabetically within the category.
##############################################################################
CNTK2 = [
    # Currently restricted to operators introduced with Python API in CNTK 1.4.

    // 1. Inputs
    // Changes: dims -> shape
    DynamicAxis(tag='') = new ComputationNode [ operation = 'DynamicAxis' ; /*plus the function args*/  ]
    Input(shape, dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'InputValue' ; shape = new TensorShape [ /*shape*/ ] ; isImage = false /*plus the function args*/ ]
    
    // 2. Variables and constants
    // Changes: ParameterTensor -> _Parameter; "dims" -> "shape"
    // Python API: 
    // - constant(value, name=None) - value: the tensor constant passed as numpy array. Forwards to parameter() with learningRateMultiplier=0 and initFromLiteral
    // - parameter: Like below, but can take an NDArray on the "init_from_literal" parameter, in which case it is serialized and turned into "initFromLiteral".
    //              (TODO: should be the value parameter instead)
    // TODO: The API for Parameter is different in current 2.0 design, getting a constant as input for the initial values. 
    // This needs to be fixed to follow the way the Constant() is exposed in Python
    // Making this an internal node with "_" until we agree on the final interface:
    _Parameter(shape, value = 0, learningRateMultiplier = 1.0, init = 'uniform'/*|fixedValue|gaussian|fromFile|fromLiteral*/, initValueScale = 1, initFromFilePath = '', initFromLiteral = '', initOnCPUOnly=true, randomSeed=-1, tag='') = new ComputationNode [ operation = 'LearnableParameter' ; shape = new TensorShape [ /*shape */ ] /*plus the function args*/ ]

    // 3. Shape operations
    // Changes: NewReshape -> Reshape, input -> _, dims -> shape
    Reshape(_, shape, beginAxis=0, endAxis=0, tag='') = new ComputationNode [ operation = 'Reshape' ; inputs = _ ; shape = new TensorShape [ /*shape*/ ] /*plus the function args*/ ]
    Slice(_, beginIndex, endIndex, axis=1, tag='') =
        if axis < 0 then [ # time axis: specify -1
            beginFlags = if beginIndex > 0 then BS.Boolean.Not (BS.Loop.IsFirstN (beginIndex, _)) else                 BS.Loop.IsLastN  (-beginIndex, _)
            endFlags   = if endIndex   > 0 then                 BS.Loop.IsFirstN (endIndex,   _)  else BS.Boolean.Not (BS.Loop.IsLastN  (-endIndex,   _))
            flags = if      beginIndex == 0 then endFlags
                    else if endIndex   == 0 then beginFlags
                    else                         BS.Boolean.And (beginFlags, endFlags)
            out = if beginIndex == 0 && endIndex == 0
                  then _
                  else BS.Sequences.Gather (flags, _)
        ].out
        else new ComputationNode [ operation = 'Slice' ; inputs = _ /*plus the function args*/ ] # non-time axis

    Splice (_, axis=1, tag='') = # TODO: This is a workaround. RowStack itself shall interpret 'axis' and be renamed to Splice().
		if axis < 1 then Fail('Splice does not yet implement splicing the time axis.')
		else if axis == 1 then [tag1=tag; out = RowStack (_, tag=tag1)].out
		else [ # workaround: swap 'axis' to first position, RowStack, swap back
			ArrayTransposeDimensions (_, axis1, axis2) = [ # transpose each element of a BS array
				inputsT[i:0..Length(_)-1] = TransposeDimensions (_[i], axis1, axis2)
			].inputsT
			out = [tag1=tag; out=TransposeDimensions (RowStack (ArrayTransposeDimensions (_, 1, axis)), 1, axis, tag=tag)].out
		].out

    // Swap two axes of a tensor
    TransposeDimensions(_, axis1, axis2, tag='') = new ComputationNode [ operation = 'TransposeDimensions' ; inputs = _ /*plus the function args*/ ]

    // 4. Tensor operations
    // Changes: Matrix -> Tensor. A -> x, B -> y. Data must come on y ("default parameter") hence not using _
    Times(x, y, outputRank=1, tag='') = new ComputationNode [ operation = 'Times' ; inputs = ( x : y ) /*plus the function args*/ ]

    // 5. Elementwise operations.
    // Changes: "Matrix" -> "Tensor"; left input -> _; Clip: move input to front. ElementDivide/Times: anotherTensor -> y
    Abs(_, tag='') = new ComputationNode [ operation = 'Abs' ; inputs = _ /*plus the function args*/ ]
    Ceil(_, tag='') = Negate(Floor(Negate(_)), tag=tag)
    Clip(_, minValue, maxValue, tag='') = new ComputationNode [ operation = 'Clip' ; inputs = (minValue : maxValue : _) /* plus the function args*/ ]
    ElementDivide(_, y, tag='') = ElementTimes(_, Reciprocal(y), tag=tag)
    ElementTimes(_, y, tag='') = new ComputationNode [ operation = 'ElementTimes' ; inputs = (_ : y) /*plus the function args*/ ]
    Exp(_, tag='') = new ComputationNode [ operation = 'Exp' ; inputs = _ /*plus the function args*/ ]
    Floor(_, tag='') = new ComputationNode [ operation = 'Floor' ; inputs = _ /*plus the function args*/ ]
    Log(_, tag='') = new ComputationNode [ operation = 'Log' ; inputs = _ /*plus the function args*/ ]
    Minus(_, y, tag='') = new ComputationNode [ operation = 'Minus' ; inputs = (_ : y) /*plus the function args*/ ]
    Plus(_, y, tag='') = new ComputationNode [ operation = 'Plus' ; inputs = (_ : y) /*plus the function args*/ ]
    Round(_, tag='') = Floor(Plus(_, ConstantTensor(0.5, (1))), tag=tag)
    Sqrt(_, tag='') = new ComputationNode [ operation = 'Sqrt' ; inputs = _ /*plus the function args*/ ]
    Square(_, tag='') = ElementTimes(_, _, tag=tag)
    Tanh(_, tag='') = new ComputationNode [ operation = 'Tanh' ; inputs = _ /*plus the function args*/ ]

    // 6. Reductions
    // None so far

    // 7. Control flow (if, composite etc.)
    // None so far

    // 8. Boolean operations
    // None so far

    // 9. Recurrent operations
    // Changes: input first; input -> _
    FutureValue(_, shape, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'FutureValue' ; inputs = _ ; shape = new TensorShape [ /*shape*/ ] /*plus the function args*/ ]
    PastValue(_, shape, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'PastValue' ; inputs = _ ; shape = new TensorShape [ /*shape*/ ] /*plus the function args*/ ]

    // 10. NN-specific operations
    // Changes: input -> _, RectifiedLinear -> Relu. [Use Relu to arrive at relu() in snake_case]
    Relu(_, tag='') = new ComputationNode [ operation = 'RectifiedLinear' ; inputs = _ /*plus the function args*/ ]
    Sigmoid(_, tag='') = new ComputationNode [ operation = 'Sigmoid' ; inputs = _ /*plus the function args*/ ]
    Softmax(_, tag='') = new ComputationNode [ operation = 'Softmax' ; inputs = _ /*plus the function args*/ ]
    Dropout(_, tag='') = new ComputationNode [ operation = 'Dropout' ; inputs = _ /*plus the function args*/ ]

    // 11. Criterion nodes
    // No changes here - we said the default input would be the label sequence here, against which the 
    // empirical sequence is compared to. Keeping this for now.
    CrossEntropyWithSoftmax(_, outProbVectorSequence, tag='') = new ComputationNode [ operation = 'CrossEntropyWithSoftmax' ; inputs = (_ : outProbVectorSequence) /*plus the function args*/ ]
    ErrorPrediction(_, outVectorSequence, tag='') = new ComputationNode [ operation = 'ErrorPrediction' ; inputs = (_ : outVectorSequence) /*plus the function args*/ ]

    // 13. Comparison nodes
    Less(_, y, tag='')         = new ComputationNode [ operation = 'Less'         ; inputs = (_ : y) /*plus the function args*/ ]
    Equal(_, y, tag='')        = new ComputationNode [ operation = 'Equal'        ; inputs = (_ : y) /*plus the function args*/ ]
    Greater(_, y, tag='')      = new ComputationNode [ operation = 'Greater'      ; inputs = (_ : y) /*plus the function args*/ ]
    GreaterEqual(_, y, tag='') = new ComputationNode [ operation = 'GreaterEqual' ; inputs = (_ : y) /*plus the function args*/ ]
    NotEqual(_, y, tag='')     = new ComputationNode [ operation = 'NotEqual'     ; inputs = (_ : y) /*plus the function args*/ ]
    LessEqual(_, y, tag='')    = new ComputationNode [ operation = 'LessEqual'    ; inputs = (_ : y) /*plus the function args*/ ]

    // 13. Others
    // 12. Others
    Identity(_, tag='') = new ComputationNode [ operation = 'Pass' ; inputs = _ /*plus the function args*/ ]    
]

LearnableParameter (outputDim, inputDim, learningRateMultiplier = 1.0, init = 'uniform'/*|fixedValue|gaussian|fromFile|fromLiteral*/, initValueScale = 1, value = 0, initFromFilePath = '', initFromLiteral = '', initOnCPUOnly=true, randomSeed=-1, tag='') = new ComputationNode [ operation = 'LearnableParameter' ; shape = new TensorShape [ dims = (outputDim : inputDim) ] /*plus the function args*/ ]
Parameter = LearnableParameter // deprecated 
# TODO: make Parameter take tensor dims?
ParameterTensor(dims, learningRateMultiplier = 1.0, init = 'uniform'/*|fixedValue|gaussian|fromFile|fromLiteral*/, initValueScale = 1, value = 0, initFromFilePath = '', initFromLiteral = '', initOnCPUOnly=true, randomSeed=-1, tag='') = new ComputationNode [ operation = 'LearnableParameter' ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
ConstantFromString(literal, tag='') = ParameterTensor((0)/*dim, will be inferred*/, init = 'fromLiteral', initFromLiteral = literal, learningRateMultiplier = 0.0)
DynamicAxis(tag='') = new ComputationNode [ operation = 'DynamicAxis' ; /*plus the function args*/  ]
Input(dims, dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'InputValue' ; shape = new TensorShape [ /*dims*/ ] ; isImage = false /*plus the function args*/ ]
# TODO: change from dynamicAxis by name to dynamicAxis being an actual object
SparseInput(dims, dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'SparseInputValue' ; shape = new TensorShape [ /*dims*/ ] ; isImage = false /*plus the function args*/ ]
ImageInput(imageWidth, imageHeight, imageChannels, imageLayout='CHW', dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'InputValue' ; isImage = true /*plus the function args*/ ]
SparseImageInput(imageWidth, imageHeight, imageChannels, imageLayout='CHW', dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'SparseInputValue' ; isImage = true /*plus the function args*/ ]
EnvironmentInput(propertyName, tag='') = new ComputationNode [ operation = 'EnvironmentInput' /*plus the function args*/ ]
# TODO: make 'dims' the first parameter, think ConstantTensor<dims> (val)
ConstantTensor(val, dims, tag='') = ParameterTensor(dims, learningRateMultiplier = 0, init = 'fixedValue', value = val) 
Constant(val, rows = 1, cols = 1, tag='') = Parameter(rows, cols, learningRateMultiplier = 0, init = 'fixedValue', value = val) 
PastValue(dims, input, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'PastValue' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
FutureValue(dims, input, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'FutureValue' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
Shift(input, fromOffset, boundaryValue, boundaryMode=-1/*context*/, dim=-1, tag='') = new ComputationNode [ operation = 'Shift' ; inputs = (input : boundaryValue) /*plus the function args*/ ]
RowSlice(beginIndex, numRows, input, tag='') = Slice(beginIndex, beginIndex + numRows, input, axis = 1)
RowRepeat(input, numRepeats, tag='') = new ComputationNode [ operation = 'RowRepeat' ; inputs = input /*plus the function args*/ ]
RowStack(inputs, tag='') = new ComputationNode [ operation = 'RowStack' /*plus the function args*/ ]
Splice (inputs, axis=1, tag='') = # TODO: This is a workaround. RowStack itself shall interpret 'axis' and be renamed to Splice().
    if axis < 1 then Fail('Splice does not yet implement splicing the time axis.')
    else if axis == 1 then [tag1=tag; out = RowStack (inputs, tag=tag1)].out
    else [ # workaround: swap 'axis' to first position, RowStack, swap back
        ArrayTransposeDimensions (inputs, axis1, axis2) = [ # transpose each element of a BS array
            inputsT[i:0..Length(inputs)-1] = TransposeDimensions (inputs[i], axis1, axis2)
        ].inputsT
        out = [tag1=tag; out=TransposeDimensions (RowStack (ArrayTransposeDimensions (inputs, 1, axis)), 1, axis, tag=tag)].out
    ].out
Reshape(input, numRows, imageWidth = 0, imageHeight = 0, imageChannels = 0, tag='') = new ComputationNode [ operation = 'LegacyReshape' ; inputs = input /*plus the function args*/ ]
NewReshape(input, dims, beginAxis=0, endAxis=0, tag='') = new ComputationNode [ operation = 'Reshape' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
ReshapeDimension(x, axis, tensorShape) = NewReshape(x, tensorShape, beginAxis=axis, endAxis=axis + 1) 
FlattenDimensions(x, axis, num) = NewReshape(x, 0, beginAxis=axis, endAxis=axis + num) 
Slice(beginIndex, endIndex, input, axis=1, tag='') =
    if axis < 0 then [ # time axis: specify -1
        beginFlags = if beginIndex > 0 then BS.Boolean.Not (BS.Loop.IsFirstN (beginIndex, input)) else                 BS.Loop.IsLastN  (-beginIndex, input)
        endFlags   = if endIndex   > 0 then                 BS.Loop.IsFirstN (endIndex,   input)  else BS.Boolean.Not (BS.Loop.IsLastN  (-endIndex,   input))
        flags = if      beginIndex == 0 then endFlags
                else if endIndex   == 0 then beginFlags
                else                         BS.Boolean.And (beginFlags, endFlags)
        out = if beginIndex == 0 && endIndex == 0
              then input
              else BS.Sequences.Gather (flags, input)
    ].out
    else new ComputationNode [ operation = 'Slice' ; inputs = input /*plus the function args*/ ] # non-time axis
SplitDimension(x, axis, N) = ReshapeDimension(x, axis, 0:N) 
TransposeDimensions(input, axis1, axis2, tag='') = new ComputationNode [ operation = 'TransposeDimensions' ; inputs = input /*plus the function args*/ ]
# TODO: make input the last arg!
Transpose(x) = TransposeDimensions(x, 1, 2)
Times(A, B, outputRank=1, tag='') = new ComputationNode [ operation = 'Times' ; inputs = ( A : B ) /*plus the function args*/ ]
Logistic(label, probability, tag='') = new ComputationNode [ operation = 'Logistic' ; inputs = (label : probability) /*plus the function args*/ ]
WeightedLogistic(label, probability, instanceWeight, tag='') = new ComputationNode [ operation = 'Logistic' ; inputs = (label : probability : instanceWeight) /*plus the function args*/ ]
ReconcileDynamicAxis(dataInput, layoutInput, tag='') = new ComputationNode [ operation = 'ReconcileDynamicAxis' ; inputs = (dataInput : layoutInput) /*plus the function args*/ ]
ReconcileMBLayout = ReconcileDynamicAxis # back compat
CastAs (type, data) = ReconcileDynamicAxis (data, type) # read as CastAs<type>(data) where the cast may consist of rearranging the data w.r.t. MBLayout or broadcasting across sequence items
Convolution(weightNode, inputValueNode, kernelDims, mapDims = 1, stride = 1, sharing = true, autoPadding = true, lowerPad = 0, upperPad = 0, imageLayout='CHW', maxTempMemSizeInSamples = 0, tag='') = new ComputationNode [ operation = 'Convolution' ; inputs = (weightNode : inputValueNode); kernelShape = new TensorShape [ dims = kernelDims ] ; mapCount = new TensorShape [ dims = mapDims ] ; strideShape = new TensorShape [ dims = stride ] ; dimSharing = new BoolVector [ items = sharing ] ; dimPadding = new BoolVector [ items = autoPadding ] ; dimPadLower = new TensorShape [ dims = lowerPad ] ; dimPadUpper = new TensorShape [ dims = upperPad ] /*plus the function args*/ ]
Pooling(input, poolKind/*'max'|'average'*/, kernelDims, stride=1, autoPadding = true, lowerPad = 0, upperPad = 0, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'Pooling' ; inputs = (input); pool = poolKind ; kernelShape = new TensorShape [ dims = kernelDims ] ; strideShape = new TensorShape [ dims = stride ] ; dimPadding = new BoolVector [ items = autoPadding ] ; dimPadLower = new TensorShape [ dims = lowerPad ] ; dimPadUpper = new TensorShape [ dims = upperPad ] /*plus the function args*/ ]
MaxPooling(input, windowWidth, windowHeight, horizontalSubsample, verticalSubsample, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'MaxPooling' ; inputs = input /*plus the function args*/ ]
AveragePooling(input, windowWidth, windowHeight, horizontalSubsample, verticalSubsample, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'AveragePooling' ; inputs = input /*plus the function args*/ ]
ColumnwiseCrossProduct = KhatriRaoProduct // deprecated 
ClassificationError = ErrorPrediction 
Delay = PastValue 

BatchNormalization(input, scale, bias, runMean, runInvStdDev, spatial, normalizationTimeConstant = 0, blendTimeConstant = 0, epsilon = 0.00001, useCntkEngine = true, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'BatchNormalization' ; inputs = (input : scale : bias : runMean : runInvStdDev) /*plus the function args*/ ]
Abs(x, tag='') = new ComputationNode [ operation = 'Abs' ; inputs = x /*plus the function args*/ ]
Ceil(x, tag='') = Negate(Floor(Negate(x)), tag=tag)
ClassBasedCrossEntropyWithSoftmax(labelClassDescriptorVectorSequence, mainInputInfo, mainWeight, classLogProbsBeforeSoftmax, tag='') = new ComputationNode [ operation = 'ClassBasedCrossEntropyWithSoftmax' ; inputs = (labelClassDescriptorVectorSequence : mainInputInfo : mainWeight : classLogProbsBeforeSoftmax) /*plus the function args*/ ]
Clip(minValue, maxValue, x, tag='') = new ComputationNode [ operation = 'Clip' ; inputs = (minValue : maxValue : x) /* plus the function args*/ ]
ColumnElementTimes(aVectorSequence, anotherVectorSequence, tag='') = new ComputationNode [ operation = 'ColumnElementTimes' ; inputs = (aVectorSequence : anotherVectorSequence) /*plus the function args*/ ]
// TODO: ColumnElementTimes = ElementTimes
CosDistance(aVectorSequence, anotherVectorSequence, tag='') = new ComputationNode [ operation = 'CosDistance' ; inputs = (aVectorSequence : anotherVectorSequence) /*plus the function args*/ ]
CosDistanceWithNegativeSamples(aVectorSequence, anotherVectorSequence, numShifts, numNegSamples, tag='') = new ComputationNode [ operation = 'CosDistanceWithNegativeSamples' ; inputs = (aVectorSequence : anotherVectorSequence : numShifts : numNegSamples) /*plus the function args*/ ]
Cosine(x, tag='') = new ComputationNode [ operation = 'Cosine' ; inputs = x /*plus the function args*/ ]
CrossEntropy(refProbVectorSequence, outProbVectorSequence, tag='') = new ComputationNode [ operation = 'CrossEntropy' ; inputs = (refProbVectorSequence : outProbVectorSequence) /*plus the function args*/ ]
CrossEntropyWithSoftmax(labelVectorSequence, outProbVectorSequence, tag='') = new ComputationNode [ operation = 'CrossEntropyWithSoftmax' ; inputs = (labelVectorSequence : outProbVectorSequence) /*plus the function args*/ ]
# once ReduceLogSum becomes proper C++, CrossEntropyWithSoftmax() will become this:
NewCrossEntropyWithSoftmax (labelSequence, z, tag='') = [ tag1 = tag; out = Minus (ReduceLogSum (z), ReduceSum (labelSequence .* z), tag=tag1) ].out
DiagTimes(diagonalMatrixAsColumnVector, matrix, tag='') = new ComputationNode [ operation = 'DiagTimes' ; inputs = (diagonalMatrixAsColumnVector : matrix) /*plus the function args*/ ]
// TODO: DiagTimes = ElementTimes
Dropout(activationVectorSequence, tag='') = new ComputationNode [ operation = 'Dropout' ; inputs = activationVectorSequence /*plus the function args*/ ]
ElementTimes(aMatrix, anotherMatrix, tag='') = new ComputationNode [ operation = 'ElementTimes' ; inputs = (aMatrix : anotherMatrix) /*plus the function args*/ ]
ElementDivide(aMatrix, anotherMatrix, tag='') = ElementTimes(aMatrix, Reciprocal(anotherMatrix), tag=tag)
ErrorPrediction(labelVectorSequence, outVectorSequence, tag='') = new ComputationNode [ operation = 'ErrorPrediction' ; inputs = (labelVectorSequence : outVectorSequence) /*plus the function args*/ ]
Exp(x, tag='') = new ComputationNode [ operation = 'Exp' ; inputs = x /*plus the function args*/ ]
Floor(x, tag='') = new ComputationNode [ operation = 'Floor' ; inputs = x /*plus the function args*/ ]
GatherPacked(indexSequence, sourceData, tag='') = new ComputationNode [ operation = 'GatherPacked' ; inputs = (indexSequence : sourceData) /*plus the function args*/ ]
GMMLogLikelihood(unnormalizedPriorVector, meansAsRows, logStdDevAsRows, dataVectorSequence, tag='') = new ComputationNode [ operation = 'GMMLogLikelihood' ; inputs = (unnormalizedPriorVector : meansAsRows : logStdDevAsRows : dataVectorSequence) /*plus the function args*/ ]
InvStdDev(dataVectorSequence, tag='') = new ComputationNode [ operation = 'InvStdDev' ; inputs = dataVectorSequence /*plus the function args*/ ]
KhatriRaoProduct(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'KhatriRaoProduct' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Log(x, tag='') = new ComputationNode [ operation = 'Log' ; inputs = x /*plus the function args*/ ]
LogPlus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'LogPlus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
LogSoftmax(z, tag='') = new ComputationNode [ operation = 'LogSoftmax' ; inputs = z /*plus the function args*/ ]
# TODO: ^^ along axis, like Softmax
MatrixL1Reg(matrix, tag='') = new ComputationNode [ operation = 'MatrixL1Reg' ; inputs = matrix /*plus the function args*/ ]
MatrixL2Reg(matrix, tag='') = new ComputationNode [ operation = 'MatrixL2Reg' ; inputs = matrix /*plus the function args*/ ]
Mean(dataVectorSequence, tag='') = new ComputationNode [ operation = 'Mean' ; inputs = dataVectorSequence /*plus the function args*/ ]
Minus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'Minus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Negate(input, tag='') = new ComputationNode [ operation = 'Negate' ; inputs = input /*plus the function args*/ ]
PackedIndex(targetObject, indexSequence, tag='') = new ComputationNode [ operation = 'PackedIndex' ; inputs = (targetObject : indexSequence) /*plus the function args*/ ]
Pass(x, tag='') = new ComputationNode [ operation = 'Pass' ; inputs = x /*plus the function args*/ ]
PerDimMeanVarDeNormalization(dataVectorSequence, meanVector, invStdDevVector, tag='') = new ComputationNode [ operation = 'PerDimMeanVarDeNormalization' ; inputs = (dataVectorSequence : meanVector : invStdDevVector) /*plus the function args*/ ]
PerDimMeanVarNormalization(dataVectorSequence, meanVector, invStdDevVector, tag='') = new ComputationNode [ operation = 'PerDimMeanVarNormalization' ; inputs = (dataVectorSequence : meanVector : invStdDevVector) /*plus the function args*/ ]
Plus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'Plus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Reciprocal(z, tag='') = new ComputationNode [ operation = 'Reciprocal' ; inputs = z /*plus the function args*/ ]
RectifiedLinear(z, tag='') = new ComputationNode [ operation = 'RectifiedLinear' ; inputs = z /*plus the function args*/ ]
ReduceSum (z, axis=0, tag='')     = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Sum"    /*plus the function args*/ ]
# the following is a temporary workaround until we have the C++ version
ReduceLogSum (z, axis=0, tag='')  = if axis != 0 then Fail("ReduceLogSum for now only supports axis=0.")
    else [ tag1=tag ; axis1=axis ; out = RowSlice (0, 1, z - LogSoftmax (z), tag=tag1) ].out
#ReduceLogSum (z, axis=0, tag='')  = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "LogSum" /*plus the function args*/ ]
#ReduceMean (z, axis=0, tag='')    = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Mean"    /*plus the function args*/ ]
#ReduceMax (z, axis=0, tag='')     = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Max"     /*plus the function args*/ ]
#ReduceMin (z, axis=0, tag='')     = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Min"     /*plus the function args*/ ]
Round(x, tag='') = Floor(Plus(x, ConstantTensor(0.5, (1))), tag=tag)
Scale(scalarScalingFactor, matrix, tag='') = new ComputationNode [ operation = 'Scale' ; inputs = (scalarScalingFactor : matrix) /*plus the function args*/ ]
# TODO: Scale = ElementTimes
ScatterPacked(cond, indexSequence, sourceData, tag='') = new ComputationNode [ operation = 'ScatterPacked' ; inputs = (cond : indexSequence : sourceData) /*plus the function args*/ ]
Sigmoid(z, tag='') = new ComputationNode [ operation = 'Sigmoid' ; inputs = z /*plus the function args*/ ]
Sin(z, tag='') = new ComputationNode [ operation = 'Sin' ; inputs = z /*plus the function args*/ ]
Softmax (z, axis=0, tag='') =  # TODO: replace this with more efficient version below once we have ReduceLogSum
    if axis == 0 then new ComputationNode [ operation = 'Softmax' ; inputs = z /*plus the function args*/ ]
    else
    [
        numerator = Softmax (z)  # do a first Softmax to bring it into harmless numeric range
        denominator = ReduceSum (axis=axis1, numerator) ; axis1 = axis # reduce along axis
        P = numerator .* Reciprocal (denominator)         # normalize numerator by the sum along the given axis

        # TODO: This is not efficient. Once we have ReduceLogSum, it will be this:
        #Z = ReduceLogSum (axis=axis0, z) # reduce along axis
        #P = Exp (z - Z)
    ].P
Hardmax(z, tag='') = new ComputationNode [ operation = 'Hardmax' ; inputs = z /*plus the function args*/ ]
Sqrt(z, tag='') = new ComputationNode [ operation = 'Sqrt' ; inputs = z /*plus the function args*/ ]
SquareError(aMatrix, anotherMatrix, tag='') = new ComputationNode [ operation = 'SquareError' ; inputs = (aMatrix : anotherMatrix) /*plus the function args*/ ]
SumColumnElements(z, tag='') = new ComputationNode [ operation = 'SumColumnElements' ; inputs = z /*plus the function args*/ ] # deprecated
SumElements(matrix, tag='') = new ComputationNode [ operation = 'SumElements' ; inputs = matrix /*plus the function args*/ ]
# ^^ TODO: Rename to ReduceSumMB?
Tanh(z, tag='') = new ComputationNode [ operation = 'Tanh' ; inputs = z /*plus the function args*/ ]
TimeReverse(vectorSequence, tag='') = new ComputationNode [ operation = 'TimeReverse' ; inputs = vectorSequence /*plus the function args*/ ]
Trace (node, say='', logFrequency=100, logFirst=10, logGradientToo=false, onlyUpToRow=100000000, onlyUpToT=100000000, format=[], tag='') = new ComputationNode [ operation = 'Trace' ; inputs = node ]
TransposeTimes(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'TransposeTimes' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Where(cond, tag='') = new ComputationNode [ operation = 'Where' ; inputs = cond /*plus the function args*/ ]

##############################################################################
# common macros
##############################################################################

BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols) ; z = W*in+B ] 
SBFF(in, rows, cols) = [ Eh = Sigmoid(BFF(in, rows, cols).z) ] 
MeanVarNorm(feat) = PerDimMeanVarNormalization(feat, Mean(feat), InvStdDev(feat)) 
LogPrior(labels) = Log(Mean(labels)) 

Embedding (embeddingDim, input, inputDim=input.dim, initFrom='fromFile'/*|gaussian|uniform*/, embeddingPath = '', sparseInput = false, learningRateWeight = 0.0) = [
    embedding = Transpose (LearnableParameter (inputDim, embeddingDim, learningRateMultiplier = learningRateWeight, init = initFrom, initFromFilePath = embeddingPath))
    lookup = if sparseInput then embedding * input
             else GatherPacked (input, embedding)
].lookup

##############################################################################
# the more specific standard things are in a namespace called 'BS'
# You can create shorthands for accessing these, e.g. saying B = BS.Boolean.
# Note: Identifiers beginning with _ should be considered for library use only.
##############################################################################

BS = [

##############################################################################
# Basic constants
##############################################################################

Constants = [
    Zero = ConstantTensor (0, (1))
    One  = ConstantTensor (1, (1))
    OnesTensor (dims) = ConstantTensor (1, dims)
    # BUGBUG: ZeroesLike() would recreate the full dimension of x. Well, no need if it considers broadcasting. But still wrong if we want to broadcast a vector of different tensor dim.
    #ZeroesLike (x) = CastAs (x, Zero) // read: Cast<x>(Zero)
    #OnesLike (x)   = CastAs (x, One)
    # CastAs() does not implement broadcasting
    ZeroesLike (x) = SumColumnElements (RowSlice (0, 1, x) .* Zero)  // hack: get one row of input and multiply with zero; double-hack: reduce extra tensor dims by SumCol
    ZeroSequenceLike = ZeroesLike   # TODO: this should yield a scalar sequence, while ZeroesLike should be a tensor
    ZeroesLike1 (x) = x .* Zero     # get a tensor of zeroes of same dim as x  TODO: Do this as a C++ node (will be simple)
    OnesLike (x) = ZeroesLike (x) + One
    # is this like Sequences.Repeat?
    True  = 1
    False = 0
    None = ConstantTensor (42, (1))
    IsNone (x) = IsSameObject (x, None)
]

##############################################################################
# Boolean operations
# These operations will have undefined behavior for input values != 0 or 1.
##############################################################################

# boolean helpers
Boolean = [
    True  = 1
    False = 0

    # basic logical operations
    And (a,b) =         a .* b
    Or  (a,b) = a + b - a .* b
    Xor (a,b) = a + b - a .* b * Constant (2)
    Not (x)   = Constants.One - x

    # on each time step where clk 1, this toggles its value
    Toggle (clk, initialValue=False) = [
        state = Xor (PastValue (1, state, defaultHiddenActivation=initialValue), clk)
    ].state

    # select a value
    # Note: This will be replaced by BrainScript 'if cond then thenVal else elseVal' and SwitchNode
    If (cond, thenVal, elseVal, tag='') =  new ComputationNode [ operation = 'If' ; inputs = (cond : thenVal : elseVal) /*plus the function args*/ ]
    #If (cond, thenVal, elseVal) = cond .* thenVal + Not (cond) .* elseVal
]

##############################################################################
# sequence operations
# These mimic LINQ operations.
##############################################################################

Sequences = [
    # broadcast a single-step sequence to a multi-step sequence
    BroadcastSequenceAs (type, data1) = [                      # type=example sequence with desired length (outside of a loop), data1=1 time step
        # BUGBUG: This should work but gives worse results.
        #ZeroSequenceLike (x) = RowSlice (0, 1, x) .* Constants.Zero # BUGBUG: SumColumnElements() has a CPU/GPU problem
        #index = /*Constants.*/ZeroSequenceLike (type)  # create an index sequence [ 0 0 0 ... ] of target length
        #packedIndex = PackedIndex (data1, index)       # convert into internal packed index w.r.t. 'data1'
        #out = GatherPacked (packedIndex, data1)        # copy data1[0] to all elements, total length like 'type'

        # alternative (slower, older) implementation (10% slower end-to-end?)
        # Gives nearly the same result, but not completely. Since Gather() above has an atomicAdd(), let's leave this on for now and check later.
        dataPadded = Sequences.Scatter (Loop.IsFirst (type), data1) # padded with zeroes until end of target sequence
        out = Boolean.If (Loop.IsFirst (dataPadded), # if first entry
                 /*then*/ dataPadded,                # then copy that
                 /*else*/ Loop.Previous (out))       # else just propagate to the front
    ].out

    # rolling window over past N samples
    # returns a record [ value=..., valid=... ], both being 1-step sequences of [dim x N]. N can optionally be moved to axes >2.
    # This implementation is suboptimal in that it creates copies for the intermediate steps.
    PastValueWindow (N, in, axis=2) = [
        isLast = Loop.IsLast (in)
        isLastIndex = PackedIndex (in, Where (isLast))
        GatherLast (x) = GatherPacked (isLastIndex, x) # 'cond' matches 'x'
        onesLikeIn = Constants.OnesLike (in)
        delayLine[t:0..N-1] = [     # shift register for encoder, last N inputs
            value = if t == 0
                    then in         # delay 0: current value
                    else PastValue (0, in, timeStep=t, defaultHiddenActivation=0)
            valid = if t == 0
                    then onesLikeIn   # BUGBUG: if I say Constant.Ones here, it outputs 0. Ones has no MBLayout
                    else PastValue (1, onesLikeIn, timeStep=t, defaultHiddenActivation=0)


            TraceDenseTransposed (h, what) = h
            #    Trace (h, say=what, logFirst=10, logFrequency=100, logGradientToo=false, onlyUpToRow=9, onlyUpToT=25, format=[ type = "real" ; transpose = true ; precisionFormat = ".4" ])


            lastValue = TraceDenseTransposed(  GatherLast (value)  ,'dvalue')  # [i, delay]
            lastValid = TraceDenseTransposed(  GatherLast (valid)  ,'dvalid')  # [i, delay]
        ]
        # delayLine[t].value = value of t steps in the past
        # delayLine[t].valid = true if we had a value t steps in the past
        SplitStack (x) =
            if      axis == 2 then SplitDimension (x, 1, N)
            else if axis > 2  then TransposeDimensions (SplitDimension (x, 1, N), 2, axis)
            else Fail ("PastValueWindow: axis>2 required.") # BUGBUG: We also require that input is a single vector. Address later.
        value = SplitStack (RowStack (array[0..N-1](t=>delayLine[t].lastValue)))  # [i, delay]
        valid = SplitStack (RowStack (array[0..N-1](t=>delayLine[t].lastValid)))  # [i, delay]
    ]

    # fold left/right: Reduce entire sequence by applying binaryOp, e.g. FoldL (Plus, 0, input)
    # LINQ calls this Aggregate; and may or may not specify the seed value; and allows a predicate
    FoldL (binaryOp, x0, x) = _Fold (PastValue,   binaryOp, x0, x)
    FoldR (binaryOp, x0, x) = _Fold (FutureValue, binaryOp, x0, x)
    _Fold (binaryOp, x0, x) = [
        acc = binaryOp (x, if Loop.IsFirst (x) then x0 else PastValue (acc))
        out = Last (acc)
    ].out
    # TODO: need a version that does not require an initial value--what would that be called?

    # LINQ-like operators
    Map (lambda, x) = lambda (x)     // that one's easy
    # Reverse (x) is a C++ node currently called TimeReverse

    # Gather and Scatter
    # We go through 3 nodes each to take advantage of x
    Gather  (cond, x) =  GatherPacked (      PackedIndex (x, Where (cond)), x) # 'cond' matches 'x'
    Scatter (cond, y) = ScatterPacked (cond, PackedIndex (y, Where (cond)), y) # 'cond' matches the result

    # sequence-altering LINQ-like operators
    # These generate new data packing (MBLayouts)

    # First and Take
    # LINQ allows predicates as well.
    First (x) = Slice (0,  1, x,  axis=-1)
    Last (x)  = Slice (-1, 0, x,  axis=-1)

    # TakeWhile and DropWhile
    #TakeWhile (predicate, x) = Filter ( _WhilePredicate (PastValue, predicate), x)
    #SkipWhile (predicate, x) = Filter (!_WhilePredicate (PastValue, predicate), x)
    #_WhilePredicate (DelayFn, predicate, input) =
    #[
    #    whilePredicateRec = Boolean.And (DelayFn (whilePredicateRec, defaultHiddenActivation=Boolean.True), predicate)
    #].whilePredicateRec
    # TODO: do we need operations from the back?

    #Take (N, x) = _Take (PastValue, N, x)
    #TakeRight (N, x) = _Take (FutureValue, N, x)
    #_Take (DelayFn, N, x) = [
    #    selected = Loop._IsWithin (DelayFn, N, x)
    #    out = Gather (selected, x)
    #].out
    #
    #Skip (N, x) = if N > 0 then _Skip (PastValue, N, x) else x
    #_Skip (DelayFn, N, x) = [ // TODO: merge with _Take
    #    selected = Loop._IsWithin (DelayFn, N, x)
    #    out = Gather (Boolean.Not (selected), x)
    #].out
    #ElementAt (n, x) = [ // not efficient, as it filters twice. Better AND the predicates. TODO: what if n is out of range? ElementAtOrDefault
    #    startMask = Skip (n, x)                     // ...000111...
    #    mask = startMask - PastValue (0, startMask) // ...000100...
    #    out = Gather (mask, x)
    #]
    #Single (predicate, x) = x

    #FirstOrDefault (x) = ? // can empty sequences exist or even be represented by CNTK?

    #Average (x) = Sum (x) / Loop.Count(x)  // TODO: patch opQuotient to check 0/0 = 0
    #Sum (x)    = FoldL (Plus,    0, x)
    #LogSum (x) = FoldL (LogPlus, 0, x)
    #Max (x) = FoldL (^.Max, ?, x) // TODO: name clash; need to implement ^.
    #Min (x) = FoldL (^.Min, ?, x) // TODO: what's the init value?
    #All (x) = FoldL (Boolean.And,  OnesLike (x), x)
    #Any (x) = FoldL (Boolean.Or, ZeroesLike (x), x)

    # Join to create 2D fields for s2s attention?

    # Concat  (a Zip but in sequence dimension)
]

##############################################################################
# index operations
# These refer to the loop iteration itself.
##############################################################################

Loop = [
    # get the current iteration index w.r.t a node in a loop, such as a Delay node
    Iteration (x) = [
      agg = OnesLike (x) + PastValue(agg, defaultHiddenActivation=0) // a recurrence that sums up ones
    ].agg

    # get the total length of a sequence
    # TODO: in LINQ, this is an aggregation operation, so it would be long into Sequences
    Count(x) = Sequences.Last (1, Iteration (x)) // take last item of recurrence that sums up ones

    # is the current iteration the first/last of the loop?
    IsFirst (x) = _IsWithin (PastValue,   1, x)
    IsLast (x)  = _IsWithin (FutureValue, 1, x)

    IsFirstN (N, x) = _IsWithin (PastValue,   N, x)
    IsLastN  (N, x) = _IsWithin (FutureValue, N, x)

    # private helpers
    # flag whether a frame is within the first or last N frames
    _IsWithin (DelayFn/*PastValue or FutureValue*/, N, x) = DelayFn (0, Constants.ZeroesLike (x)/*false*/, timeStep=N, defaultHiddenActivation=Constants.True)

    # opposite of Id's "next"
    Previous (x) = PastValue   (0, x, timeStep=1)
    Next (x)     = FutureValue (0, x, timeStep=1)

    PreviousOrDefault (x, defaultValue=Constant (0)) =   # a delay node with initial value  --TODO: merge the two, then do in C++
    [
        flags = BS.Loop.IsFirst (x)
        out = BS.Boolean.If (flags,
                    /*then*/ BS.Sequences.Scatter (flags, defaultValue),
                    /*else*/ Previous (x))
    ].out

    NextOrDefault (x, defaultValue=Constant (0)) =   # a delay node with initial value
    [
        flags = BS.Loop.IsLast (x)
        out = BS.Boolean.If (flags,
                    /*then*/ BS.Sequences.Scatter (flags, defaultValue),
                    /*else*/ Next (x))
    ].out
]

##############################################################################
# parameter definitions
##############################################################################

Parameters =
[
    WeightParam (outputDim, inputDim) = Parameter (outputDim, inputDim, init='uniform', initValueScale=1, initOnCPUOnly=true, randomSeed=1)
    DiagWeightParam (outputDim)       = ParameterTensor ((outputDim), init='uniform', initValueScale=1, initOnCPUOnly=true, randomSeed=1) # meant to be applied elementwise
    BiasParam (dim)                   = ParameterTensor ((dim), init='fixedValue', value=0.0)
    ScalarParam()                     = BiasParam (1)

    # route input through an extra weight, for stabilization
    StabilizeElements (x, inputDim=x.dim, enabled=true) =
        if enabled
        then [
            #beta = Exp (BiasParam ((inputDim))) # init value is 0
            #beta = ParameterTensor ((inputDim), init='fixedValue', value=1.0) # init value is 1
            # or SoftPlus: ln(1+e^beta)
            #beta = Log (Constants.One + Exp (ParameterTensor ((inputDim), init='fixedValue', value=0.54132485/*ln (e-1)*/))) # init value is 1

            # sharpened Softplus: 1/f ln(1+e^{f*beta})
            # this behaves linear for weights around 1, yet guarantees positiveness

            f = ConstantTensor (4, (1))
            fInv = Reciprocal (f)
            beta = fInv .* Log (Constants.One + Exp (f .* ParameterTensor ((inputDim), init='fixedValue', value=0.99537863/* 1/f*ln (e^f-1) */))) # init value is 1

            TraceDense (h, what) = h  # delete h and uncomment Trace to trace the beta values. They are a valuable indicator.
                //Trace (h, say=what, logFirst=10, logFrequency=100, logGradientToo=false, onlyUpToRow=9, onlyUpToT=25, format=[ type = "real" ; transpose = false ; precisionFormat = ".6" ])

            result = TraceDense (    beta,    'beta') .* x
        ].result
        else x

    # and the same with a scalar stabilizer shared across all components
    Stabilize (x, enabled=true) = if enabled then StabilizeElements (x, inputDim=1, enabled=true) else x
]

##############################################################################
# recurrent networks
##############################################################################

RNNs =
[
    # LSTMP -- LSTM function with projection and self-stabilization
    # Projection is enabled by passing different values for outputDim and cellDim.
    # This is the stateless version that takes the previous state as an input.
    # It returns a dictionary with three members: h and c, and dim=h.dim for convenience. prevState must have h and c.
    # This function also takes an optional auxiliary input, e.g. for suporting attention models.
    LSTMP (outputDim, cellDim=outputDim, x, inputDim=x.dim, aux=Constants.None, auxDim=aux.dim, prevState, enableSelfStabilization=false) =
    [
        S(x) = Parameters.Stabilize (x, enabled=enableSelfStabilization)

        # TODO: rename to just _
        _privateInnards = [     // encapsulate the inner workings
            dh = prevState.h // previous values
            dc = prevState.c

            dhs = S(dh) // previous values, stabilized
            dcs = S(dc)
            # note: input does not get a stabilizer here, user is meant to do that outside

            // parameter macros
            # note: each invocation comes with its own set of weights
            B() = Parameters.BiasParam (cellDim)
            W() = Parameters.WeightParam (cellDim, inputDim)        // input
            A() = Parameters.WeightParam (cellDim, auxDim)          // aux input
            H() = Parameters.WeightParam (cellDim, outputDim)       // hidden-to-hidden
            C() = Parameters.DiagWeightParam (cellDim)              // cell-to-hiddden (note: applied elementwise)

            # projected contribution from input(s) and bias
            pin() = if Constants.IsNone (aux)
                    then B() + W() * x
                    else B() + W() * x + A() * aux

            it = Sigmoid (pin() + H() * dhs + C() .* dcs)           // input gate(t)
            bit = it .* Tanh (pin() + H() * dhs)                    // applied to tanh of input network

            ft = Sigmoid (pin() + H() * dhs + C() .* dcs)           // forget-me-not gate(t)
            bft = ft .* dc                                          // applied to cell(t-1)

            ct = bft + bit                                          // c(t) is sum of both

            ot = Sigmoid (pin() + H() * dhs + C() .* S(ct))         // output gate(t)
            ht = ot .* Tanh (ct)                                    // applied to tanh(cell(t))
        ]

        # our return values
        c = _privateInnards.ct          // cell value
        h = if outputDim != cellDim     // output/hidden state
            then [                      // project
                Wmr = Parameters.WeightParam (outputDim, cellDim);
                htp = Wmr * S(_privateInnards.ht)
            ].htp
            else _privateInnards.ht     // no projection
        dim = outputDim
    ]

    # helper function to delay h and c
    # Callers can provide their own, e.g. useful for beam decoding.
    PreviousHC (lstmState, layerIndex=0) = [
       h = Loop.Previous (lstmState.h)         // hidden state(t-1)
       c = Loop.Previous (lstmState.c)         // cell(t-1)
       dim = lstmState.dim
    ]

    # pass previousHook=BS.RNNs.NextHC instead of PreviousHC to get a right-to-left recurrence
    NextHC (lstmState, layerIndex=0) = [
       h = Loop.Next (lstmState.h)             // hidden state(t-1)
       c = Loop.Next (lstmState.c)             // cell(t-1)
       dim = lstmState.dim
    ]

    NoAuxInputHook (input, lstmState) = Constants.None

    # this implements a recurrent (stateful) LSTM with projection and self-stabilization
    # It returns a record (h,c). To use its output, say .h
    # By default, this is left-to-right. Pass previousHook=BS.RNNs.NextHC for a right-to-left model.
    RecurrentLSTMP (outputDim/*h.dim*/, cellDim=outputDim,
                    x, inputDim=x.dim,
                    previousHook=BS.RNNs.PreviousHC,
                    augmentInputHook=NoAuxInputHook, augmentInputDim=0,
                    layerIndex=0,
                    enableSelfStabilization=false) =
    [
        enableSelfStabilization1 = enableSelfStabilization ; cellDim1 = cellDim ; inputDim1 = inputDim ; layerIndex1 = layerIndex # workaround

        prevState = previousHook (lstmState, layerIndex=layerIndex1) # recurrent memory. E.g. Previous or Next, with or without initial state, beam reordering etc.

        auxInput = augmentInputHook(x, prevState)   # optionally augment input. Constants.None if none.

        lstmState = BS.RNNs.LSTMP (outputDim, cellDim=cellDim1, x, inputDim=inputDim1, aux=auxInput, auxDim=augmentInputDim, prevState, enableSelfStabilization=enableSelfStabilization1)
    ].lstmState // that's the value we return

    # a stack of recurrent LSTMs (unidirectional)
    RecurrentLSTMPStack (layerDims, cellDims=layerDims,
                         input, inputDim=input.dim,
                         previousHook=PreviousHC,
                         augmentInputHook=NoAuxInputHook, augmentInputDim=0,
                         enableSelfStabilization=false) =
    [
        previousHook1 = previousHook ; useStabilizer = enableSelfStabilization ; augmentInputHook1 = augmentInputHook ; augmentInputDim1 = augmentInputDim
        layers[i:0..Length (layerDims)-1] =
            RecurrentLSTMP (layerDims[i], cellDim=cellDims[i],
                            if i == 0 then input else Parameters.Stabilize (layers[i-1].h, enabled=useStabilizer), inputDim=if i == 0 then inputDim else layers[i-1].dim,
                            previousHook=previousHook1,
                            augmentInputHook=if i == 0 then augmentInputHook1 else NoAuxInputHook, augmentInputDim=if i == 0 then augmentInputDim1 else 0,
                            layerIndex=i,
                            enableSelfStabilization=useStabilizer)
    ].layers

    # a stack of recurrent LSTMs (bidirectional)
    # TODO: Should we define layerDims as the total (sum of both forward and backward direction)?
    RecurrentBirectionalLSTMPStack (layerDims, cellDims=layerDims, input, inputDim=input.dim, previousHook=PreviousHC, nextHook=NextHC, enableSelfStabilization=false) = [
        previousHook1 = previousHook ; nextHook1 = nextHook ; useStabilizer = enableSelfStabilization
        layers[i:0..Length (layerDims)-1] =
        [
            v    = if i == 0 then input    else Parameters.Stabilize (layers[i-1].h, enabled=useStabilizer)
            vDim = if i == 0 then inputDim else                       layers[i-1].dim
            fwd = RecurrentLSTMP (layerDims[i], cellDim=cellDims[i],
                                  v, inputDim=vDim,
                                  previousHook=previousHook1,
                                  layerIndex=i,
                                  enableSelfStabilization=useStabilizer)
            bwd = RecurrentLSTMP (layerDims[i], cellDim=cellDims[i],
                                  v, inputDim=vDim,
                                  previousHook=nextHook1,
                                  layerIndex=i,
                                  enableSelfStabilization=useStabilizer)
            h = Splice ((fwd.h : bwd.h), axis=1)
            c = Splice ((fwd.c : bwd.c), axis=1)
            dim = layerDims[i] * 2  # output dimension
        ]
    ].layers
]

##############################################################################
# sequence-to-sequence models
# This implements attention model and beam decoding.
##############################################################################

Seq2Seq =
[
    # attention model
    # The attention model is an additional input vector to the LSTM.
    # Here, it is implemented by augmenting this vector to the regular input of the LSTM.
    # The RecurrentLSTMP function does this inside through an optional lambda that the caller can pass in.
    # This function creates such a lambda, which augments the input vector from a fixed-size attention window.
    CreateAugmentWithFixedWindowAttentionHook (attentionDim, attentionSpan, decoderDynamicAxis, encoderOutput, enableSelfStabilization=false) =
    [
        # attention (fixed rolling window)
        attentionWindow = Sequences.PastValueWindow (attentionSpan, encoderOutput.h, axis=2) # BUGBUG: We should have this in axis=3 right away for beam search. Track this down.

        S(x) = Parameters.Stabilize (x, enabled=enableSelfStabilization)

        # project it for Tanh() expression
        # expected to be [attentionDim x 1 x attentionSpan], where that 1 is the axis of the beam in beam decoding
        projectedAttentionWindowBroadcast = [
            W = Parameters.WeightParam (attentionDim, encoderOutput.dim)
            # inject an additional singleton dimension at second axis, as a stand-in for the beam depth in decoding
            InjectBeamDepth (node) = SplitDimension (node, /*axis*/1, /*N:*/1)
           #projectedValue = Sequences.BroadcastSequenceAs (decoderDynamicAxis, InjectBeamDepth (W * attentionWindow.value)) # apply the projection columnwise to the attentionWindow tensor
            projectedValue = if enableSelfStabilization  # apply the projection columnwise to the attentionWindow tensor
                        then Sequences.BroadcastSequenceAs (decoderDynamicAxis, InjectBeamDepth (W * S(attentionWindow.value .* attentionWindow.valid))) # (mask invalid frames for stabilizer)
                        else Sequences.BroadcastSequenceAs (decoderDynamicAxis, InjectBeamDepth (W *   attentionWindow.value))
            value          = Sequences.BroadcastSequenceAs (decoderDynamicAxis, InjectBeamDepth (      attentionWindow.value))
            valid          = Sequences.BroadcastSequenceAs (decoderDynamicAxis, InjectBeamDepth (      attentionWindow.valid))
            dim            = encoderOutput.dim
        ]

        # the return value of this function is this lambda, which gets passed to the RecurrentLSTMP() function as the augmentInputHook parameter
        AugmentInputHook (input, prevState) =
        [
            # compute additional hidden state from attention
            outputDim = prevState.dim
            W = Parameters.WeightParam (attentionDim, outputDim)
            projectedH = W * S(prevState.h)                           # [outputDim] or [outputDim x D] in beam search
            tanHOut = Tanh (projectedAttentionWindowBroadcast.projectedValue + projectedH) # [attentionDim x beamDepth x attentionSpan]

            # You can enable (uncomment) these Trace macros to enable tracing of the attention weights, which is a useful indicator.
            TraceDense (h, what) = h
                //Trace (h, say=what, logFirst=10, logFrequency=100, logGradientToo=false, onlyUpToRow=9, onlyUpToT=25, format=[ type = "real" ; transpose = false ; precisionFormat = ".4" ])
            TraceDenseTransposed (h, what) = h
                //Trace (h, say=what, logFirst=10, logFrequency=100, logGradientToo=false, onlyUpToRow=9, onlyUpToT=25, format=[ type = "real" ; transpose = true ; precisionFormat = ".4" ])

            v = TraceDenseTransposed(    Parameters.WeightParam (1, attentionDim)     ,'v')                           # [1 x attentionDim]
            u = v * S(tanHOut .* projectedAttentionWindowBroadcast.valid) # [1 x beamDepth x attentionSpan]
            # ^^ mask 'v' for purpose of stabiliziation; TODO: don't do that if no stabiliziation
            uValid = u + Log (projectedAttentionWindowBroadcast.valid)    # [1 x beamDepth x attentionSpan]

            attentionWeights = Softmax (uValid, axis=3)                    # [1 x beamDepth x attentionSpan]
            weightedAttentionWindow = projectedAttentionWindowBroadcast.value .* TraceDense(  attentionWeights    ,'weights') # [encoderHiddenDim x beamDepth x attentionSpan]
            # TODO: use ReduceSum:
            # this is the auxiliary input to the LSTMP function
            weightedAttentionAverage = S(Times (weightedAttentionWindow, BS.Constants.OnesTensor (attentionSpan), outputRank=2)) # [encoderHiddenDim x beamDepth]
        ].weightedAttentionAverage
    ].AugmentInputHook

    # helper macro that extracts top D hypotheses from a 2D tensor
    # input: scores[w,n]    w = word index, d = hyp index in beam (d=0 is the best one)
    # output: [w,n1,n2]     n1 = input hyp index (prev top N); n2 = output hyp index (new top N)
    # e.g. 4 words, beam 3; view this as 3 [4x3] planes "drawn" 3-dimensionally, with depth being the 3rd tensor index
    GetTopNTensor (D, scores) = [
        # recurse over up to D elements
        # In each recursion:
        #  - pick the best over (w,n)
        #  - subtract it out from scores
        recursion[n:0..D-1] =
        [
            curBestScores = if n == 0                            # scores excluding paths better than rank n
                            then scores                          # top: just the path scores
                            else recursion[n - 1].nextBestScores # next: path scores after removing all we already got
            best = Hardmax (curBestScores)                       # best = one-hot over (w,n)
            nextBestScores = curBestScores + Constant (-1e30) .* best     # set the ones we've already got to -INF
            # TODO: use proper -INF; e.g. -1/0 in BS. Needs to be tested thoroughly.
        ]
        # splice them together into a single tensor
        asArray[n:0..D-1] = recursion[n].best  # this is a BS array consisting only of the 'best' field    ('from r in recursion select r.best')
        spliced = Splice (axis = 3, asArray)   # convert BS array index n to tensor index n1
    ].spliced

    # Create a greedy decoder model from an existing trained model.
    # The input model is expected to have these nodes:
    #  - decoderHistoryFromOutput: the decoding output of a time step (Hardmax (outputProbability))
    #  - decoderHistoryHook: a node that is the word sequence that will be used as the history for the next time step
    #    In training, this is the label sequence.
    #    In greedy decoding, it must be decoderHistoryHook = decoderHistoryFromOutput
    #  - z: scaled log prediction probability   --TODO: rename this: scoreSequence = Pass (z)
    #  - inputSequence
    #  - labelSequence (only passed through for scoring, not used in decoding)
    # The returned model has the following one-hot outputs:
    #  - decodedSequence  --TODO: currently decodeOut; rename this
    #  - inputSequence
    #  - labelSequence
    # To decode greedily, in "write" or "eval" specify the model as:
    #    BrainScriptNetworkBuilder = (BS.S2S.GreedySequenceDecoderFrom (BS.Network.Load ("$decodeModelPath$")))
    GreedySequenceDecoderFrom (modelAsTrained) = [
        scoreSequence = modelAsTrained.z
        decodeOut = Pass (      Hardmax (scoreSequence), tag='output')
        inputsOut = Pass (modelAsTrained.inputSequence,  tag='output')
        labelsOut = Pass (modelAsTrained.labelSequence,  tag='output')
        model = BS.Network.Edit (modelAsTrained,
                                 #BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.decoderInput/*delayedDecoderFeedback*/, delayedDecoderFeedback),
                                 BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.decoderHistoryHook, modelAsTrained.decoderHistoryFromOutput),
                                 decodeOut : inputsOut : labelsOut)
    ].model

    # turning a regular LSTM to a top-N beam-search decoder:
    #  - add a depth axis of dimension N to all nodes inside the decoder loop
    #     - only needs the init signal for PastValue to be that
    #  - h and c must be shuffled versions of their PastValue
    #     - since what are the top N in one time step is not the top N in the next
    #     - reshufling and adding depth to the init signal can be done at the same place
    #  - decoder output must determine the top N and a reshuffling matrix for h and c
    #     - the current Hardmax needs to be replaced by something that outputs these (output depth N)
    #     - we get a N^2 depth: [V x (input set) x (top N output hypos)]
    #     - reshuffling matrix is reduction over V (multiply with row of V ones) plus possibly a transposition
    #  - we need an accumulated path score
    #     - start value constructed by stacking a 0 and N-1 -INF
    #  - for testing, we can output the current best in each step
    #     - that's a Slice()
    #  - traceback is a right-to-left recurrence
    #     - output best hypo conditioned on the path (it is already known)
    # beam search of width 'beamDepth'
    BeamSearchSequenceDecoderFrom (modelAsTrained, beamDepth) = [

        scoreSequence = modelAsTrained.z
        vocabSize    = scoreSequence.dim

        # TODO: use ReduceSum
        ReduceAxis (axisDim, x, axis=1) =   # unfortunately, we must feed in the dimension of the axis, it can't be inferred
            if      axis == 1 then Times (Constants.OnesTensor (axisDim), x, outputRank=0)
            else if axis == 2 then ReduceAxis (axisDim, TransposeDimensions (x, 1, 2), axis=1)
            else Fail("ReduceAxis: Only supports axes 1 and 2.")

        # === BEGIN DECODER ===

        # constants for initial score and final traceback
        initialPathScores = FirstAndOther (0, LOGZERO, beamDepth, axis = 2)  # [1 x D]: [ 0, -INF, -INF, -INF, ... ]
        finalHyp          = FirstAndOther (1, 0,       beamDepth, axis = 1)  # [D] the final token is the top-scoring hypothesis, that is, hyp[0]

        # path expansion of the D hypotheses that were best in previous time step (ordered as in previous time step)
        logLLs = Columnwise (LogSoftmax, beamDepth, scoreSequence)                                      # [V x Dprev] log  P(w|hist)
        expandedPathScores = logLLs + Boolean.If (Loop.IsFirst (logLLs), initialPathScores, Loop.Previous (tokens.score)) # [V x Dprev] log (P(w|hist) * P(hist)) for all top D hypotheses

        # determine top D of expanded paths
        topPaths      = GetTopNTensor (beamDepth, expandedPathScores) # [V x Dprev] -> [V x Dprev x Dnew]
        topPathScores = topPaths .* expandedPathScores                #                [V x Dprev x Dnew]

        # form new decoding token, by reducing topPaths(Scores) along relevant dimensions
        tokens = [                                    # [. x Dnew]
            from  = ReduceAxis (axis=1, vocabSize, topPaths) # [Dprev x Dnew], reduced over V
            word  = ReduceAxis (axis=2, beamDepth, topPaths) # [V x Dnew], reduced over Dprev
            score = Constants.OnesTensor (1/*output dim*/ : /*reduction dims: */vocabSize : beamDepth/*Dprev*/) * topPathScores # [1 x Dnew], reduced over [V x Dprev] and inserted a '1'
        ]

        # network feedback for next time step
        # BUGBUG: Need to import EmbedLabels functionality from models
        decoderFeedback = /*EmbedLabels*/ (tokens.word) # [embeddingDim x Dnew]
        delayedDecoderFeedback = Boolean.If (Loop.IsFirst (labelSentenceStartEmbeddedScattered), labelSentenceStartEmbeddedScattered, Loop.Previous (decoderFeedback))

        # final traceback
        traceback = Boolean.If (Loop.IsLast (modelAsTrained.labelSentenceStartEmbeddedScattered/*tokens.from*/), finalHyp, Loop.Next (tokens.from * traceback)) # [D] one-hot, multiplying tokens.from from the left will select another one-hot row of tokens.from
        decodeHyp = Times (topPaths, traceback, outputRank=2) # [V x Dprev] 2D one-hot, selected the best hyp according to traceback
        decode = decodeHyp * Constants.OnesTensor (beamDepth) # [V] reduces over Dprev -> 1D one-hot
        # TODO: Can this be done in one ^^ go?

        # === END DECODER ===

        # propagate LSTM state to the right top-N rank given where that rank came from in the previous time step

        # PropagateTopN:
        # tokens.from: [Dprev, Dnew]
        #   v--------- best came from input hyp[1]
        #     v------- second best came from input hyp[0]
        #       v----- third best came from input hyp[2]
        #   0 1 0
        #   1 0 0
        #   0 0 1
        # tokens.from[:,n] one-hot encodes the best predecessor at top-N rank n
        # each column is a one-hot vector
        # multiplying with such a column from the right will select the column represented by the one-hot value

        # logLLs: get decoder log likelihoods

        # initialPathScores: decoder start token: 0 for first hyp, -INF for the others
        LOGZERO = -1e30

        # expandedPathScores: path expansion, [V x 1] + [1 x D] -> [V x D]

        # topPaths:
        #   +-----+
        #   |0 0 0|
        #   |0 0 0|-+
        #   |0 1 0|0|     means word[2] in input hyp[1] was the best
        #   |0 0 0|0|-+
        #   +-----+0|0|
        #     |1 0 0|0|   means word[3] in input hyp[0] was the second best
        #     +-----+1|   means word[2] in input hyp[2] was the third best
        #       |0 0 0|
        #       +-----+

        # tokens.word:
        #tokens.word = ReduceSum (axis=2, topPaths) # TODO: add an axis parameter to SumColumnElements()
        #   +-+
        #   |0|
        #   |0|-+
        #   |1|0|     means word[2] in input hyp[1] was the best
        #   |0|0|-+
        #   +-+0|0|
        #     |1|0|   means word[3] in input hyp[0] was the second best
        #     +-+1|   means word[2] in input hyp[2] was the third best
        #       |0|
        #       +-+

        # tokens.from:
        # before dropping the first dimension: [V x Dprev x Dnew]
        #   +-----+
        #   |0 1 0|       means input hyp[1] gave rise to the best    
        #   +-----+-+  
        #     |1 0 0|     means input hyp[0] gave rise to second best
        #     +-----+-+
        #       |0 0 1|   means input hyp[2] gave rise to third best
        #       +-----+
        # after: [Dprev x Dnew]        e.g. "0 1 0" goes into first column, vertically
        #   v--------- best came from input hyp[1]
        #     v------- second best came from input hyp[0]
        #       v----- third best came from input hyp[2]
        #   0 1 0
        #   1 0 0
        #   0 0 1
        # tokens.from[:,n] one-hot encodes the best predecessor at top-N rank n

        # topPathScores:
        #   +-----+
        #   |0 0 0|
        #   |0 0 0|-+
        #   |0 x 0|0|     x denotes the accumulated path score max_w P(w|hyp[1])
        #   |0 0 0|0|-+
        #   +-----+0|0|
        #     |y 0 0|0|   y denotes the accumulated path score max_w P(w|hyp[0])
        #     +-----+z|   z denotes the accumulated path score max_w P(w|hyp[2])
        #       |0 0 0|
        #       +-----+

        # traceback:
        # last state: take Hardmax over tokens.score
        # previous states: multiply wth respective tokens.from matrix
        # -> hyp index for every time step
        # then finally use that to select the actual output   TODO: That's a sample-wise matrix product between two sequences!!!
        # TODO: condition must be 1-dim, not 2-dim tensor, so we use labelSentenceStartEmbeddedScattered instead of tokens.from
        # +-+
        # |0|
        # |1|  means at this time step, hyp[1] was the best globally
        # |0|
        # +-+

        # decode: and the actual decoding output
        # This is the one to output (top sentence-level hypothesis after traceback).

        # traceback : [Dnew]
        # topPaths : [V x Dprev x Dnew]
        #   +-----+
        #   |0 0 0|
        #   |0 0 0|-+
        #   |0 1 0|0|     means word[2] in input hyp[1] was the best
        #   |0 0 0|0|-+
        #   +-----+0|0|
        #     |1 0 0|0|   means word[3] in input hyp[0] was the second best
        #     +-----+1|   means word[2] in input hyp[2] was the third best
        #       |0 0 0|
        #       +-----+

        # helper macros  --> move to BS.core.bs

        Columnwise (f, beamDepth, z) = # TODO: Takes LogSoftmax over axis=1. it is more tricky to do this over arbitrary axes
        [
            cols[d:0..beamDepth-1] = f (Slice (d, d+1, z, axis=2) /*[:,d]*/ )
            out = Splice (cols, axis=2)
        ].out

        FirstAndOther (firstVal, otherVals, N, axis = 1) = if N == 1 then ConstantTensor (firstVal, (1)) else [
            axis1 = axis  # TODO: Is this really necessary? Why? Then we need the syntax   axis = ^.axis or ^axis
            out = if axis == 1  # maybe this can be unified or pushed into Splice?
                  then RowStack (ConstantTensor (firstVal, (1)) : ConstantTensor (otherVals, (N -1)))                                # col vector: [ 1; 0; 0; 0 ... ]
                  else Splice   (Constant       (firstVal)      : ConstantTensor (otherVals, (1 : N -1)), axis = axis1 /*, axis*/)   # row vector: [ 0, -INF, -INF, -INF, ... ]
        ].out

        model = BS.Network.Edit (modelAsTrained,
                                 (
                                     BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.beamSearchReorderHook, tokens.from) :   # reorder LSTM states
                                     BS.Network.Editing.ReplaceLinksToNode (modelAsTrained.decoderHistoryHook,    decoderFeedback) # feed decoder output back in
                                 ),
                                 (inputsOut : labelsOut : decodeOut)) # additional roots

        inputsOut = Pass (modelAsTrained.inputSequence, tag='output')
        labelsOut = Pass (modelAsTrained.labelSequence, tag='output')
        decodeOut = Pass (decode, tag='output')
    ].model
]

##############################################################################
# Network-level operations
# These operations will have undefined behavior for input values != 0 or 1.
##############################################################################

Network = [
    Load(pathName) = new ComputationNetworkFromFile [ /*pathName; also needs 'precision' somewhere*/ ]
    Edit(inputModel, editFunctions, additionalRoots) = new ComputationNetworkWithEdits [ /*inputModel, editFunctions, additionalRoots*/ ]

    Editing = [
        // Create a lambda that returns its argument unless that argument == 'old', then it will return 'replacement'
        ReplaceLinksToNode (old, replacement) = (node => if IsSameObject (node, old) then replacement else node)
        ReplaceLinksToNamedNode (name, replacement) = (node => if node.name == name then replacement else node)
    ]
]

] # end of BS namespace

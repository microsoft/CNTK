//
// Copyright (c) Microsoft. All rights reserved.
// Licensed under the MIT license. See LICENSE.md file in the project root for full license information.
//
// CNTK.core.bs -- core BrainScript library including both general and CNTK-specific definitions
//

##############################################################################
# standard functions
##############################################################################

Print(value, format='') = new PrintAction [ what = value /*; how = format*/ ] 
Fail(what) = new FailAction [ /*what*/ ]
Format(value, format) = new StringFunction [ what = 'Format' ; arg = value ; how = format ] 
Replace(s, from, to) = new StringFunction [ what = 'Replace' ; arg = s ; replacewhat = from ; withwhat = to ] 
Substr(s, begin, num) = new StringFunction [ what = 'Substr' ; arg = s ; pos = begin ; chars = num ] 
Chr(c) = new StringFunction [ what = 'Chr' ;  arg = c ] 
Floor(x)  = new NumericFunction [ what = 'Floor' ;  arg = x ] 
Length(x) = new NumericFunction [ what = 'Length' ; arg = x ] 
Ceil(x) = -Floor(-x) 
Round(x) = Floor(x+0.5)
Sign(x) = if x > 0 then 1 else if x < 0 then -1 else 0 
Min(a,b) = if a < b then a else b 
Max(a,b) = if a > b then a else b 
Fac(n) = if n > 1 then Fac(n-1) * n else 1 
IsSameObject(a,b) = new CompareFunction [ what = 'IsSameObject' ; args = (a : b) ]

##############################################################################
# ComputationNodes
##############################################################################

LearnableParameter (outputDim, inputDim, learningRateMultiplier = 1.0, init = 'uniform'/*|fixedValue|gaussian|fromFile*/, initValueScale = 1, value = 0, initFromFilePath = '', initFromLiteral = '', initOnCPUOnly=true, randomSeed=-1, tag='') = new ComputationNode [ operation = 'LearnableParameter' ; shape = new TensorShape [ dims = (outputDim : inputDim) ] /*plus the function args*/ ]
Parameter = LearnableParameter // deprecated 
# TODO: make Parameter take tensor dims?
ParameterTensor(dims, learningRateMultiplier = 1.0, init = 'uniform'/*|fixedValue|gaussian|fromFile*/, initValueScale = 1, value = 0, initFromFilePath = '', initFromLiteral = '', initOnCPUOnly=true, randomSeed=-1, tag='') = new ComputationNode [ operation = 'LearnableParameter' ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
ConstantFromString(literal, tag='') = ParameterTensor((0)/*dim, will be inferred*/, init = 'fromLiteral', initFromLiteral = literal, learningRateMultiplier = 0.0)
DynamicAxis(tag='') = new ComputationNode [ operation = 'DynamicAxis' ; /*plus the function args*/  ]
Input(dims, dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'InputValue' ; shape = new TensorShape [ /*dims*/ ] ; isImage = false /*plus the function args*/ ]
# TODO: change from dynamicAxis by name to dynamicAxis being an actual object
SparseInput(dims, dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'SparseInputValue' ; shape = new TensorShape [ /*dims*/ ] ; isImage = false /*plus the function args*/ ]
ImageInput(imageWidth, imageHeight, imageChannels, imageLayout='CHW', dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'InputValue' ; isImage = true /*plus the function args*/ ]
SparseImageInput(imageWidth, imageHeight, imageChannels, imageLayout='CHW', dynamicAxis='', tag='feature') = new ComputationNode [ operation = 'SparseInputValue' ; isImage = true /*plus the function args*/ ]
EnvironmentInput(propertyName, tag='') = new ComputationNode [ operation = 'EnvironmentInput' /*plus the function args*/ ]
# TODO: make 'dims' the first parameter, think ConstantTensor<dims> (val)
ConstantTensor(val, dims, tag='') = ParameterTensor(dims, learningRateMultiplier = 0, init = 'fixedValue', value = val) 
Constant(val, rows = 1, cols = 1, tag='') = Parameter(rows, cols, learningRateMultiplier = 0, init = 'fixedValue', value = val) 
PastValue(dims, input, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'PastValue' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
FutureValue(dims, input, timeStep = 1, defaultHiddenActivation = 0.1, tag='') = new ComputationNode [ operation = 'FutureValue' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
Shift(input, fromOffset, boundaryValue, boundaryMode=-1/*context*/, dim=-1, tag='') = new ComputationNode [ operation = 'Shift' ; inputs = (input : boundaryValue) /*plus the function args*/ ]
RowSlice(beginIndex, numRows, input, tag='') = Slice(beginIndex, beginIndex + numRows, input, axis = 1)
RowRepeat(input, numRepeats, tag='') = new ComputationNode [ operation = 'RowRepeat' ; inputs = input /*plus the function args*/ ]
RowStack(inputs, tag='') = new ComputationNode [ operation = 'RowStack' /*plus the function args*/ ]
Splice (inputs, axis=1) = # TODO: This is a workaround. RowStack itself shall interpret 'axis' and be renamed to Splice().
    if axis < 1 then Fail('Splice does not yet implement splicing the time axis.')
    else if axis == 1 then RowStack (inputs)
    else [ # workaround: swap 'axis' to first position, RowStack, swap back
        ArrayTransposeDimensions (inputs, axis1, axis2) = [ # transpose each element of a BS array
            inputsT[i:0..Length(inputs)-1] = TransposeDimensions (inputs[i], axis1, axis2)
        ].inputsT
        out = TransposeDimensions (RowStack (ArrayTransposeDimensions (inputs, 1, axis)), 1, axis)
    ].out
Reshape(input, numRows, imageWidth = 0, imageHeight = 0, imageChannels = 0, tag='') = new ComputationNode [ operation = 'LegacyReshape' ; inputs = input /*plus the function args*/ ]
NewReshape(input, dims, beginAxis=0, endAxis=0, tag='') = new ComputationNode [ operation = 'Reshape' ; inputs = input ; shape = new TensorShape [ /*dims*/ ] /*plus the function args*/ ]
ReshapeDimension(x, axis, tensorShape) = NewReshape(x, tensorShape, beginAxis=axis, endAxis=axis + 1) 
FlattenDimensions(x, axis, num) = NewReshape(x, 0, beginAxis=axis, endAxis=axis + num) 
Slice(beginIndex, endIndex, input, axis=1, tag='') =
    if axis < 0 then [ # time axis: specify -1
        beginFlags = if beginIndex > 0 then BS.Boolean.Not (BS.Loop.IsFirstN (beginIndex, input)) else                 BS.Loop.IsLastN  (-beginIndex, input)
        endFlags   = if endIndex   > 0 then                 BS.Loop.IsFirstN (endIndex,   input)  else BS.Boolean.Not (BS.Loop.IsLastN  (-endIndex,   input))
        flags = if      beginIndex == 0 then endFlags
                else if endIndex   == 0 then beginFlags
                else                         BS.Boolean.And (beginFlags, endFlags)
        out = if beginIndex == 0 && endIndex == 0
              then input
              else BS.Sequences.Gather (flags, input)
    ].out
    else new ComputationNode [ operation = 'Slice' ; inputs = input /*plus the function args*/ ] # non-time axis
SplitDimension(x, axis, N) = ReshapeDimension(x, axis, 0:N) 
TransposeDimensions(input, axis1, axis2, tag='') = new ComputationNode [ operation = 'TransposeDimensions' ; inputs = input /*plus the function args*/ ]
# TODO: make input the last arg!
Transpose(x) = TransposeDimensions(x, 1, 2)
Times(A, B, outputRank=1, tag='') = new ComputationNode [ operation = 'Times' ; inputs = ( A : B ) /*plus the function args*/ ]
Logistic(label, probability, tag='') = new ComputationNode [ operation = 'Logistic' ; inputs = (label : probability) /*plus the function args*/ ]
WeightedLogistic(label, probability, instanceWeight, tag='') = new ComputationNode [ operation = 'Logistic' ; inputs = (label : probability : instanceWeight) /*plus the function args*/ ]
ReconcileDynamicAxis(dataInput, layoutInput, tag='') = new ComputationNode [ operation = 'ReconcileDynamicAxis' ; inputs = (dataInput : layoutInput) /*plus the function args*/ ]
ReconcileMBLayout = ReconcileDynamicAxis # back compat
CastAs (type, data) = ReconcileDynamicAxis (data, type) # read as CastAs<type>(data) where the cast may consist of rearranging the data w.r.t. MBLayout or broadcasting across sequence items
Convolution(weightNode, inputValueNode, kernelDims, mapDims = 1, stride = 1, sharing = true, autoPadding = true, lowerPad = 0, upperPad = 0, imageLayout='CHW', maxTempMemSizeInSamples = 0, tag='') = new ComputationNode [ operation = 'Convolution' ; inputs = (weightNode : inputValueNode); kernelShape = new TensorShape [ dims = kernelDims ] ; mapCount = new TensorShape [ dims = mapDims ] ; strideShape = new TensorShape [ dims = stride ] ; dimSharing = new BoolVector [ items = sharing ] ; dimPadding = new BoolVector [ items = autoPadding ] ; dimPadLower = new TensorShape [ dims = lowerPad ] ; dimPadUpper = new TensorShape [ dims = upperPad ] /*plus the function args*/ ]
Pooling(input, poolKind/*'max'|'average'*/, kernelDims, stride=1, autoPadding = true, lowerPad = 0, upperPad = 0, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'Pooling' ; inputs = (input); pool = poolKind ; kernelShape = new TensorShape [ dims = kernelDims ] ; strideShape = new TensorShape [ dims = stride ] ; dimPadding = new BoolVector [ items = autoPadding ] ; dimPadLower = new TensorShape [ dims = lowerPad ] ; dimPadUpper = new TensorShape [ dims = upperPad ] /*plus the function args*/ ]
MaxPooling(input, windowWidth, windowHeight, horizontalSubsample, verticalSubsample, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'MaxPooling' ; inputs = input /*plus the function args*/ ]
AveragePooling(input, windowWidth, windowHeight, horizontalSubsample, verticalSubsample, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'AveragePooling' ; inputs = input /*plus the function args*/ ]
ColumnwiseCrossProduct = KhatriRaoProduct // deprecated 
ClassificationError = ErrorPrediction 
Delay = PastValue 

BatchNormalization(input, scale, bias, runMean, runInvStdDev, spatial, normalizationTimeConstant = 0, blendTimeConstant = 0, epsilon = 0.00001, useCntkEngine = true, imageLayout='CHW', tag='') = new ComputationNode [ operation = 'BatchNormalization' ; inputs = (input : scale : bias : runMean : runInvStdDev) /*plus the function args*/ ]
Abs(x, tag='') = new ComputationNode [ operation = 'Abs' ; inputs = x /*plus the function args*/ ]
ClassBasedCrossEntropyWithSoftmax(labelClassDescriptorVectorSequence, mainInputInfo, mainWeight, classLogProbsBeforeSoftmax, tag='') = new ComputationNode [ operation = 'ClassBasedCrossEntropyWithSoftmax' ; inputs = (labelClassDescriptorVectorSequence : mainInputInfo : mainWeight : classLogProbsBeforeSoftmax) /*plus the function args*/ ]
ColumnElementTimes(aVectorSequence, anotherVectorSequence, tag='') = new ComputationNode [ operation = 'ColumnElementTimes' ; inputs = (aVectorSequence : anotherVectorSequence) /*plus the function args*/ ]
// TODO: ColumnElementTimes = ElementTimes
CosDistance(aVectorSequence, anotherVectorSequence, tag='') = new ComputationNode [ operation = 'CosDistance' ; inputs = (aVectorSequence : anotherVectorSequence) /*plus the function args*/ ]
CosDistanceWithNegativeSamples(aVectorSequence, anotherVectorSequence, numShifts, numNegSamples, tag='') = new ComputationNode [ operation = 'CosDistanceWithNegativeSamples' ; inputs = (aVectorSequence : anotherVectorSequence : numShifts : numNegSamples) /*plus the function args*/ ]
Cosine(x, tag='') = new ComputationNode [ operation = 'Cosine' ; inputs = x /*plus the function args*/ ]
CrossEntropy(refProbVectorSequence, outProbVectorSequence, tag='') = new ComputationNode [ operation = 'CrossEntropy' ; inputs = (refProbVectorSequence : outProbVectorSequence) /*plus the function args*/ ]
CrossEntropyWithSoftmax(labelVectorSequence, outProbVectorSequence, tag='') = new ComputationNode [ operation = 'CrossEntropyWithSoftmax' ; inputs = (labelVectorSequence : outProbVectorSequence) /*plus the function args*/ ]
DiagTimes(diagonalMatrixAsColumnVector, matrix, tag='') = new ComputationNode [ operation = 'DiagTimes' ; inputs = (diagonalMatrixAsColumnVector : matrix) /*plus the function args*/ ]
// TODO: DiagTimes = ElementTimes
Dropout(activationVectorSequence, tag='') = new ComputationNode [ operation = 'Dropout' ; inputs = activationVectorSequence /*plus the function args*/ ]
ElementTimes(aMatrix, anotherMatrix, tag='') = new ComputationNode [ operation = 'ElementTimes' ; inputs = (aMatrix : anotherMatrix) /*plus the function args*/ ]
ElementDivide(aMatrix, anotherMatrix, tag='') = ElementTimes(aMatrix, Reciprocal(anotherMatrix), tag=tag)
ErrorPrediction(labelVectorSequence, outVectorSequence, tag='') = new ComputationNode [ operation = 'ErrorPrediction' ; inputs = (labelVectorSequence : outVectorSequence) /*plus the function args*/ ]
Exp(x, tag='') = new ComputationNode [ operation = 'Exp' ; inputs = x /*plus the function args*/ ]
GatherPacked(indexSequence, sourceData, tag='') = new ComputationNode [ operation = 'GatherPacked' ; inputs = (indexSequence : sourceData) /*plus the function args*/ ]
GMMLogLikelihood(unnormalizedPriorVector, meansAsRows, logStdDevAsRows, dataVectorSequence, tag='') = new ComputationNode [ operation = 'GMMLogLikelihood' ; inputs = (unnormalizedPriorVector : meansAsRows : logStdDevAsRows : dataVectorSequence) /*plus the function args*/ ]
InvStdDev(dataVectorSequence, tag='') = new ComputationNode [ operation = 'InvStdDev' ; inputs = dataVectorSequence /*plus the function args*/ ]
KhatriRaoProduct(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'KhatriRaoProduct' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Log(x, tag='') = new ComputationNode [ operation = 'Log' ; inputs = x /*plus the function args*/ ]
LogPlus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'LogPlus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
LogSoftmax(z, tag='') = new ComputationNode [ operation = 'LogSoftmax' ; inputs = z /*plus the function args*/ ]
MatrixL1Reg(matrix, tag='') = new ComputationNode [ operation = 'MatrixL1Reg' ; inputs = matrix /*plus the function args*/ ]
MatrixL2Reg(matrix, tag='') = new ComputationNode [ operation = 'MatrixL2Reg' ; inputs = matrix /*plus the function args*/ ]
Mean(dataVectorSequence, tag='') = new ComputationNode [ operation = 'Mean' ; inputs = dataVectorSequence /*plus the function args*/ ]
Minus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'Minus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Negate(input, tag='') = new ComputationNode [ operation = 'Negate' ; inputs = input /*plus the function args*/ ]
PackedIndex(targetObject, indexSequence, tag='') = new ComputationNode [ operation = 'PackedIndex' ; inputs = (targetObject : indexSequence) /*plus the function args*/ ]
Pass(x, tag='') = new ComputationNode [ operation = 'Pass' ; inputs = x /*plus the function args*/ ]
PerDimMeanVarDeNormalization(dataVectorSequence, meanVector, invStdDevVector, tag='') = new ComputationNode [ operation = 'PerDimMeanVarDeNormalization' ; inputs = (dataVectorSequence : meanVector : invStdDevVector) /*plus the function args*/ ]
PerDimMeanVarNormalization(dataVectorSequence, meanVector, invStdDevVector, tag='') = new ComputationNode [ operation = 'PerDimMeanVarNormalization' ; inputs = (dataVectorSequence : meanVector : invStdDevVector) /*plus the function args*/ ]
Plus(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'Plus' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Reciprocal(z, tag='') = new ComputationNode [ operation = 'Reciprocal' ; inputs = z /*plus the function args*/ ]
RectifiedLinear(z, tag='') = new ComputationNode [ operation = 'RectifiedLinear' ; inputs = z /*plus the function args*/ ]
ReducePlus (z, axis=0, tag='')     = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Plus"    /*plus the function args*/ ]
#ReduceLogPlus (z, axis=0, tag='') = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "LogPlus" /*plus the function args*/ ]
#ReduceMean (z, axis=0, tag='')    = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Mean"    /*plus the function args*/ ]
#ReduceMax (z, axis=0, tag='')     = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Max"     /*plus the function args*/ ]
#ReduceMin (z, axis=0, tag='')     = new ComputationNode [ operation = 'ReduceElements' ; inputs = z ; reductionOp = "Min"     /*plus the function args*/ ]
Scale(scalarScalingFactor, matrix, tag='') = new ComputationNode [ operation = 'Scale' ; inputs = (scalarScalingFactor : matrix) /*plus the function args*/ ]
// TODO: Scale = ElementTimes
ScatterPacked(cond, indexSequence, sourceData, tag='') = new ComputationNode [ operation = 'ScatterPacked' ; inputs = (cond : indexSequence : sourceData) /*plus the function args*/ ]
Sigmoid(z, tag='') = new ComputationNode [ operation = 'Sigmoid' ; inputs = z /*plus the function args*/ ]
Sin(z, tag='') = new ComputationNode [ operation = 'Sin' ; inputs = z /*plus the function args*/ ]
Softmax(z, tag='') = new ComputationNode [ operation = 'Softmax' ; inputs = z /*plus the function args*/ ]
Hardmax(z, tag='') = new ComputationNode [ operation = 'Hardmax' ; inputs = z /*plus the function args*/ ]
Sqrt(z, tag='') = new ComputationNode [ operation = 'Sqrt' ; inputs = z /*plus the function args*/ ]
SquareError(aMatrix, anotherMatrix, tag='') = new ComputationNode [ operation = 'SquareError' ; inputs = (aMatrix : anotherMatrix) /*plus the function args*/ ]
SumColumnElements(z, tag='') = new ComputationNode [ operation = 'SumColumnElements' ; inputs = z /*plus the function args*/ ] // deprecated
SumElements(matrix, tag='') = new ComputationNode [ operation = 'SumElements' ; inputs = matrix /*plus the function args*/ ]
# ^^ TODO: Rename to ReduceSumMB?
Tanh(z, tag='') = new ComputationNode [ operation = 'Tanh' ; inputs = z /*plus the function args*/ ]
TimeReverse(vectorSequence, tag='') = new ComputationNode [ operation = 'TimeReverse' ; inputs = vectorSequence /*plus the function args*/ ]
TransposeTimes(leftMatrix, rightMatrix, tag='') = new ComputationNode [ operation = 'TransposeTimes' ; inputs = (leftMatrix : rightMatrix) /*plus the function args*/ ]
Where(cond, tag='') = new ComputationNode [ operation = 'Where' ; inputs = cond /*plus the function args*/ ]

##############################################################################
# common macros
##############################################################################

BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols) ; z = W*in+B ] 
SBFF(in, rows, cols) = [ Eh = Sigmoid(BFF(in, rows, cols).z) ] 
MeanVarNorm(feat) = PerDimMeanVarNormalization(feat, Mean(feat), InvStdDev(feat)) 
LogPrior(labels) = Log(Mean(labels)) 

Embedding (embeddingDim, input, inputDim=input.dim, initFrom='fromFile'/*|gaussian|uniform*/, embeddingPath = '', sparseInput = false, learningRateWeight = 0.0) = [
    embedding = Transpose (LearnableParameter (inputDim, embeddingDim, learningRateMultiplier = learningRateWeight, init = initFrom, initFromFilePath = embeddingPath))
    lookup = if sparseInput then embedding * input
             else GatherPacked (input, embedding)
].lookup

##############################################################################
# the more specific standard things are in a namespace called 'BS'
# You can create shorthands for accessing these, e.g. saying B = BS.Boolean.
# Note: Identifiers beginning with _ should be considered for library use only.
##############################################################################

BS = [

##############################################################################
# Basic constants
##############################################################################

Constants = [
    Zero = ConstantTensor (0, (1))
    One  = ConstantTensor (1, (1))
    OnesTensor (dims) = ConstantTensor (1, dims)
    # BUGBUG: ZeroesLike() would recreate the full dimension of x. Well, no need if it considers broadcasting. But still wrong if we want to broadcast a vector of different tensor dim.
    #ZeroesLike (x) = CastAs (x, Zero) // read: Cast<x>(Zero)
    #OnesLike (x)   = CastAs (x, One)
    # CastAs() does not implement broadcasting
    ZeroesLike (x) = SumColumnElements (RowSlice (0, 1, x) .* Zero)  // hack: get one row of input and multiply with zero; double-hack: reduce extra tensor dims by SumCol
    ZeroSequenceLike = ZeroesLike   # TODO: this should yield a scalar sequence, while ZeroesLike should be a tensor
    ZeroesLike1 (x) = x .* Zero     # get a tensor of zeroes of same dim as x  TODO: Do this as a C++ node (will be simple)
    OnesLike (x) = ZeroesLike (x) + One
    # is this like Sequences.Repeat?
    True  = 1
    False = 0
]

##############################################################################
# Boolean operations
# These operations will have undefined behavior for input values != 0 or 1.
##############################################################################

# boolean helpers
Boolean = [
    True  = 1
    False = 0

    # basic logical operations
    And (a,b) =         a .* b
    Or  (a,b) = a + b - a .* b
    Xor (a,b) = a + b - a .* b * Constant (2)
    Not (x)   = Constants.One - x

    # on each time step where clk 1, this toggles its value
    Toggle (clk, initialValue=False) = [
        state = Xor (PastValue (1, state, defaultHiddenActivation=initialValue), clk)
    ].state

    # select a value
    # Note: This will be replaced by BrainScript 'if cond then thenVal else elseVal' and SwitchNode
    If (cond, thenVal, elseVal) = cond .* thenVal + Not (cond) .* elseVal
]

##############################################################################
# sequence operations
# These mimic LINQ operations.
##############################################################################

Sequences = [
    # broadcast a single-step sequence to a multi-step sequence
    BroadcastSequenceAs (type, data1) = [                      # type=example sequence with desired length (outside of a loop), data1=1 time step
        # BUGBUG: This should work but gives worse results.
        #ZeroSequenceLike (x) = RowSlice (0, 1, x) .* Constants.Zero # BUGBUG: SumColumnElements() has a CPU/GPU problem
        #index = /*Constants.*/ZeroSequenceLike (type)  # create an index sequence [ 0 0 0 ... ] of target length
        #packedIndex = PackedIndex (data1, index)       # convert into internal packed index w.r.t. 'data1'
        #out = GatherPacked (packedIndex, data1)        # copy data1[0] to all elements, total length like 'type'

        # alternative (slower, older) implementation (10% slower end-to-end?)
        # Gives nearly the same result, but not completely. Since Gather() above has an atomicAdd(), let's leave this on for now and check later.
        dataPadded = Sequences.Scatter (Loop.IsFirst (type), data1) # padded with zeroes until end of target sequence
        out = Boolean.If (Loop.IsFirst (dataPadded), # if first entry
                 /*then*/ dataPadded,                # then copy that
                 /*else*/ Loop.Previous (out))       # else just propagate to the front
    ].out

    # rolling window over past N samples
    # returns a record [ value=..., valid=... ], both being 1-step sequences of [dim x N]. N can optionally be moved to axes >2.
    # This implementation is suboptimal in that it creates copies for the intermediate steps.
    PastValueWindow (N, in, axis=2) = [
        delayLine[t:0..N-1] = [     # shift register for encoder, last N inputs
            value = if t == 0
                    then in        # delay 0: current value
                    else Loop.PastValue (0, in, timeStep=t)
            valid = if t == 0
                    then Constants.One
                    else Constants.One - PastValue (1, Constants.ZeroesLike (in), timeStep=t, defaultHiddenActivation=1)
        ]
        # delayLine[t].value = value of t steps in the past
        # delayLine[t].valid = true if we had a value t steps in the past
        SplitStack (x) =
            if      axis == 2 then SplitDimension (x, 1, N)
            else if axis > 2  then TransposeDimensions (SplitDimension (x, 1, N), 2, axis)
            else Fail ("PastValueWindow: axis>2 required.") # BUGBUG: We also require that input is a single vector. Address later.
        value = Slice (-1, 0, axis=-1, SplitStack (RowStack (array[0..N-1](t=>delayLine[t].value))))  # [i, delay]
        valid = Slice (-1, 0, axis=-1, SplitStack (RowStack (array[0..N-1](t=>delayLine[t].valid))))  # [i, delay]
    ]

    # fold left/right: Reduce entire sequence by applying binaryOp, e.g. FoldL (Plus, 0, input)
    # LINQ calls this Aggregate; and may or may not specify the seed value; and allows a predicate
    FoldL (binaryOp, x0, x) = _Fold (PastValue,   binaryOp, x0, x)
    FoldR (binaryOp, x0, x) = _Fold (FutureValue, binaryOp, x0, x)
    _Fold (binaryOp, x0, x) = [
        acc = binaryOp (x, if Loop.IsFirst (x) then x0 else PastValue (acc))
        out = Last (acc)
    ].out
    # TODO: need a version that does not require an initial value--what would that be called?

    # LINQ-like operators
    Map (lambda, x) = lambda (x)     // that one's easy
    # Reverse (x) is a C++ node currently called TimeReverse

    # Gather and Scatter
    # We go through 3 nodes each to take advantage of x
    Gather  (cond, x) =  GatherPacked (      PackedIndex (x, Where (cond)), x) # 'cond' matches 'x'
    Scatter (cond, y) = ScatterPacked (cond, PackedIndex (y, Where (cond)), y) # 'cond' matches the result

    # sequence-altering LINQ-like operators
    # These generate new data packing (MBLayouts)

    # TakeWhile and DropWhile
    TakeWhile (predicate, x) = Filter ( _WhilePredicate (PastValue, predicate), x)
    SkipWhile (predicate, x) = Filter (!_WhilePredicate (PastValue, predicate), x)
    _WhilePredicate (DelayFn, predicate, input) =
    [
        whilePredicateRec = Boolean.And (DelayFn (whilePredicateRec, defaultHiddenActivation=Boolean.True), predicate)
    ].whilePredicateRec
    # TODO: do we need operations from the back?

    # First and Take
    # LINQ allows predicates as well.
    First (x)   =  Take (1, x)
    Take (N, x) = _Take (PastValue, N, x)
    _Take (DelayFn, N, x) = [
        selected = Loop._IsWithin (DelayFn, N, x)
        out = Gather (selected, x)
    ].out
    # Last and TakeRight
    Last (x) = TakeRight (1, x)
    TakeRight (N, x) = _Take (FutureValue, N, x)
    Skip (N, x) = if N > 0 then _Skip (PastValue, N, x) else x
    _Skip (DelayFn, N, x) = [ // TODO: merge with _Take
        selected = Loop._IsWithin (DelayFn, N, x)
        out = Gather (Boolean.Not (selected), x)
    ].out
    ElementAt (n, x) = [ // not efficient, as it filters twice. Better AND the predicates. TODO: what if n is out of range? ElementAtOrDefault
        startMask = Skip (n, x)                     // ...000111...
        mask = startMask - PastValue (0, startMask) // ...000100...
        out = Gather (mask, x)
    ]
    Single (predicate, x) = x

    #FirstOrDefault (x) = ? // can empty sequences exist or even be represented by CNTK?

    Average (x) = Sum (x) / Loop.Count(x)  // TODO: patch opQuotient to check 0/0 = 0
    Sum (x)    = FoldL (Plus,    0, x)
    LogSum (x) = FoldL (LogPlus, 0, x)
    #Max (x) = FoldL (^.Max, ?, x) // TODO: name clash; need to implement ^.
    #Min (x) = FoldL (^.Min, ?, x) // TODO: what's the init value?
    All (x) = FoldL (Boolean.And,  OnesLike (x), x)
    Any (x) = FoldL (Boolean.Or, ZeroesLike (x), x)

    # Join to create 2D fields for s2s attention?

    # Concat  (a Zip but in sequence dimension)
]

##############################################################################
# index operations
# These refer to the loop iteration itself.
##############################################################################

Loop = [
    # get the current iteration index w.r.t a node in a loop, such as a Delay node
    Iteration (x) = [
      agg = OnesLike (x) + PastValue(agg, defaultHiddenActivation=0) // a recurrence that sums up ones
    ].agg

    # get the total length of a sequence
    # TODO: in LINQ, this is an aggregation operation, so it would be long into Sequences
    Count(x) = Sequences.Last (1, Iteration (x)) // take last item of recurrence that sums up ones

    # is the current iteration the first/last of the loop?
    IsFirst (x) = _IsWithin (PastValue,   1, x)
    IsLast (x)  = _IsWithin (FutureValue, 1, x)

    IsFirstN (N, x) = _IsWithin (PastValue,   N, x)
    IsLastN  (N, x) = _IsWithin (FutureValue, N, x)

    # private helpers
    # flag whether a frame is within the first or last N frames
    _IsWithin (DelayFn/*PastValue or FutureValue*/, N, x) = DelayFn (0, Constants.ZeroesLike (x)/*false*/, timeStep=N, defaultHiddenActivation=Constants.True)

    # opposite of Id's "next"
    Previous (x) = PastValue   (0, x, timeStep=1)
    Next (x)     = FutureValue (0, x, timeStep=1)

    PreviousOrDefault (x, defaultValue=Constant (0)) =   # a delay node with initial value  --TODO: merge the two, then do in C++
    [
        flags = BS.Loop.IsFirst (x)
        out = BS.Boolean.If (flags,
                    /*then*/ BS.Sequences.Scatter (flags, defaultValue),
                    /*else*/ Previous (x))
    ].out

    NextOrDefault (x, defaultValue=Constant (0)) =   # a delay node with initial value
    [
        flags = BS.Loop.IsLast (x)
        out = BS.Boolean.If (flags,
                    /*then*/ BS.Sequences.Scatter (flags, defaultValue),
                    /*else*/ Next (x))
    ].out
]

##############################################################################
# parameter definitions
##############################################################################

Parameters =
[
    WeightParam (outputDim, inputDim) = Parameter (outputDim, inputDim, init='uniform', initValueScale=1, initOnCPUOnly=true, randomSeed=1)
    DiagWeightParam (outputDim)       = ParameterTensor ((outputDim), init='uniform', initValueScale=1, initOnCPUOnly=true, randomSeed=1) # meant to be applied elementwise
    BiasParam (dim)                   = ParameterTensor ((dim), init='fixedValue', value=0.0)
    ScalarParam()                     = BiasParam (1)

    # route input through an extra weight, for stabilization
    StabilizeElements (x, inputDim=x.dim, enabled=true) =
        if enabled
        then [
            beta = Exp (BiasParam ((inputDim)))
            result = beta .* x
        ].result
    else x

    # and the same with a scalar stabilizer shared across all components
    Stabilize (x, enabled=true) = if enabled then StabilizeElements (x, inputDim=1, enabled=true) else x
]

##############################################################################
# recurrent networks
##############################################################################

RNNs =
[
    # LSTMP -- LSTM function with projection and self-stabilization
    # Projection it enabled by passing different values for outputDim and cellDim.
    # This is the stateless version that takes the previous state as an input.
    # It returns a dictionary with three members: h and c, and dim=h.dim for convenience. prevState must have h and c.
    LSTMP (outputDim, cellDim=outputDim, x, inputDim=x.dim, prevState, enableSelfStabilization=false) =
    [
        # TODO: rename to just _
        _privateInnards = [       // encapsulate the inner workings
            dh = prevState.h // previous values
            dc = prevState.c

            // parameter macros--these carry their own weight matrices
            B() = Parameters.BiasParam (cellDim)

            #inputDim1 = inputDim
            #W(v) = Parameters.WeightParam (cellDim, inputDim)  * Parameters.StabilizeElements (v, inputDim=inputDim1, enabled=enableSelfStabilization) // input-to-hidden
            # ^^ element-wise stab, use if input is a concatenation; vv stab for entire matrix
            W(v) = Parameters.WeightParam (cellDim, inputDim)  * Parameters.Stabilize (v, enabled=enableSelfStabilization) // input-to-hidden
            H(h) = Parameters.WeightParam (cellDim, outputDim) * Parameters.Stabilize (h, enabled=enableSelfStabilization) // hidden-to-hidden
            C(c) = Parameters.DiagWeightParam (cellDim)       .* Parameters.Stabilize (c, enabled=enableSelfStabilization) // cell-to-hiddden (note: applied elementwise)

            // note: the W(x) here are all different, they all come with their own set of weights; same for H(dh), C(dc), and B()
            it = Sigmoid (W(x) + B() + H(dh) + C(dc))          // input gate(t)
            bit = it .* Tanh (W(x) + (H(dh) + B()))            // applied to tanh of input network

            ft = Sigmoid (W(x) + B() + H(dh) + C(dc))          // forget-me-not gate(t)
            bft = ft .* dc                                     // applied to cell(t-1)

            ct = bft + bit                                     // c(t) is sum of both

            ot = Sigmoid (W(x) + B() + H(dh) + C(ct))          // output gate(t)
            ht = ot .* Tanh (ct)                               // applied to tanh(cell(t))
        ]

        # our return values
        c = _privateInnards.ct          // cell value
        h = if outputDim != cellDim     // output/hidden state
            then [                      // project
                Wmr = Parameters.WeightParam (outputDim, cellDim);
                htp = Wmr * Parameters.Stabilize (_privateInnards.ht, enabled=enableSelfStabilization)
            ].htp         // TODO: ^^ extend BS syntax to allow to say: then [ Wmr = WeightParam(outputDim, cellDim) ] in Wmr * Stabilize (...)
            else _privateInnards.ht     // no projection
        dim = outputDim
    ]

    # helper function to delay h and c
    # Callers can provide their own, e.g. useful for beam decoding.
    PreviousHC (lstmState) = [
       h = Loop.Previous (lstmState.h)         // hidden state(t-1)
       c = Loop.Previous (lstmState.c)         // cell(t-1)
    ]

    # pass previousHook=BS.RNNs.NextHC instead of PreviousHC to get a right-to-left recurrence
    NextHC (lstmState) = [
       h = Loop.Next (lstmState.h)             // hidden state(t-1)
       c = Loop.Next (lstmState.c)             // cell(t-1)
    ]

    # this implements a recurrent (stateful) LSTM with projection and self-stabilization
    # It returns a record (h,c). To use its output, say .h
    # By default, this is left-to-right. Pass previousHook=BS.RNNs.NextHC for a right-to-left model.
    # TODO: remove the -2 once this works
    RecurrentLSTMP = RecurrentLSTMP2
    RecurrentLSTMP2 (outputDim, cellDim=outputDim.dim, x, inputDim=x.dim, previousHook=PreviousHC, enableSelfStabilization=false) =
    [
        prevState = previousHook (lstmState)
        inputDim1 = inputDim ; cellDim1 = cellDim ; enableSelfStabilization1 = enableSelfStabilization // TODO: BS syntax needs to allow to say ^.enableSelfStabilization
        lstmState = BS.RNNs.LSTMP (outputDim, cellDim=cellDim1, x, inputDim=inputDim1, prevState, enableSelfStabilization=enableSelfStabilization1)
    ].lstmState // we return the state record (h,c)

    # a stack of recurrent LSTMs (unidirectional)
    RecurrentLSTMPStack = RecurrentLSTMP2Stack  # TODO: remove the -2 name once this works
    RecurrentLSTMP2Stack (hiddenDims, cellDims=hiddenDims, input, inputDim=input.dim, previousHook=PreviousHC, enableSelfStabilization=false) = [
        previousHook1 = previousHook ; useStabilizer = enableSelfStabilization
        layers[i:0..Length (hiddenDims)-1] =
            RecurrentLSTMP2 (hiddenDims[i], cellDim=cellDims[i],
                             if i == 0 then input else layers[i-1].h, inputDim=if i == 0 then inputDim else hiddenDims[i-1] /*TODO: layers[i-1].dim*/,
                             previousHook=previousHook1,
                             enableSelfStabilization=useStabilizer)
    ].layers
]

##############################################################################
# Network operations
# These operations will have undefined behavior for input values != 0 or 1.
##############################################################################

Network = [
    Load(pathName) = new ComputationNetworkFromFile [ /*pathName; also needs 'precision' somewhere*/ ]
    Edit(inputModel, editFunctions, additionalRoots) = new ComputationNetworkWithEdits [ /*inputModel, editFunctions, additionalRoots*/ ]

    Editing = [
        // Create a lambda that returns its argument unless that argument == 'old', then it will return 'replacement'
        ReplaceLinksToNode (old, replacement) = (node => if IsSameObject (node, old) then replacement else node)
        ReplaceLinksToNamedNode (name, replacement) = (node => if node.name == name then replacement else node)
    ]
]

] # end of BS namespace

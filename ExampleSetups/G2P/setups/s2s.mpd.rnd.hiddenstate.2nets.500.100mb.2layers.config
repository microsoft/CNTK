# configuration file for CNTK ATIS for language understanding tasks

stderr=$LogDir$\ATIS\log
command=LSTM

type=double

LSTM=[
    # this is the maximum size for the minibatch, since sequence minibatches are really just a single sequence
  # can be considered as the maximum length of a sentence
    action=trainEncoderDecoder
  makeMode=true

#  recurrent networks are trained with minibatch
#  minibatch size, for example in language model, is the number of input words
#  e.g., 6, corresponds to having 6 inputs words from one sentence
#  In the learning process, we split an input sequence into a vector of subsequences of size T_bptt .
    minibatchSize=1000

  # need to be small since models are updated for each minibatch
    traceLevel=1
    # CPU is -1
  deviceId=$DeviceNumber$

    # for each epoch, maximum number of input words is set below
    epochSize=486085

    EncoderNetworkBuilder=[
      trainingCriterion=crossentropywithsoftmax
      evalCriterion=crossentropywithsoftmax

      defaultHiddenActivity=0.1

        # randomization range
        initValueScale=1.6

        # first layer, second layer, and output layer size
    layerSizes=84:500:500:500
    # the letter stream doesn't support context-dependent inputs
    streamSizes=84
    lookupTableOrderSizes=3

        rnnType=LSTMENCODER
        lookupTableOrder=3

       addPrior=false
       addDropoutNodes=false
       applyMeanVarNorm=false
       uniformInit=true
    ]

    DecoderNetworkBuilder=[
      trainingCriterion=crossentropywithsoftmax
      evalCriterion=crossentropywithsoftmax

 #       # default hidden layer activity
      defaultHiddenActivity=0.1

        # randomization range
        initValueScale=1.6

        # first layer, second layer, and output layer size
        # the second layer must have the same dimension as the first layer
        # because 40 is matched to the output layer dimension from encoder network
    layerSizes=40:500:500:500:40
    recurrentLayer=2:3
    # the letter stream doesn't support context-dependent inputs
    streamSizes=40
    lookupTableOrderSizes=1

        rnnType=LSTM
        lookupTableOrder=1

       addPrior=false
       addDropoutNodes=false
       applyMeanVarNorm=false
       uniformInit=true
    ]

    # configuration file, base parameters
    SGD=[
    learningRatesPerSample=0.0005:0.0005:0.0005:0.0005:0.00035:0.000175:0.0000875
        momentumPerMB=0.0

        gradientClippingWithTruncation=true
    clippingThresholdPerSample=5.0

      # use hidden states for encoder decoder training
      useHiddenStates=true
      encoderNodes="LSTM0:LSTM2"
      decoderNodes="LSTM0:LSTM2"

        # maximum number of epochs
      maxEpochs=100

#        gradientcheck=true
        sigFigs=4

        # for information purpose, number of minibatches to report progress
        numMBsToShowResult=1000

        # Whether use AdaGrad
        #gradUpdateType=AdaGrad
        
        # output model path        
        modelPath=$ExpDir$\cntkdebug.dnn

        # if validation shows that the model has no improvement, then do back-up to the previously 
        # estimated model and reduce learning rate
        loadBestModel=true

        # settings for Auto Adjust Learning Rate
        AutoAdjust=[
            # auto learning rate adjustment
          autoAdjustLR=adjustafterepoch
            reduceLearnRateIfImproveLessThan=0
            increaseLearnRateIfImproveMoreThan=1000000000

            # how much learning rate is reduced 
            learnRateDecreaseFactor=0.5

            # if continously improved, can increase learning rate by the following ratio
            learnRateIncreaseFactor=1.0

            numMiniBatch4LRSearch=100
            numPrevLearnRates=5
            numBestSearchEpoch=1
        ]


        dropoutRate=0
    ]

    encoderReader=[
      # reader to use for encoder
      # this is letter only observations

      readerType=LUSequenceReader

      ioNodeNames=letterInForward
      
      #### write definition
      wfile=$ExpDir$\ltrsequenceSentence.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      equalLength=false
      dataMultiPass=true

      letterInForward=[
        unk="<unk>"
        wordmap=$DataDir$\s31.encoder.input.map
        file=$DataDir$\s31.s2s.encoder.train.txt
  
        #typedef argvector<size_t> intargvector which is not compatible with negative number
        wordContext=0:1:2
        randomize=Auto

        inputLabel=labelsIn
        outputLabel=labels

        # number of utterances to be allocated for each minibatch
        nbruttsineachrecurrentiter=100

        # this node must be exist in the network description
        features=[
          dim=84
        ]

        #labels sections
        # this name must be exist in the network description
        labelsIn=[
          dim=1
          usewordmap=true

      # vocabulary size
          labelDim=10000
          labelMappingFile=$ExpDir$\sentenceLabelsfwd.input.txt
        labelType=Category
          beginSequence="BOS"

          usewordmap=true

          # input word list
          token=$DataDir$\s31.encoder.input.lst

          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          elementSize=4
          sectionType=labels
          mapping=[
            #redefine number of records for this section, since we don't need to save it for each data record
            wrecords=11
            #variable size so use an average string size
            elementSize=10
            sectionType=labelMapping
          ]
          category=[
            dim=11
            #elementSize=sizeof(ElemType) is default
            sectionType=categoryLabels
          ]
        ]

        #labels sections
        # this name must be exist in the network description
        labels=[
          dim=1
        labelType=Category
          beginSequence="<EOS>"

          # output token list
          token=$DataDir$\s31.decoder.input.lst

          labelMappingFile=$ExpDir$\sentenceLabelsfwd.out.txt
          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          sectionType=labels
          mapping=[
            sectionType=labelMapping
          ]
          category=[
            sectionType=categoryLabels
          ]
        ]
      ]
    ]

    decoderReader=[
      # reader to use for encoder
      # this is letter only observations

      readerType=LUSequenceReader

      ioNodeNames=phnInForward
      
      #### write definition
      wfile=$ExpDir$\ltrsequenceSentence.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      equalLength=false
      dataMultiPass=true

      ### set to true so that it can can use state activities from an encoder network
      ignoresentencebegintag=true

      phnInForward=[
        unk="<unk>"
        wordmap=$DataDir$\s31.decoder.input.map
        file=$DataDir$\s31.s2s.decoder.train.txt
  
        #typedef argvector<size_t> intargvector which is not compatible with negative number
        wordContext=0
        randomize=Auto

        inputLabel=labelsIn
        outputLabel=labels

        # number of utterances to be allocated for each minibatch
        nbruttsineachrecurrentiter=100

        # this node must be exist in the network description
        features=[
          dim=40
        ]

        #labels sections
        # this name must be exist in the network description
        labelsIn=[
          dim=1
          usewordmap=true

      # vocabulary size
          labelDim=10000
          labelMappingFile=$ExpDir$\sentenceLabelsfwd.output.txt
        labelType=Category
          beginSequence="<EOS>"

          usewordmap=true

          # input word list
          token=$DataDir$\s31.decoder.input.lst

          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          elementSize=4
          sectionType=labels
          mapping=[
            #redefine number of records for this section, since we don't need to save it for each data record
            wrecords=11
            #variable size so use an average string size
            elementSize=10
            sectionType=labelMapping
          ]
          category=[
            dim=11
            #elementSize=sizeof(ElemType) is default
            sectionType=categoryLabels
          ]
        ]

        #labels sections
        # this name must be exist in the network description
        labels=[
          dim=1
        labelType=Category
          # output token list
          token=$DataDir$\s31.decoder.input.lst
          endSequence="<EOS>"

          labelMappingFile=$ExpDir$\sentencePhnfwd.out.txt
          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          sectionType=labels
          mapping=[
            sectionType=labelMapping
          ]
          category=[
            sectionType=categoryLabels
          ]
        ]
      ]
    ]

    encoderCVReader=[
      # reader to use for encoder
      # this is letter only observations

      readerType=LUSequenceReader

      ioNodeNames=letterInForward
      
      #### write definition
      wfile=$ExpDir$\ltrsequenceSentence.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      equalLength=false

      letterInForward=[
        unk="<unk>"
        wordmap=$DataDir$\s31.encoder.input.map
        file=$DataDir$\s31.s2s.encoder.validation.txt
  
        #typedef argvector<size_t> intargvector which is not compatible with negative number
        wordContext=0:1:2
        randomize=None

        inputLabel=labelsIn
        outputLabel=labels

        # number of utterances to be allocated for each minibatch
        nbruttsineachrecurrentiter=100

        # this node must be exist in the network description
        features=[
          dim=84
        ]

        #labels sections
        # this name must be exist in the network description
        labelsIn=[
          dim=1
          usewordmap=true

      # vocabulary size
          labelDim=10000
          labelMappingFile=$ExpDir$\sentenceLabelsfwd.input.txt
        labelType=Category
          beginSequence="BOS"

          usewordmap=true

          # input word list
          token=$DataDir$\s31.encoder.input.lst

          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          elementSize=4
          sectionType=labels
          mapping=[
            #redefine number of records for this section, since we don't need to save it for each data record
            wrecords=11
            #variable size so use an average string size
            elementSize=10
            sectionType=labelMapping
          ]
          category=[
            dim=11
            #elementSize=sizeof(ElemType) is default
            sectionType=categoryLabels
          ]
        ]

        #labels sections
        # this name must be exist in the network description
        labels=[
          dim=1
        labelType=Category
          beginSequence="<EOS>"

          # output token list
          token=$DataDir$\s31.decoder.input.lst

          labelMappingFile=$ExpDir$\sentenceLabelsfwd.out.txt
          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          sectionType=labels
          mapping=[
            sectionType=labelMapping
          ]
          category=[
            sectionType=categoryLabels
          ]
        ]
      ]
    ]

    decoderCVReader=[
      # reader to use for encoder
      # this is letter only observations

      readerType=LUSequenceReader

      ioNodeNames=phnInForward
      
      #### write definition
      wfile=$ExpDir$\ltrsequenceSentence.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      equalLength=false

      phnInForward=[
        unk="<unk>"
        wordmap=$DataDir$\s31.decoder.input.map
        file=$DataDir$\s31.s2s.decoder.validation.txt
  
        #typedef argvector<size_t> intargvector which is not compatible with negative number
        wordContext=0
        randomize=None

        inputLabel=labelsIn
        outputLabel=labels

        # number of utterances to be allocated for each minibatch
        nbruttsineachrecurrentiter=100

        # this node must be exist in the network description
        features=[
          dim=40
        ]

        #labels sections
        # this name must be exist in the network description
        labelsIn=[
          dim=1
          usewordmap=true

      # vocabulary size
          labelDim=10000
          labelMappingFile=$ExpDir$\sentenceLabelsfwd.output.txt
        labelType=Category
          beginSequence="<EOS>"

          usewordmap=true

          # input word list
          token=$DataDir$\s31.decoder.input.lst

          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          elementSize=4
          sectionType=labels
          mapping=[
            #redefine number of records for this section, since we don't need to save it for each data record
            wrecords=11
            #variable size so use an average string size
            elementSize=10
            sectionType=labelMapping
          ]
          category=[
            dim=11
            #elementSize=sizeof(ElemType) is default
            sectionType=categoryLabels
          ]
        ]

        #labels sections
        # this name must be exist in the network description
        labels=[
          dim=1
        labelType=Category
          # output token list
          token=$DataDir$\s31.decoder.input.lst
          endSequence="<EOS>"

          labelMappingFile=$ExpDir$\sentencePhnfwd.out.txt
          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          sectionType=labels
          mapping=[
            sectionType=labelMapping
          ]
          category=[
            sectionType=categoryLabels
          ]
        ]
      ]
    ]
  ]
]

# set output files path
# set the nodes for outputs
# for LSTM
# accuracy:  98.16%; precision:  94.37%; recall:  94.57%; FB1:  94.47
 
LSTMTest=[
    # this is the maximum size for the minibatch, since sequence minibatches are really just a single sequence
  # can be considered as the maximum length of a sentence
  action=testEncoderDecoder

# correspond to the number of words/characteres to train in a minibatch
    minibatchSize=1
  # need to be small since models are updated for each minibatch
    traceLevel=1
  deviceId=$DeviceNumber$
  epochSize=4430000
  # which is 886 * 5000
  #recurrentLayer=1
    defaultHiddenActivity=0.1

    modelPath=$MdlDir$\cntkdebug.dnn

    # this is the node to evaluate scores
    evalNodeNames=PosteriorProb

    # this is the node to output results
    outputNodeNames=outputs

    beamWidth=1
    maxNbrTokens=10

     minibatchSize=1000

      encoderNodes="LSTM0:LSTM2"
      decoderNodes="LSTM0:LSTM2"

    encoderReader=[
      # reader to use for encoder
      # this is letter only observations

      readerType=LUSequenceReader

      ioNodeNames=letterInForward
      
      #### write definition
      wfile=$ExpDir$\ltrsequenceSentence.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      equalLength=false

      letterInForward=[
        unk="<unk>"
        wordmap=$DataDir$\s31.encoder.input.map
        file=$DataDir$\s31.s2s.encoder.test.txt
  
        #typedef argvector<size_t> intargvector which is not compatible with negative number
        wordContext=0:1:2
        randomize=None

        inputLabel=labelsIn
        outputLabel=labels

        # number of utterances to be allocated for each minibatch
        nbruttsineachrecurrentiter=1

        # this node must be exist in the network description
        features=[
          dim=84
        ]

        #labels sections
        # this name must be exist in the network description
        labelsIn=[
          dim=1
          usewordmap=true

      # vocabulary size
          labelDim=10000
          labelMappingFile=$ExpDir$\sentenceLabelsfwd.input.txt
        labelType=Category
          beginSequence="BOS"

          usewordmap=true

          # input word list
          token=$DataDir$\s31.encoder.input.lst

          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          elementSize=4
          sectionType=labels
          mapping=[
            #redefine number of records for this section, since we don't need to save it for each data record
            wrecords=11
            #variable size so use an average string size
            elementSize=10
            sectionType=labelMapping
          ]
          category=[
            dim=11
            #elementSize=sizeof(ElemType) is default
            sectionType=categoryLabels
          ]
        ]

        #labels sections
        # this name must be exist in the network description
        labels=[
          dim=1
        labelType=Category
          beginSequence="<EOS>"

          # output token list
          token=$DataDir$\s31.decoder.input.lst

          labelMappingFile=$ExpDir$\sentenceLabelsfwd.out.txt
          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          sectionType=labels
          mapping=[
            sectionType=labelMapping
          ]
          category=[
            sectionType=categoryLabels
          ]
        ]
      ]
    ]

    decoderReader=[
      # reader to use for encoder
      # this is letter only observations

      readerType=LUSequenceReader

      ioNodeNames=phnInForward
      
      #### write definition
      wfile=$ExpDir$\ltrsequenceSentence.bin
      #wsize - inital size of the file in MB
      # if calculated size would be bigger, that is used instead
      wsize=256

      #wrecords - number of records we should allocate space for in the file
      # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
      wrecords=1000
      #windowSize - number of records we should include in BinaryWriter window
      windowSize=10000

      equalLength=false

      phnInForward=[
        unk="<unk>"
        wordmap=$DataDir$\s31.decoder.input.map
        file=$DataDir$\s31.s2s.decoder.test.txt
  
        TestEncodingForDecoding=false

        #typedef argvector<size_t> intargvector which is not compatible with negative number
        wordContext=0
        randomize=None

        inputLabel=labelsIn
        outputLabel=labels

        # number of utterances to be allocated for each minibatch
        nbruttsineachrecurrentiter=1

        # this node must be exist in the network description
        features=[
          dim=40
        ]

        #labels sections
        # this name must be exist in the network description
        labelsIn=[
          dim=1
          usewordmap=true

      # vocabulary size
          labelDim=10000
          labelMappingFile=$ExpDir$\sentenceLabelsfwd.output.txt
        labelType=Category
          beginSequence="<EOS>"

          usewordmap=true

          isproposal=true
          proposalSymbolList=$DataDir$\s31.decoder.input.proposal.lst

          # input word list
          token=$DataDir$\s31.decoder.input.lst

          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          elementSize=4
          sectionType=labels
          mapping=[
            #redefine number of records for this section, since we don't need to save it for each data record
            wrecords=11
            #variable size so use an average string size
            elementSize=10
            sectionType=labelMapping
          ]
          category=[
            dim=11
            #elementSize=sizeof(ElemType) is default
            sectionType=categoryLabels
          ]
        ]

        #labels sections
        # this name must be exist in the network description
        labels=[
          dim=1
        labelType=Category
          # output token list
          token=$DataDir$\s31.decoder.input.lst
          endSequence="<EOS>"

          labelMappingFile=$ExpDir$\sentencePhnfwd.out.txt
          #### Write definition ####
          # sizeof(unsigned) which is the label index type
          sectionType=labels
          mapping=[
            sectionType=labelMapping
          ]
          category=[
            sectionType=categoryLabels
          ]
        ]
      ]
    ]

    writer=[
        writerType=LUSequenceWriter

        outputs=[
            file=$OutDir$\output.rec.txt
            token=$DataDir$\s31.decoder.input.lst
        ]
    ]
]


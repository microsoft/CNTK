#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass extbook
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Introduction
\begin_inset CommandInset label
LatexCommand label
name "chap:Introduction"

\end_inset


\end_layout

\begin_layout Section
Overview
\end_layout

\begin_layout Standard
The Computational Network Toolkit (CNTK) is a software package that makes
 it easy to design and test computational networks such as deep neural networks.
 A computational network is a style of computation where data flows through
 a graph and computations happen in the nodes of the graph.
 The goal of a computational network is to take feature data, transform
 the data through a network of simple computations, and then produce one
 or more outputs.
 The output is often some sort of decision based on the input features.
 A computational network can take many forms―such as feed-forward, recursive
 or convolutional―and includes various forms of computations and non-linearities.
 The network’s parameters are optimized to produce the 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 possible outcome for a given set of data and an optimization criteria.
\end_layout

\begin_layout Standard
In this chapter we can only give readers a taste for CNTK, and specific
 features are described in more detail later.
 With this purpose, CNTK has 5 primary features:
\end_layout

\begin_layout Itemize
CNTK is a general solution for training and testing many kinds of neural
 networks
\end_layout

\begin_layout Itemize
A user specifies a network using a simple text configuration file.
 The configuration file specifies the type of network, where to find the
 input data, and how to optimize the parameters.
 All of these design parameters are fixed in the configuration file.
\end_layout

\begin_layout Itemize
CNTK is designed to seamlessly put as many of the computations onto a GPU,
 if available, as possible.
 These types of computational networks are easy to vectorize and fit nicely
 onto GPUs.
 CNTK is compatible with GPUs that support the CUDA programming environment.
\end_layout

\begin_layout Itemize
CNTK automatically calculates the needed derivatives in order to efficiently
 perform the necessary optimizations.
 Networks are composed of many simple elements, and CNTK can keep track
 of the details to insure that the optimization is done correctly.
\end_layout

\begin_layout Itemize
CNTK can be extended by adding small amounts of C++ code to implement the
 necessary blocks.
 New data readers, non-linearities, and objective functions are easy to
 add.
\end_layout

\begin_layout Standard
Consider the work needed to build a non-standard neural network such as
 the variable parameter DNN 
\begin_inset CommandInset citation
LatexCommand cite
key "Variable-Component-Deep-Neural-Network:2014"

\end_inset

.
 Conventionally, one needs to design the network, derive the derivatives
 needed to optimize the network, implement the algorithm, and then run the
 experiments.
 These steps are error prone and time consuming.
 With CNTK, however in many cases, you only need to write a simple configuration
 file.
 The rest of this chapter describes the configuration file needed to implement
 two simple examples.
\end_layout

\begin_layout Standard
The purpose of this documentation is to describe how to use CNTK for your
 own experiments.
 The bulk of this documentation describes the definition and mathematics
 of computational networks (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CN"

\end_inset

), how to run and configure CNTK (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK"

\end_inset

), how to design networks and edit the models (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Adv"

\end_inset

), and how to extend CNTK (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Programmer"

\end_inset

).
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:ASRDecoder"

\end_inset

 describes a modern speech decoder known as Argon, and Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:ExampleSetup"

\end_inset

 describes a number of CNTK examples.
 But first we start with a couple of simple examples to show the basics
 of CNTK.
 
\end_layout

\begin_layout Section
A Simple Demo
\begin_inset Index idx
status open

\begin_layout Plain Layout
Demo ! Simple
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We used a simple Matlab script (included in the distribution) to generate
 two-dimensional data with binary labels.
 The raw data is shown below in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:A-simple-test"

\end_inset

.
 This data was stored as a text file, one sample per line, that can be read
 by the UCIFastReader method (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:UCIFastReader"

\end_inset

) in CNTK.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../figures/SimpleDemoDataReference.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:A-simple-test"

\end_inset

A simple test set to demonstrate the power and ease of CNTK.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The following configuration file tells CNTK how to process this data.
 There are three sections of this file:
\end_layout

\begin_layout Itemize
The preamble specifies which command blocks to run (Simple_Demo), which
 GPU to use, and how to log CNTK’s output.
\end_layout

\begin_layout Itemize
The Simple_Demo block specifies one action for CNTK.
 The start of the block specifies the action to take (train) but more importantl
y specifies the actions needed to train the desired network.
 This block specifies three components in order to train a new network:
\end_layout

\begin_deeper
\begin_layout Itemize
The SimpleNetworkBuilder
\begin_inset Index idx
status open

\begin_layout Plain Layout
SimpleNetworkBuilder
\end_layout

\end_inset

 block specifies the topology of the network, how to initialize the network,
 and how to evaluate its performance (cross-entry with softmax in this case).
\end_layout

\begin_layout Itemize
The SGD
\begin_inset Index idx
status open

\begin_layout Plain Layout
SGD
\end_layout

\end_inset

 (Stochastic Gradient Descent
\begin_inset Index idx
status open

\begin_layout Plain Layout
Stochastic Gradient Descent
\end_layout

\end_inset

) block tells CNTK how to optimize the network to find the best parameters.
 This includes information about miniblock size (so the computation is more
 efficient), the learning rate, and how many epochs to train.
\end_layout

\begin_layout Itemize
The reader
\begin_inset Index idx
status open

\begin_layout Plain Layout
reader
\end_layout

\end_inset

 block specifies from where to read the input data, how the features and
 labels are formatted, and how to randomize the order the data is applied
 to the network during training 
\end_layout

\end_deeper
\begin_layout Itemize
Finally, the Simple_Demo_Output block specifies how to test the network.
 Again we must specify the source of the data (we are training and testing
 on the same data to keep the example small) and how to evaluate the results.
 CNTK write the value of the two output nodes, for each test case, into
 a text file.
\end_layout

\begin_layout Standard
The entire script is shown below.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

# command=Simple_Demo_Output 
\end_layout

\begin_layout Plain Layout

command=Simple_Demo:Simple_Demo_Output
\end_layout

\begin_layout Plain Layout

# deviceId=-1 for CPU, >=0 for GPU devices 
\end_layout

\begin_layout Plain Layout

DeviceNumber=0 
\end_layout

\begin_layout Plain Layout

stderr=Demo
\end_layout

\begin_layout Plain Layout

precision=float
\end_layout

\begin_layout Plain Layout

modelPath=models/simple.dnn 
\end_layout

\begin_layout Plain Layout

deviceId=$DeviceNumber$
\end_layout

\begin_layout Plain Layout

outputNodeNames=ScaledLogLikelihood 
\end_layout

\begin_layout Plain Layout

traceLevel=1
\end_layout

\begin_layout Plain Layout

####################################### 
\end_layout

\begin_layout Plain Layout

#  TRAINING CONFIG (Simple, Fixed LR) # 
\end_layout

\begin_layout Plain Layout

#######################################
\end_layout

\begin_layout Plain Layout

Simple_Demo=[
\end_layout

\begin_layout Plain Layout

    action=train
\end_layout

\begin_layout Plain Layout

    # Notation xxx:yyy*n:zzz is equivalent to xxx,
\end_layout

\begin_layout Plain Layout

    #  then yyy repeated n times, then zzz
\end_layout

\begin_layout Plain Layout

    # example: 10:20*3:5 is equivalent to 10:20:20:20:5
\end_layout

\begin_layout Plain Layout

    SimpleNetworkBuilder=[
\end_layout

\begin_layout Plain Layout

        # 2 input, 2 50-element hidden, 2 output
\end_layout

\begin_layout Plain Layout

        layerSizes=2:50*2:2
\end_layout

\begin_layout Plain Layout

        trainingCriterion=CrossEntropyWithSoftmax
\end_layout

\begin_layout Plain Layout

        evalCriterion=ErrorPrediction
\end_layout

\begin_layout Plain Layout

        layerTypes=Sigmoid
\end_layout

\begin_layout Plain Layout

        initValueScale=1.0
\end_layout

\begin_layout Plain Layout

        applyMeanVarNorm=true
\end_layout

\begin_layout Plain Layout

        uniformInit=true
\end_layout

\begin_layout Plain Layout

        needPrior=true     ]
\end_layout

\begin_layout Plain Layout

    SGD=[
\end_layout

\begin_layout Plain Layout

        # epochSize=0 means epochSize is the size of
\end_layout

\begin_layout Plain Layout

        # the training set.
 Must be evenly divisible
\end_layout

\begin_layout Plain Layout

        # into number of data frames.
\end_layout

\begin_layout Plain Layout

        epochSize=0
\end_layout

\begin_layout Plain Layout

        minibatchSize=25
\end_layout

\begin_layout Plain Layout

        learningRatesPerMB=0.5:0.2*20:0.1
\end_layout

\begin_layout Plain Layout

        momentumPerMB=0.9
\end_layout

\begin_layout Plain Layout

        dropoutRate=0.0
\end_layout

\begin_layout Plain Layout

        maxEpochs=500
\end_layout

\begin_layout Plain Layout

    ]
\end_layout

\begin_layout Plain Layout

    # Parameter values for the reader
\end_layout

\begin_layout Plain Layout

    reader=[
\end_layout

\begin_layout Plain Layout

      # reader to use
\end_layout

\begin_layout Plain Layout

      readerType=UCIFastReader
\end_layout

\begin_layout Plain Layout

      file=SimpleDataTrain.txt
\end_layout

\begin_layout Plain Layout

      miniBatchMode=Partial
\end_layout

\begin_layout Plain Layout

      randomize=Auto
\end_layout

\begin_layout Plain Layout

      verbosity=1
\end_layout

\begin_layout Plain Layout

      features=[
\end_layout

\begin_layout Plain Layout

          dim=2      # two-dimensional input data
\end_layout

\begin_layout Plain Layout

          start=0    # Start with first element on line
\end_layout

\begin_layout Plain Layout

      ]
\end_layout

\begin_layout Plain Layout

      labels=[
\end_layout

\begin_layout Plain Layout

        start=2      # Skip two elements
\end_layout

\begin_layout Plain Layout

        dim=1        # One label dimension
\end_layout

\begin_layout Plain Layout

        labelDim=2   # Two labels possible
\end_layout

\begin_layout Plain Layout

        labelMappingFile="SimpleMapping.txt"
\end_layout

\begin_layout Plain Layout

      ]
\end_layout

\begin_layout Plain Layout

    ]
\end_layout

\begin_layout Plain Layout

]
\end_layout

\begin_layout Plain Layout

####################################### 
\end_layout

\begin_layout Plain Layout

#  OUTPUT RESUTLS (Simple)            # 
\end_layout

\begin_layout Plain Layout

#######################################
\end_layout

\begin_layout Plain Layout

Simple_Demo_Output=[
\end_layout

\begin_layout Plain Layout

    action=write
\end_layout

\begin_layout Plain Layout

    # Parameter values for the reader
\end_layout

\begin_layout Plain Layout

    reader=[
\end_layout

\begin_layout Plain Layout

      # reader to use
\end_layout

\begin_layout Plain Layout

      readerType=UCIFastReader
\end_layout

\begin_layout Plain Layout

      file=SimpleDataTest.txt
\end_layout

\begin_layout Plain Layout

      features=[
\end_layout

\begin_layout Plain Layout

          dim=2
\end_layout

\begin_layout Plain Layout

          start=0
\end_layout

\begin_layout Plain Layout

      ]
\end_layout

\begin_layout Plain Layout

      labels=[
\end_layout

\begin_layout Plain Layout

        start=2
\end_layout

\begin_layout Plain Layout

        dim=1
\end_layout

\begin_layout Plain Layout

        labelDim=2
\end_layout

\begin_layout Plain Layout

        labelMappingFile="SimpleMapping.txt"
\end_layout

\begin_layout Plain Layout

      ]
\end_layout

\begin_layout Plain Layout

    ]
\end_layout

\begin_layout Plain Layout

    outputPath=SimpleOutput    # Dump output as text
\end_layout

\begin_layout Plain Layout

] 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
You can run this script using the following command on Windows 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=Simple.config 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
or under Linux 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=Simple.config 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This CNTK job trains a feedforward network with 2-dimensional input (x and
 y coordinates), and two hidden layers with 50 units each, and then with
 two output units (one for each decision).
 CNTK uses cross entropy with softmax to measure and optimize the network’s
 performance.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Trained-output-values"

\end_inset

 shows the optimized output (before the sigmoid) of one of the two output
 nodes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../figures/SimpleDemoOutputReference.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Trained-output-values"

\end_inset

Trained output values over the region of interest for the first (of two)
 output neurons.
 The dashed line shows the position where the output is zero (the decision
 boundary).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The output neuron successfully captures the shape of the decision boundary.
 The network's error rate, shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-error-rate"

\end_inset

, over time is interesting.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../figures/SimpleDemoErrorRateReference.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training-error-rate"

\end_inset

Training error rate vs.
 epoch number for the simple demo.
 The network quickly converges to a solution that gets most of the data
 correct, before finding the non-linearity that allows it to track the 
\begin_inset Quotes eld
\end_inset

bumps
\begin_inset Quotes erd
\end_inset

 in the decision boundary.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
On a 2.2GHz Xeon processor with a low-end Quadro 4000 GPU, the training and
 testing take 11 minutes 9 seconds (it is this long because the model size
 and minibatch size are both small, and the GPU is more efficient with larger
 amounts of data).
\end_layout

\begin_layout Section
A Speech Demo
\begin_inset Index idx
status open

\begin_layout Plain Layout
Demo ! Speech
\end_layout

\end_inset

 (TIMIT
\begin_inset Index idx
status open

\begin_layout Plain Layout
TIMIT
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
A second example shows how to use CNTK to do a simple speech experiment.
 We use the LDC’s TIMIT speech database as training and testing data.
 While this data is not free, it is relatively common and illustrates a
 real speech task.
 The data is available at https://catalog.ldc.upenn.edu/LDC93S1.
\end_layout

\begin_layout Standard
There are three steps in using TIMIT with CNTK: prepare the data, train
 the network, and finally test the network’s performance.
 We will describe each step in turn.
 We provide a (cygwin) shell script to perform the necessary actions, but
 the necessary steps can easily be replicated with other shells.
\end_layout

\begin_layout Standard
The first step is to prepare the data using HTK (Hidden Markov Toolkit).
 This involves creating a list of files (the HTK script) and then passing
 this script to HTK’s HCopy program, which calculates the filterbank features
 we use in our experiment.
 Not all the files in TIMIT are usable, so we start with a list of valid
 subjects (the text file TimitSubjects.txt).
 A python script uses this list of subjects, prepends the directory where
 the TIMIT distribution is found, and creates two script files.
 The first script file is used by HCopy to tell it where to read each audio
 file (in NIST’s wave format, which is different from Microsoft’s wave format)
 and where to put the output (in a directory called features.) We also take
 this opportunity to change the directory structure so the output feature
 files (___.zda) are stored in a flat directory called Features.
 The second script file is used as input to CNTK and tells CNTK where to
 find each input file, already processed into features.
 If you do not have python, or you prefer, you can also edit these two included
 .scp files by hand so each line contains the directory where your data is
 stored.
\end_layout

\begin_layout Standard
The second step is to use HTK's HCopy program to compute the necessary speech
 features.
 In this study the features are 24-dimensional filterbank features, plus
 delta and delta-delta features, and the results are stored in the Features/
 directory.
 The original TIMIT database is supplied in multiple directories and subdirector
ies, but we've flattened the directory structure to make later processing
 easier.
 (No extra directories to create.)
\end_layout

\begin_layout Standard
The third and final step is to use CNTK to compute a model that predicts
 the correct tri-phone label.
 The neural network has 792 input nodes, 3 hidden layers with 512 nodes
 each, and 183 output nodes.
\end_layout

\begin_layout Description
Input: The number of input nodes in the network divided by the dimensionality
 of the HTK feature vector determines the width of the context window.
 In this case 11 frames are used as input to the network.
 
\end_layout

\begin_layout Description
Output: The TIMIT data is labeled with 61 different phonemes, and the network
 is trained to recognize tri-phones.
 Thus there are 61x3=183 output nodes.
\end_layout

\begin_layout Standard
We use a couple of Matlab scripts to analyze the output data and compute
 the phone confusion matrix.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:phone-confusion-matrices"

\end_inset

 shows the confusion matrices after 1 and 100 epochs.
 As expected, there is much less energy in the off-diagonal terms as the
 number of epochs grows.
 This is also shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:speech-training-error-rate"

\end_inset

 which shows the training error rate calculated after each epoch.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../figures/ConfusionData1.png
	scale 35

\end_inset


\begin_inset Graphics
	filename ../figures/ConfusionData100.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:phone-confusion-matrices"

\end_inset

(Square root of the) phone confusion matrices after 1 (left) and 100 (right)
 epochs.
 To highlight the off-diagonal terms in these figures, we are displaying
 the square root of all the matrix elements.
 The single bright yellow off-diagonal pixel in the epoch 1 result corresponds
 to phones that are supposed to be 
\begin_inset Quotes eld
\end_inset

pau
\begin_inset Quotes erd
\end_inset

 (silence) but are confused with 
\begin_inset Quotes eld
\end_inset

h#
\begin_inset Quotes erd
\end_inset

 (the beginning/end of the word.).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../figures/SpeechErrorRate.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:speech-training-error-rate"

\end_inset

Training error rate as a function of epoch number for the TIMIT speech example.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

On a 2.2GHz Xeon processor the training takes 167 minutes without a GPU,
 and 61 minutes with a low-end Quadro 4000 GPU.
 
\end_layout

\end_body
\end_document

#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass extbook
\begin_preamble
\usepackage{algorithm}
\usepackage{algpseudocode}  
\end_preamble
\use_default_options false
\master CNTKBook-master.lyx
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\small},breaklines=true,frame=tb"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Computational Network Toolkit
\begin_inset CommandInset label
LatexCommand label
name "chap:CNTK"

\end_inset


\end_layout

\begin_layout Standard
The computational network toolkit (CNTK) implements CNs.
 It is written in C++ and supports both GPU (CUDA) and CPU execution.
 CNTK supports arbitrary valid computational networks and makes building
 DNNs, CNNs, RNNs, LSTMs, and other complicated networks as simple as describing
 the operations of the networks.
 The toolkit is implemented with efficiency in mind.
 It removes duplicated computations in both forward and backward passes,
 uses minimal memory needed and reduces memory reallocation by reusing them.
 It also speeds up the model training and evaluation by doing batch computation
 whenever possible.
 In this chapter we introduce how to exploit CNTK to accelerate your research.
\end_layout

\begin_layout Section
Run CNTK
\begin_inset Index idx
status open

\begin_layout Plain Layout
Run CNTK
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To run CNTK you use a command line similar to
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp.cntk
\end_layout

\end_inset

where yourExp.cntk is a CNTK configuration file, which typically contains
 several command blocks.
 A command block is a top level block of the configuration.
 Each command block must specify what action to be carried out with related
 information.
 To illustrate configuration and command blocks, we use a simple example
 below.
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

command=mnistTrain
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mnistTrain=[ 
\end_layout

\begin_layout Plain Layout

  action="train"
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  NDLNetworkBuilder=[ 
\end_layout

\begin_layout Plain Layout

    networkDescription="c:
\backslash
cntk
\backslash
config
\backslash
sample.ndl" 
\end_layout

\begin_layout Plain Layout

    run=ndlMacroUse
\end_layout

\begin_layout Plain Layout

  ] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  SGD=[ 
\end_layout

\begin_layout Plain Layout

    modelPath="c:
\backslash
cntk
\backslash
model
\backslash
mnist.cn"
\end_layout

\begin_layout Plain Layout

    learningRatesPerMB=0.001 
\end_layout

\begin_layout Plain Layout

    minibatchSize=32 
\end_layout

\begin_layout Plain Layout

    epochSize=60000 
\end_layout

\begin_layout Plain Layout

    maxEpochs=50 
\end_layout

\begin_layout Plain Layout

  ] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  reader=[ 
\end_layout

\begin_layout Plain Layout

    readerType="UCIFastReader"
\end_layout

\begin_layout Plain Layout

    file="c:
\backslash
cntk
\backslash
data
\backslash
mnist
\backslash
mnist_train.txt"
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    features=[ 
\end_layout

\begin_layout Plain Layout

      dim=784 
\end_layout

\begin_layout Plain Layout

      start=1 
\end_layout

\begin_layout Plain Layout

    ] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    labels=[ 
\end_layout

\begin_layout Plain Layout

      dim=1 
\end_layout

\begin_layout Plain Layout

      start=0 
\end_layout

\begin_layout Plain Layout

      labelDim=10 
\end_layout

\begin_layout Plain Layout

	  labelMappingFile="c:
\backslash
cntk
\backslash
data
\backslash
mnist
\backslash
mnistlabels.txt"
\end_layout

\begin_layout Plain Layout

    ] 
\end_layout

\begin_layout Plain Layout

  ]
\end_layout

\begin_layout Plain Layout

]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, you can notice that all configuration values are specified
 as a name-value pair.
 A value can be a numeric, a string, a list, or even a block of configurations.
 
\end_layout

\begin_layout Standard
The top-level configuration parameter 
\emph on
command
\emph default
 determines what command blocks are to be executed and in what order they
 are executed if more than one command block is specified.
 In this example, the mnistTrain command block will be executed.
 This command block specifies the action, which is 
\emph on
train
\emph default
 in this case, to execute.
 There are often three parameter blocks associated with the 
\emph on
train
\emph default
 action: a network builder block, which specifies how to build a network
 from scratch and how to load a model from an existing model file, a learner
 block, which specifies what training algorithm to use, and a reader block,
 which specifies where and how to load features and labels.
 In this specific example, the NDL (network definition language) network
 builder indicated by the NDLNetworkBuilder block is used to define the
 network, the stochastic gradient descent learning algorithm as indicated
 by the SGD block is used to train the model, and the UCIFastReader is used
 to load the feature and labels from file in UCI format.
 Note that readers are implemented as separate DLLs, and the name of the
 reader is also the name of the DLL file that will be loaded to read data.
\end_layout

\begin_layout Standard
The most frequently used configuration blocks are:
\end_layout

\begin_layout Itemize
Network Builders
\begin_inset Index idx
status open

\begin_layout Plain Layout
Network ! Builders
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
SimpletNetworkBuilder
\begin_inset Index idx
status open

\begin_layout Plain Layout
SimpletNetworkBuilder
\end_layout

\end_inset

: creates one of the predefined networks with limited customization.
 
\end_layout

\begin_layout Itemize
NDLNetworkBuilder
\begin_inset Index idx
status open

\begin_layout Plain Layout
NDLNetworkBuilder
\end_layout

\end_inset

: creates a network defined using the network description language (NDL).
 It provides full flexibility in designing your own network operations and
 structure.
 
\end_layout

\end_deeper
\begin_layout Itemize
Learners
\begin_inset Index idx
status open

\begin_layout Plain Layout
Learners
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
SGD
\begin_inset Index idx
status open

\begin_layout Plain Layout
SGD
\end_layout

\end_inset

 : uses the stochastic gradient descent algorithm to train the model.
 It is the desired trainer for most applications.
 
\end_layout

\end_deeper
\begin_layout Itemize
Data Readers
\begin_inset Index idx
status open

\begin_layout Plain Layout
Data Reader
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
UCIFastReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
UCIFastReader
\end_layout

\end_inset

: reads the text-based UCI format, which contains labels and features combined
 in one file.
\end_layout

\begin_layout Itemize
HTKMLFReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
HTKMLFReader
\end_layout

\end_inset

: reads the HTK/MLF format files, often used in speech recognition applications.
\end_layout

\begin_layout Itemize
BinaryReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
BinaryReader
\end_layout

\end_inset

: reads files in a CNTK binary format.
 It is also used by UCIFastReader to cache the dataset in a binary format
 for faster processing.
 
\end_layout

\begin_layout Itemize
LMSequenceReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
LMSequenceReader
\end_layout

\end_inset

: reads text-based files that contain word sequences, for predicting word
 sequences.
 This is often used in language modeling.
\end_layout

\begin_layout Itemize
LUSequenceReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
LUSequenceReader
\end_layout

\end_inset

: reads text-based files that contain word sequences and their labels.
 This is often used for language understanding.
 
\end_layout

\end_deeper
\begin_layout Standard
In the following, we will describe CNTK configuration and the above blocks
 in detail.
 
\end_layout

\begin_layout Section
Network Builders
\end_layout

\begin_layout Standard
Computational networks are constructed using either the network description
 language (NDL) builder or the simple network builder.
 
\end_layout

\begin_layout Subsection
The Simple Network Builder
\begin_inset Index idx
status open

\begin_layout Plain Layout
Simple Network Builder
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The behavior of the simple network builder is controlled by the SimpleNetworkBui
lder block of the options.
 When an option is omitted the default value is assumed.
 Here we list all the control parameters available.
\end_layout

\begin_layout Itemize

\emph on
initValueScale
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
initValueScale
\end_layout

\end_inset

: the value for scaling the range of the random numbers used for initialization.
 Default is 1.
 If the model parameters are initialized using the uniform distribution,
 the random number range will be adjusted to 
\begin_inset Formula $\left[-0.05\times initValueScale,0.05\times initValueScale\right]$
\end_inset

.
 If the model parameters are initialized using the Gaussian distribution,
 the standard deviation will be adjusted to
\begin_inset Formula $0.2\times initValueScale/\sqrt{fanout}$
\end_inset

.
\end_layout

\begin_layout Itemize

\emph on
layerTypes
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
layerTypes
\end_layout

\end_inset

: the type of nonlinear operation in hidden layers.
 Valid values are sigmoid (default), Tanh, and RectifiedLinear.
 
\end_layout

\begin_layout Itemize

\emph on
uniformInit
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
uniformInit
\end_layout

\end_inset

: determines whether to use uniform distribution to initialize model parameters.
 Valid values are true (default) and false (using Gaussian distribution
 to initialize model parameters).
\end_layout

\begin_layout Itemize

\emph on
applyMeanVarNorm
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
applyMeanVarNorm
\end_layout

\end_inset

: whether to apply mean/variance normalization on the input.
 Valid values are true and false (default) 
\end_layout

\begin_layout Itemize

\emph on
addDropoutNodes
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
addDropoutNodes
\end_layout

\end_inset

: whether to add drop-out nodes.
 The default is false.
 If specified to true, a drop-out node will be applied to the input node
 and the output of every hidden layer.
 
\end_layout

\begin_layout Itemize

\emph on
layerSize
\emph default
s
\begin_inset Index idx
status open

\begin_layout Plain Layout
layerSizes
\end_layout

\end_inset

: specifies the dimensions of layers.
 For instance, layerSizes=128:10:200:4000 describes a neural network with
 two hidden layers.
 The first hidden layer has a dimension of 10, and the second hidden layer
 has a dimension of 200.
 The input and output layers have a dimension of 128 and 4000, respectively.
 
\end_layout

\begin_layout Itemize

\emph on
trainingCriterion
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
trainingCriterion
\end_layout

\end_inset

: the criterion used for training.
 The default is CrossEntropyWithSoftmax.
 Alternatives are SquareError, CrossEntropy, and ClassBasedCrossEntropyWithSoftm
ax.
 The ClassBasedCrossEntropyWithSoftmax is for class-based training, which
 would be useful if the output dimension is large and therefore need to
 be split into classes to speed-up the training and evaluation.
\end_layout

\begin_layout Itemize

\emph on
evalCriterion
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
evalCriterion
\end_layout

\end_inset

: the criterion for evaluation.
 The selection of values are the same as the 
\emph on
trainingCriterion
\emph default
.
 
\end_layout

\begin_layout Itemize

\emph on
lookupTableOrder
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
lookupTableOrder
\end_layout

\end_inset

: specifies the order of context expanding in the lookupNode.
 The default value is 1.
 Setting it to a value such as 3 would expand the input dimension in a context-d
ependent way by an order of 3.
 For example, if the input observation has a dimension of 20, setting this
 value to 3 would set the input node dimension to 60.
 
\end_layout

\begin_layout Standard
For recurrent neural networks (RNNs), there are additional parameters.
 
\end_layout

\begin_layout Itemize

\emph on
recurrentLayer
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
recurrentLayer
\end_layout

\end_inset

: specifies the layers that contain self recurrent connections.
 By default there is no recurrent layer.
 Use the syntax n1:n2:n3 to specify that layers n1, n2, and n3 have recurrent
 connections.
\end_layout

\begin_layout Itemize

\emph on
defaultHiddenActivity
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
defaultHiddenActivity
\end_layout

\end_inset

: the default hidden layer activity value used by the delay node when accessing
 values before the first observation.
 The default value is 0.1.
 
\end_layout

\begin_layout Itemize

\emph on
rnnType
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
rnnType
\end_layout

\end_inset

: the type of predefined networks.
 Valid values are 
\end_layout

\begin_deeper
\begin_layout Itemize
SIMPLENET
\begin_inset Index idx
status open

\begin_layout Plain Layout
SIMPLENET
\end_layout

\end_inset

: the feed-forward neural network.
 This is the default network type.
 
\end_layout

\begin_layout Itemize
SIMPLERNN
\begin_inset Index idx
status open

\begin_layout Plain Layout
SIMPLERNN
\end_layout

\end_inset

: the simple RNN, which may be a deep RNN in which several layers have recurrent
 loops.
 
\end_layout

\begin_layout Itemize
CLASSLM
\begin_inset Index idx
status open

\begin_layout Plain Layout
CLASSLM
\end_layout

\end_inset

: the class-based simple RNN.
 It uses sparse input, sparse parameter and sparse output.
 This is often used for language modeling tasks.
\end_layout

\begin_layout Itemize
LBLM
\begin_inset Index idx
status open

\begin_layout Plain Layout
LBLM
\end_layout

\end_inset

: the log-bilinear neural network.
 
\end_layout

\begin_layout Itemize
LSTM
\begin_inset Index idx
status open

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset

: the long short-term memory neural network.
 
\end_layout

\begin_layout Itemize
CLASSLSTM
\begin_inset Index idx
status open

\begin_layout Plain Layout
CLASSLSTM
\end_layout

\end_inset

: the class-based long short-term memory neural network.
 It uses sparse input, sparse parameter and sparse output.
 This is often used for language modeling tasks.
\end_layout

\end_deeper
\begin_layout Subsection
The Network Description Language
\begin_inset Index idx
status open

\begin_layout Plain Layout
Network ! Description Language
\end_layout

\end_inset

 (NDL) 
\end_layout

\begin_layout Standard
The simple network builder only allows you to build one of the predefined
 network types.
 To build more complicated networks, you need to use the NDL network builder
 which has the following parameters.
 Detailed description on NDL can be found in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Adv"

\end_inset

.
\end_layout

\begin_layout Itemize

\emph on
networkDescription
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
networkDescription
\end_layout

\end_inset

: the file path of the NDL script, which will be described in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Adv"

\end_inset

, to execute.
 If there is no networkDescription file specified then the NDL is assumed
 to be in the same configuration file as the NDLNetworkBuilder subblock,
 specified with the 
\emph on
run
\emph default
 parameter.
  Note that only one file path may be specified via the 
\emph on
networkDescription
\emph default
 parameter.
 To load multiple files of macros, use the 
\emph on
ndlMacros
\emph default
 parameter.
\end_layout

\begin_layout Itemize

\emph on
ru
\emph default
n
\begin_inset Index idx
status open

\begin_layout Plain Layout
run
\end_layout

\end_inset

: the block of the NDL that will be executed.
 If an external NDL file is specified via the 
\emph on
networkDescription
\emph default
 parameter, the run parameter identifies a block in that file.
 This parameter overrides any run parameters that may already exist in the
 file.
 If no 
\emph on
networkDescription
\emph default
 file is specified, the run parameter identifies a block in the current
 configuration file.
\end_layout

\begin_layout Itemize

\emph on
load
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
load
\end_layout

\end_inset

: the blocks of NDL scripts to load.
  Multiple blocks can be specified via a ":" separated list.
 The blocks specified by the load parameter typically contain macros, which
 will be described in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Adv"

\end_inset

, for use by the run block.
 Similar to the run parameter, the load parameter identifies blocks in an
 external NDL file and overrides any load parameters that may already exist
 in the file, if a file is specified by the networkDescription parameter.
 If no networkDescription file is specified, load identifies a block in
 the current configuration file.
\end_layout

\begin_layout Itemize

\emph on
ndlMacros
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
ndlMacros
\end_layout

\end_inset

: the file path where NDL macros may be loaded.
 This parameter is usually used to load a default set of NDL macros that
 can be used by all NDL scripts.
 Multiple NDL files, each specifying different sets of macros, can be loaded
 by specifying a "+" separated list of file paths for this 
\emph on
ndlMacros
\emph default
 parameter.
 In order to share macros with other command blocks such as model editing
 language (MEL) blocks we will describe in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Adv"

\end_inset

, you should define it at the root level of the configuration file.
\end_layout

\begin_layout Itemize

\emph on
randomSeedOffset
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
randomSeedOffset
\end_layout

\end_inset

: a non-negative random seed offset value in initializing the learnable
 parameters.
 The default value is 0.
 This allows users to run experiments with different random initialization.
\end_layout

\begin_layout Section
Learners
\end_layout

\begin_layout Standard
Up to now, CNTK implements an SGD learner.
 New learners will be added in the future.
 
\end_layout

\begin_layout Subsection
Stochastic Gradient Descent Learner
\begin_inset Index idx
status open

\begin_layout Plain Layout
Stochastic Gradient Descent ! Learner
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The behavior of the SGD algorithm is controlled by the SGD block of the
 options.
 When an option is omitted the default value is assumed.
 Here we list all the control parameters used.
\end_layout

\begin_layout Subsubsection
Training process control
\end_layout

\begin_layout Itemize

\emph on
modelPath
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
modelPath
\end_layout

\end_inset

: the full path to save the final model.
 Must be provided and points to a valid file name.
\end_layout

\begin_layout Itemize

\emph on
trainCriterionNodeName
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
trainCriterionNodeName
\end_layout

\end_inset

: the name of the training criterion node.
 If not provided the default training criterion node in the network will
 be used.
\end_layout

\begin_layout Itemize

\emph on
evalCriterionNodeName
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
evalCriterionNodeName
\end_layout

\end_inset

: the name of the evaluation criterion node.
 If not provided the default evaluation criterion node in the network will
 be used.
\end_layout

\begin_layout Itemize

\emph on
epochSize
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
epochSize
\end_layout

\end_inset

: epoch size, i.e., the number of samples in each epoch.
 Often is but may be different from the dataset size.
 An intermediate model and other check point information is saved for each
 epoch.
 When set to 0 the whole dataset size is used.
\end_layout

\begin_layout Itemize

\emph on
keepCheckPointFiles
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
keepCheckPointFiles
\end_layout

\end_inset

: whether you want to keep the check point file after a new epoch starts.
 Valid values are true and false (default).
 
\end_layout

\begin_layout Itemize

\emph on
maxEpochs
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
maxEpochs
\end_layout

\end_inset

: maximum number of epochs to run.
\end_layout

\begin_layout Itemize

\emph on
minibatchSize
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
minibatchSize
\end_layout

\end_inset

: minibatch size for each epoch.
 Default value is 256.
 Can use syntax such as 128*2:1024 which means using minibatch size 128
 for 2 epochs and then 1024 for the rest.
\end_layout

\begin_layout Itemize

\emph on
dropoutRate
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
dropoutRate
\end_layout

\end_inset

: dropout rate during the training procedure.
 Default is 0.0.
 Can use syntax such as 0.5*10:0.2 which means using dropout rate 0.5 for 10
 epochs and then 0.2 for the rest.
\end_layout

\begin_layout Itemize

\emph on
maxTempMemSizeInSamplesForCNN
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
maxTempMemSizeInSamplesForCNN
\end_layout

\end_inset

: maximum temporary memory used (in number of samples) when packaging and
 unpackaging input features.
 Default is 0, which means using any value as needed.
 Useful to control the memory foot print esp.
 when run under GPU.
\end_layout

\begin_layout Itemize

\emph on
executionEngine
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
executionEngine
\end_layout

\end_inset

: the execution engine to use.
 Valid values is synchronous (default).
\end_layout

\begin_layout Subsubsection
Learning rate and momentum control
\end_layout

\begin_layout Itemize

\emph on
learningRatesPerMB
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
learningRatesPerMB
\end_layout

\end_inset

: learning rates per minibatch.
 Useful when you want to use the same learning rate while the minibatch
 size is changed.
 Can use syntax such as 0.8*10:0.2 which means using the learning rate 0.8
 for 10 epochs and then 0.2 for the rest.
 learningRatesPerMB may be missing, for example, when learningRatesPerSample
 is provided or the automatic learning rate determination algorithm is used.
\end_layout

\begin_layout Itemize

\emph on
learningRatesPerSample
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
learningRatesPerSample
\end_layout

\end_inset

: learning rates per sample.
 Useful when you want to keep the learning rates per sample constant, i.e.,
 automatically increases effective learning rate for the minibatch when
 the minibatch size is increased.
 Can use syntax such as 0.008*10:0.002 which means using the learning rate
 0.008 for 10 epochs and then 0.002 for the rest.
 learningRatesPerSample may be missing, for example, when learningRatesPerMB
 is provided or the automatic learning rate determination algorithm is used.
\end_layout

\begin_layout Itemize

\emph on
momentumPerMB
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
momentumPerMB
\end_layout

\end_inset

: momentum per minibatch.
 Default is 0.9.
 Can use syntax such as 0.1*2:0.9 which means using momentum 0.1 for 2 epochs
 and then 0.9 for the rest.
\end_layout

\begin_layout Itemize

\emph on
momentumPerSample
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
momentumPerSample
\end_layout

\end_inset

: momentum per sample.
 Useful when you want to keep the momentum per sample constant, i.e., automaticall
y scales effective momentum for the minibatch when the minibatch size is
 changed.
 Can use syntax such as 0.9996*10:0.998 which means using the per sample momentum
 0.9996 for 10 epochs and then 0.998 for the rest.
 momentumPerSample may be missing, for example, when momentumPerMB is provided.
\end_layout

\begin_layout Itemize

\emph on
autoAdjust
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
autoAdjust
\end_layout

\end_inset

: contains the information related to the automatic learning rate control.
 Default value is empty (
\begin_inset Quotes eld
\end_inset


\begin_inset Quotes erd
\end_inset

) which means no automatic learning rate control.
 Inside the block, there can be following values:
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
autoAdjustLR
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
autoAdjustLR
\end_layout

\end_inset

: the automatic learning rate adjustment algorithm to use.
 Valid values are None (default, don't auto adjust learning rate), AdjustAfterEp
och (check the training criterion after each epoch using the development
 set of the training set and decide whether to adjust the learning rate),
 and SearchBeforeEpoch (search the learning rate based on a small portion
 of the training set before each epoch starts).
\end_layout

\begin_layout Standard
used in the AdjustAfterEpoch
\begin_inset Index idx
status open

\begin_layout Plain Layout
AdjustAfterEpoch
\end_layout

\end_inset

 mode:
\end_layout

\begin_layout Itemize

\emph on
reduceLearnRateIfImproveLessThan
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
reduceLearnRateIfImproveLessThan
\end_layout

\end_inset

: reduce the learning rate if the improvement is less than this value.
 Default is 0.
 
\end_layout

\begin_layout Itemize

\emph on
learnRateDecreaseFactor
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
learnRateDecreaseFactor
\end_layout

\end_inset

: the learning rate decrease factor.
 Default value is 0.618.
 
\end_layout

\begin_layout Itemize

\emph on
increaseLearnRateIfImproveMoreThan
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
increaseLearnRateIfImproveMoreThan
\end_layout

\end_inset

: increase the learning rate if the improvement is larger than this value.
 Default value is 1#INF (infinity) which means never increase.
 
\end_layout

\begin_layout Itemize

\emph on
learnRateIncreaseFactor
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
learnRateIncreaseFactor
\end_layout

\end_inset

: the learning rate increase factor.
 Default value is 1.382.
 
\end_layout

\begin_layout Itemize

\emph on
loadBestModel
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
loadBestModel
\end_layout

\end_inset

: weather to load the best model if the current model decreases the performance.
 Valid values are true (default) and false.
 
\end_layout

\begin_layout Itemize

\emph on
learnRateAdjustInterval
\begin_inset Index idx
status open

\begin_layout Plain Layout
learnRateAdjustInterval
\end_layout

\end_inset


\emph default
: determine the frequency of applying the learning rate adjustment check.
 Default is 1 epoch.
 If this value is set to a value larger than 1 the learning rate adjustment
 will be based on the average criterion computed from the last learnRateAdjustIn
terval epochs.
\end_layout

\begin_layout Standard
Used in the SearchBeforeEpoch mode.
\end_layout

\begin_layout Itemize

\emph on
numMiniBatch4LRSearch
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
numMiniBatch4LRSearch
\end_layout

\end_inset

: the number of minibatches used to search the learning rate.
 Default value is 500.
 It's typically set to 10-20% of the total minibatches in an epoch.
 
\end_layout

\begin_layout Itemize

\emph on
numPrevLearnRate
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
numPrevLearnRate
\end_layout

\end_inset

: number of previous learning rates used as a hint to the search range.
 Default value is 5.
 
\end_layout

\begin_layout Itemize

\emph on
numBestSearchEpoch
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
numBestSearchEpoch
\end_layout

\end_inset

: number of epochs in which we use the best learning rate instead of the
 sufficient learning rate .
 Default value is 1.
 
\end_layout

\begin_layout Standard
Used in the Adaptive Minibatch Sizing mode.
\end_layout

\begin_layout Itemize

\emph on
numMiniBatch4LRSearch
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
numMiniBatch4LRSearch
\end_layout

\end_inset

: the number of minibatches used to search the minibatch size when in adaptive
 minibatch size mode.
 Default value is 500.
 It's typically set to 10-20% of the total minibatches in an epoch.
 This is shared with the search for learning rate in SearchBeforeEpoch mode.
 
\end_layout

\begin_layout Itemize

\emph on
autoAdjustMinibatch
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
autoAdjustMinibatch
\end_layout

\end_inset

: enable or disable whether minibatch size is adaptively adjusted.
 Default value is false.
 Adaptive minibatch sizing will begin on epochs starting after user minibatch
 sizes explicitly specified are complete.
  For example if the userspecified minibatchSize=256:1024, then 256 and 1024are
 used in the first 2 Epochs and adaptive minibatchsizing is used afterwards
 
\end_layout

\begin_layout Itemize

\emph on
minibatchSizeTuningFrequency
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
minibatchSizeTuningFrequency
\end_layout

\end_inset

: The number of epochs to skip, on a periodic basis, before dynamically
 adjusting the minibatch size.
 Default value is 1.
 
\end_layout

\begin_layout Itemize

\emph on
minibatchSizeTuningMax
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
minibatchSizeTuningMax
\end_layout

\end_inset

: The maximum size allowed for an adaptively adjusted minibatch size.
 Default value is 1048576.
 
\end_layout

\end_deeper
\begin_layout Subsubsection
Gradient control
\end_layout

\begin_layout Itemize

\emph on
gradientClippingWithTruncation
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
gradientClippingWithTruncation
\end_layout

\end_inset

: whether to use the truncation based gradient clipping to control gradient
 explosion.
 Valid values are true (default) and false.
 If it is false the norm based clipping will be used instead which is more
 expensive.
\end_layout

\begin_layout Itemize

\emph on
clippingThresholdPerSample
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
clippingThresholdPerSample
\end_layout

\end_inset

: the clipping thread for each sample.
 Default value is 1#INF which means infinity (i.e., clipping is turned off).
\end_layout

\begin_layout Itemize
\begin_inset Index idx
status open

\begin_layout Plain Layout
L2RegWeight
\end_layout

\end_inset


\emph on
L2RegWeight
\emph default
: the L2 regularization weight.
 Default is 0.
\end_layout

\begin_layout Itemize
\begin_inset Index idx
status open

\begin_layout Plain Layout
L1RegWeight
\end_layout

\end_inset


\emph on
L1RegWeight
\emph default
: the L1 regularization weight.
 Default is 0.
\end_layout

\begin_layout Itemize

\emph on
gradUpdateType
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
gradUpdateType
\end_layout

\end_inset

: gradient update type.
 Valid values are None (default, no special treatment to the gradient),
 AdaGrad, and RmsProp.
 When gradUpdateType equals to AdaGrad or RmsProp, you can control the behavior
 of the gradient update using following parameters: 
\end_layout

\begin_deeper
\begin_layout Itemize
normWithAveMultiplier
\begin_inset Index idx
status open

\begin_layout Plain Layout
normWithAveMultiplier
\end_layout

\end_inset

 : normalize the gradient with the average multipliers applied to the gradients
 by the AdaGrad/RmsProp algorithm.
 Default is true (default).
 
\end_layout

\begin_layout Standard
When gradUpdateType equals to RmsProp, you can control the behavior of the
 gradient update using following parameters:
\end_layout

\begin_layout Itemize

\emph on
rms_wgt_inc
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
rms_wgt_inc
\end_layout

\end_inset

: multiplicative increment of the learning rate scale.
 Default is 1.2.
\end_layout

\begin_layout Itemize

\emph on
rms_wgt_dec
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
rms_wgt_dec
\end_layout

\end_inset

: multiplicative decrement of the learning rate scale.
 Default is 0.75.
\end_layout

\begin_layout Itemize

\emph on
rms_wgt_max
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
rms_wgt_max
\end_layout

\end_inset

: maximum learning rate scale allowed.
 A value closer to 1 makes the learning rate adjustment more stable but
 slower.
 Default is 10.
\end_layout

\begin_layout Itemize

\emph on
rms_wgt_min
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
rms_wgt_min
\end_layout

\end_inset

: minimum learning rate scale allowed.
 A value closer to 1 makes the learning rate adjustment more stable but
 slower.
 Default is 0.1.
\end_layout

\begin_layout Itemize

\emph on
rms_gamma
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
rms_gamma
\end_layout

\end_inset

: smoothing factor used to estimate the moving average of the variance.
 The smaller the value, the quicker it forgets the past information.
 Default is 0.99.
 
\end_layout

\end_deeper
\begin_layout Itemize

\emph on
gaussianNoiseInjectStd
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
gaussianNoiseInjectStd
\end_layout

\end_inset

: the standard deviation of the Gaussian noise added when using the AdaGrad
 approach.
 Default is 0.
\end_layout

\begin_layout Subsubsection
Adaptation
\end_layout

\begin_layout Standard
Only KL divergence regularization is directly supported.
 Other adaptation techniques can be easily implemented by adding computation
 nodes to the network using the model editing language (MEL) that we will
 describe in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Adv"

\end_inset

.
\end_layout

\begin_layout Itemize

\emph on
adaptationRegType
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
adaptationRegType
\end_layout

\end_inset

: adaptation regularization type.
 Valid values are None (default) and KL (for KL divergence based regularization)
\end_layout

\begin_layout Itemize

\emph on
adaptationRegWeight
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
adaptationRegWeight
\end_layout

\end_inset

: adaptation regularization weight.
 Default value is 0.
\end_layout

\begin_layout Subsubsection
Information display
\end_layout

\begin_layout Itemize

\emph on
traceLevel
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
traceLevel
\end_layout

\end_inset

: trace level to decide what information to print out in the stderr.
 Valid values are 0 (default) and 1.
\end_layout

\begin_layout Itemize

\emph on
numMBsToShowResult
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
numMBsToShowResult
\end_layout

\end_inset

: display training statistics after how many minibatches.
 Default is 10.
\end_layout

\begin_layout Subsubsection
Gradient Check
\end_layout

\begin_layout Itemize

\emph on
gradientCheck
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
gradientcheck
\end_layout

\end_inset

: determines whether to use the gradient checker.
 The default value is false.
 When using the gradient checker you need to use a minibatch size that is
 larger than the sequence length for RNNs due to the truncated back-propagation
 through time (BPTT) algorithm used to train RNNs, and a smaller learning
 rate to prevent numerical issues caused by divergence.
 In addition, precision should be set to double.
\end_layout

\begin_layout Section
Data Readers
\end_layout

\begin_layout Standard
The reader block is used for all types of readers and the readerType parameter
 determines which reader to use.
 Each reader implements the same IDataReader interface which will be described
 in detail in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Programmer"

\end_inset

.
 Many parameters in the reader block are shared across different types of
 readers.
 Some are specific to a particular reader.
 In this block we will introduce the main data readers implemented in CNTK.
 
\end_layout

\begin_layout Subsection
UCIFastReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
UCIFastReader
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sub:UCIFastReader"

\end_inset


\end_layout

\begin_layout Standard
UCIFastReader reads text-based UCI format data, in which each data record
 is a line of space-delimited floating point feature and label values.
 The label information is either at the beginning or the end of each line,
 if label information is provided.
 To use the UCIFastReader you set the readerType to UCIFastReader as in
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

  reader=[ 
\end_layout

\begin_layout Plain Layout

    readerType="UCIFastReader"
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    file="c:
\backslash
cntk
\backslash
data
\backslash
mnist
\backslash
mnist_train.txt" 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    features=[ 
\end_layout

\begin_layout Plain Layout

      dim=784 
\end_layout

\begin_layout Plain Layout

      start=1 
\end_layout

\begin_layout Plain Layout

    ] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    labels=[ 
\end_layout

\begin_layout Plain Layout

      dim=1 
\end_layout

\begin_layout Plain Layout

      start=0 
\end_layout

\begin_layout Plain Layout

      labelDim=10 
\end_layout

\begin_layout Plain Layout

	  labelMappingFile="c:
\backslash
cntk
\backslash
data
\backslash
mnist
\backslash
mnistlabels.txt"
\end_layout

\begin_layout Plain Layout

    ] 
\end_layout

\begin_layout Plain Layout

  ] 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example you can also notice two sub-blocks named features and labels.
 These names are used by the data readers to match the computation node
 in your network and the data loaded from the files.
 If simple network builders are used to create your network, features and
 labels are the standard names of the feature and label nodes, respectively.
 If you defined your network using the NDL network builder you need to make
 sure these names matches the corresponding nodes in your network.
 The UCIFastReader has following parameters: 
\end_layout

\begin_layout Itemize

\emph on
file
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
file
\end_layout

\end_inset

: the file that contains the dataset.
 This parameter has been moved up from the features and labels sub-blocks,
 because UCIFastReader requires the file be the same, and moving up a level
 is an excellent way to make sure this restriction is met.
 
\end_layout

\begin_layout Itemize

\emph on
dim
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
dim
\end_layout

\end_inset

: the dimension of the input value.
 Note that each column in the UCI data file represents one dimension of
 the input data.
\end_layout

\begin_layout Itemize

\emph on
start
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
start
\end_layout

\end_inset

: the start column (zero-based) of the input data.
\end_layout

\begin_layout Itemize

\emph on
labelDim
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
labelDim
\end_layout

\end_inset

: the number of possible label values.
 This parameter is required for categorical labels since the dimension of
 the label node will be determined by this value.
 Note that the label value itself is typically specified in one column in
 the UCI data file.
\end_layout

\begin_layout Itemize

\emph on
labelMappingFile
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
labelMappingFile
\end_layout

\end_inset

: the path to a file used to map from the label value to a numerical label
 identifier.
 The file typically lists all possible label values, one per line, which
 might be text or numeric.
 The zero-based line number is the identifier that will be used by CNTK
 to identify that label.
 It's important that the same label mapping file is used for training and
 evaluation.
 This can be done by moving the 
\emph on
labelMappingFile 
\emph default
parameter up so that it can be shared by both the training and evaluation
 blocks.
\end_layout

\begin_layout Itemize
customDelimiter : the customized delimiter.
 By default, spaces are used as the delimiter.
 With this parameter you can choose to use another delimiter such as comma
 or semicolon in addition to spaces.
 Note though, this reader does not handle empty fields, e.g., two commas in
 a row, with or without whitespace in-between.
 
\end_layout

\begin_layout Itemize
customDecimalPoint: the customized decimal point.
 By default, dot is used as the decimal point.
 With this parameter you can choose to use another decimal point such as
 those used in European countries.
 
\end_layout

\begin_layout Itemize
labelType : to indicate how the label columns should be interpreted.
 By default it is set to "Category" which means it requires a mapping table
 to map from values (can be string) to class IDs so that values are converted
 to 1-hot representation.
 It can be set to "Regression" to indicate that the values are used directly
 without any conversion or "None" to indicate no labels are used.
 
\end_layout

\begin_layout Subsection
HTKMLFReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
HTKMLFReader
\end_layout

\end_inset


\end_layout

\begin_layout Standard
HTKMLFReader is a data reader that reads files typically associated with
 speech recognition tasks, and specifically with the HTK suite of tools.
 The reader can take as input two types of files, a list of feature files
 known in HTK parlance as an SCP file (
\begin_inset Quotes eld
\end_inset

script
\begin_inset Quotes erd
\end_inset

 file) and a label file known as a MLF file (
\begin_inset Quotes eld
\end_inset

model label file
\begin_inset Quotes erd
\end_inset

).
 Because CNTK can be used to build networks with arbitrary topology including
 networks with multiple inputs and outputs, the HTKMLFReader can process
 one or multiple SCP and MLF files.
 A typical example of usage for the HTKMLFReader is as follows:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

reader=[
\end_layout

\begin_layout Plain Layout

  readerType="HTKMLFReader"
\end_layout

\begin_layout Plain Layout

  readMethod="rollingWindow"
\end_layout

\begin_layout Plain Layout

  miniBatchMode="partial"
\end_layout

\begin_layout Plain Layout

  randomize="auto"
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  features=[
\end_layout

\begin_layout Plain Layout

    dim=792
\end_layout

\begin_layout Plain Layout

    scpFile="$ScpDir$
\backslash
TIMIT.train.scp.fbank.fullpath"
\end_layout

\begin_layout Plain Layout

  ]
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  labels=[
\end_layout

\begin_layout Plain Layout

    mlfFile="$MlfDir$
\backslash
TIMIT.train.align_cistate.mlf.cntk"
\end_layout

\begin_layout Plain Layout

    labelDim=183
\end_layout

\begin_layout Plain Layout

    labelMappingFile="$MlfDir$
\backslash
TIMIT.statelist"
\end_layout

\begin_layout Plain Layout

  ]
\end_layout

\begin_layout Plain Layout

] 
\end_layout

\end_inset

The HTKMLFReader has the following configuration parameters:
\end_layout

\begin_layout Itemize

\emph on
readMethod
\begin_inset Index idx
status open

\begin_layout Plain Layout
readMethod
\end_layout

\end_inset

:
\emph default
 the method used to read the features files into memory for processing during
 network training.
 The 
\begin_inset Quotes eld
\end_inset

rollingWindow
\begin_inset Quotes erd
\end_inset

 option reads in all feature files and stores them on disk in one large
 temporary binary file.
 The data is randomized by running a large rolling window over the data
 in this file and randomizing the data within the window.
 This method produces more thorough randomization of the data but requires
 a large temporary file written to disk.
 The other option is 
\begin_inset Quotes eld
\end_inset

blockRandomize
\begin_inset Quotes erd
\end_inset

 which divides the data into large blocks and then randomizes the order
 of the blocks and the order of the data within the block.
 The data in each block is read directly from the feature files.
 Because this does not require any temporary feature files, this option
 is more appropriate for large corpora.
 The default is blockRandomize.
 Note that the blockRandomize option requires an archive format of SCP file,
 described below.
 Also, as described below, when used in conjunction with 
\begin_inset Quotes eld
\end_inset

frameMode=false
\begin_inset Quotes erd
\end_inset

 the blockRandomize read method will randomize over utterances, but not
 frames.
 This is appropriate for use with recurrent architectures when preserving
 the sequential nature of the training examples is desired.
 
\end_layout

\begin_layout Itemize

\emph on
pageFilePath
\begin_inset Index idx
status open

\begin_layout Plain Layout
pageFilePath
\end_layout

\end_inset


\emph default
: specify where the temporary page file of the features should be stored.
 By default it will use system provided temp file.
\end_layout

\begin_layout Itemize

\emph on
randomize
\begin_inset Index idx
status open

\begin_layout Plain Layout
randomize
\end_layout

\end_inset

:
\emph default
 Auto or None if readMethod is rollingWindow, Auto, None, or a specified
 block size if readMethod is blockRandomize.
 It determines how the reader should randomize the data.
 When a block size is specified, only that much of data will be in memory
 at one time.
\end_layout

\begin_layout Itemize

\emph on
minibatchMode
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
minibatchMode
\end_layout

\end_inset

: Partial or Full, this option decides how to handle the last minibatch
 if there are not enough frames to form a complete minibatch of the requested
 size.
 The default is Partial, which will use the remaining frames in a smaller
 final minibatch of the training epoch.
 The Full option will only process complete minibatches.
\end_layout

\begin_layout Standard
The above example has two sources of data being processed by the reader,
 
\emph on
features,
\emph default
 in the form of a list of HTK feature files, and 
\emph on
labels,
\emph default
 which are in the form of an HTK MLF file.
 Both features and labels correspond to nodes in the computational network,
 in this case, the input and output nodes, respectively.
 Note that 
\begin_inset Quotes eld
\end_inset

features
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

labels
\begin_inset Quotes erd
\end_inset

 are the default names used by the SimpleNetworkBuilder but if the network
 is designed using the Network Description Language (NDL), then any names
 can be used, as long as they each have a corresponding node in the network.
\end_layout

\begin_layout Standard
To specify continuous-valued features, e.g.
 MFCC's or log mel filterbank coefficients, the following parameters should
 be included in the a configuration block:
\end_layout

\begin_layout Itemize

\emph on
scpFile
\begin_inset Index idx
status open

\begin_layout Plain Layout
scpFile
\end_layout

\end_inset

: 
\emph default
a list of files to be processed.
 The files should be HTK compatible files and can be specified in standard
 mode or 
\begin_inset Quotes eld
\end_inset

archive
\begin_inset Quotes erd
\end_inset

 model.
 The details of using an archive
\emph on
 
\emph default
are described below.
  configuration parameter.
\end_layout

\begin_layout Itemize

\emph on
dim
\begin_inset Index idx
status open

\begin_layout Plain Layout
dim
\end_layout

\end_inset

:
\emph default
 an integer that specifies the full feature vector dimension with the desired
 context window.
 For example, if you had 72-dimensional features (24-dimensional filterbank
 features plus delta and delta-delta coefficients) and the network is designed
 to process a context window of 11 frames, the specified dimension should
 be 792.
 Note that only symmetric context windows are supported.
 
\end_layout

\begin_layout Standard
To specify the corresponding labels, e.g.
 phoneme or senone labels, a configuration block should be used that specifies
 the following parameters:
\end_layout

\begin_layout Itemize

\emph on
mlfFile
\begin_inset Index idx
status open

\begin_layout Plain Layout
mlfFile
\end_layout

\end_inset

:
\emph default
 an HTK-style MLF file that contains the labels for all utterances specified
 in the SCP file.
 
\end_layout

\begin_layout Itemize

\emph on
labelDim
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
labelDim
\end_layout

\end_inset

: the total cardinality of the label set 
\end_layout

\begin_layout Itemize

\emph on
labelMappingFile
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
labelMappingFile
\end_layout

\end_inset

: a list of all the labels seen in the MLF file 
\end_layout

\begin_layout Standard
Note that multiple inputs and outputs can be specified using additional
 reader blocks for either features read from files listed in an SCP file
 or labels read from an MLF file.
 For example, in a multi-task learning scenario, where the network was predictin
g both speaker identity and senone label, the user would specify an additional
 block that includes an MLF file that contains labels corresponding about
 speaker identity.
 Similarly, for a network with multiple inputs, e.g.
 both MFCC and PLP features, an additional feature block would be used.
 
\end_layout

\begin_layout Standard
For recurrent structures, such as an RNN or an LSTM, there are additional
 configuration options in the HTKMLFReader
\end_layout

\begin_layout Itemize

\emph on
frameMode
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
frameMode
\end_layout

\end_inset

: true or false, whether the reader should randomize the data at the frame
 level or the utterance level.
 Setting frameMode to true is the default and is appropriate for training
 networks without any temporal connections.
 For networks that are designed to learn sequential information, e.g.
 RNN, frameMode should be set to false, which means utterances are randomized
 but proper sequence is maintained within an utterance.
 Note that this only works with the 
\begin_inset Quotes eld
\end_inset

blockRandomize
\begin_inset Quotes erd
\end_inset

 read method.
 
\end_layout

\begin_layout Itemize

\emph on
nbruttsineachrecurrentiter
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
nbruttsineachrecurrentiter
\end_layout

\end_inset

: specifies the number of utterances to process together for efficient training
 of networks with recurrent structures.
 The default is 1.
 
\end_layout

\begin_layout Itemize

\emph on
truncated
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
truncated
\end_layout

\end_inset

: true or false
\end_layout

\begin_layout Standard
It is important to note that there are some small differences between the
 standard SCP and MLF files used in HTK and the ones used in CNTK.
 
\end_layout

\begin_layout Standard
Most notably, the MLF file must contain the actual symbols used for classificati
on.
 For continuous speech recognition, this typically means labels corresponding
 to the senones (physical HMM states).
 However, HVite typically generates an MLF during forced alignment that
 includes only the logical HMM state names.
 Thus, to use this MLF in CNTK, either the MLF must be post-processed to
 replace the logical state names with thee corresponding senone labels,
 or HVite must be modified so that it writes the senone labels directly.
 
\end_layout

\begin_layout Standard
The SCP files
\begin_inset Index idx
status open

\begin_layout Plain Layout
SCP files
\end_layout

\end_inset

 processed by the HTKMLFReader can be one of two varieties: either the standard
 format, where each line corresponds to a physical feature file, or the
 aliased format, where each line contains a logical name and refers to a
 segment of a possibly larger physical file, defined by start and end frames.
 The logical name is used to identify the corresponding labels in the MLF
 file.
 Even if the files are stored individually, as in the first case, the aliased
 format must always be used with the blockRandomize read method, as it uses
 the information about the starting and ending frames in the SCP file to
 determine the utterance lengths without having to open the files themselves.
 In this case, the start frame should be 0 and the ending frame should be
 set appropriately based on the length of the utterance.
 In addition, for multiple inputs and/or outputs, the aliased format should
 also be used so that all SCP files and MLF files have their logical names
 in common.
 If the rollingWindow read method is used instead of blockRandomize, then
 the start and end frame information may be omitted.
 
\end_layout

\begin_layout Standard
Here are example snippets of SCP and MLF files for the TIMIT corpus for
 appropriate for a network with 2 feature inputs (in this case, MFCC and
 PLP features) and 1 output corresponding to phoneme states.
 The SCP file for the MFCC features contains entries such as these.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

train-dr1-fcjf0-si1027.mfc=//hostname/data/TIMIT/mfc/train/dr1/fcjf0/train-dr1-fc
jf0-si1027.mfc[0,306]
\end_layout

\begin_layout Plain Layout

train-dr1-fcjf0-si1657.mfc=//hostname/data/TIMIT/mfc/train/dr1/fcjf0/train-dr1-fc
jf0-si1657.mfc[0,281]
\end_layout

\begin_layout Plain Layout

train-dr1-fcjf0-si648.mfc=//hostname/data/TIMIT/mfc/train/dr1/fcjf0/train-dr1-fcj
f0-si648.mfc[0,359]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The SCP file for the PLP features is be similar but point to different physical
 files.
 Note that the logical root name in both SCP files is the same.
 
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "basicstyle={\footnotesize}"
inline false
status open

\begin_layout Plain Layout

train-dr1-fcjf0-si1027.plp=//hostname/data/TIMIT/plp/train/dr1/fcjf0/train-dr1-fc
jf0-si1027.plp[0,306]
\end_layout

\begin_layout Plain Layout

train-dr1-fcjf0-si1657.plp=//hostname/data/TIMIT/plp/train/dr1/fcjf0/train-dr1-fc
jf0-si1657.plp[0,281]
\end_layout

\begin_layout Plain Layout

train-dr1-fcjf0-si648.plp=//hostname/data/TIMIT/plp/train/dr1/fcjf0/train-dr1-fcj
f0-si648.plp[0,359]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The MLF file also shares the logical name with both SCP files.
 In this 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

#!MLF!# 
\end_layout

\begin_layout Plain Layout

"train-dr1-fcjf0-si1027.rec"
\end_layout

\begin_layout Plain Layout

0 200000 h#_s2 -136.655975 h# -589.680481 h#
\end_layout

\begin_layout Plain Layout

200000 400000 h#_s3 -145.780716
\end_layout

\begin_layout Plain Layout

400000 800000 h#_s4 -307.243774
\end_layout

\begin_layout Plain Layout

800000 1200000 q_s2 -349.529327 q -897.429504 q
\end_layout

\begin_layout Plain Layout

1200000 1500000 q_s3 -280.568817
\end_layout

\begin_layout Plain Layout

1500000 1800000 q_s4 -267.331390
\end_layout

\begin_layout Plain Layout

1800000 1900000 iy_s2 -76.825096 iy -673.892883 iy
\end_layout

\begin_layout Plain Layout

1900000 2400000 iy_s3 -305.832458
\end_layout

\begin_layout Plain Layout

2400000 2800000 iy_s4 -291.235352 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally, it is important to note that in most speech recognition applications,
 the continuous-valued features are used as the inputs discrete categorical
 labels are used as the output.
 However, the HTKMLFReader just associates data with node names and is agnostic
 as to how this data is used.
 For example, an appropriate MLF file of speaker identity labels could be
 used to generate a one-hot vector of speaker identity features as an input
 to the network.
 
\end_layout

\begin_layout Subsection
LMSequenceReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
LMSequenceReader
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
LMSequenceReader is a reader that reads text string.
 It is mostly often used for language modeling tasks.
 An example of its setup is as follows
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

  reader=[ 
\end_layout

\begin_layout Plain Layout

    readerType="LMSequenceReader"
\end_layout

\begin_layout Plain Layout

    randomize=false
\end_layout

\begin_layout Plain Layout

    nbruttineachrecurrentiter=10
\end_layout

\begin_layout Plain Layout

    unk="<unk>"
\end_layout

\begin_layout Plain Layout

    wordclass="$DataDir$
\backslash
wordclass.txt"
\end_layout

\begin_layout Plain Layout

    file="$DataDir$
\backslash
penntreebank.train.txt"
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    labelIn=[ 
\end_layout

\begin_layout Plain Layout

      labelDim=10000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

      beginSequence="</s>"
\end_layout

\begin_layout Plain Layout

      endSequence="</s>"
\end_layout

\begin_layout Plain Layout

    ] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

 ] 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the LMSequenceReader has following parameters: 
\end_layout

\begin_layout Itemize

\emph on
randomize
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
randomize
\end_layout

\end_inset

: it is either None or Auto.
 This specifies the mode of whether doing sentence randomization of the
 whole corpus.
 
\end_layout

\begin_layout Itemize

\emph on
nbruttsineachrecurrentiter
\begin_inset Index idx
status open

\begin_layout Plain Layout
nbruttsineachrecurrentiter
\end_layout

\end_inset

 
\emph default
: this specifies the limit of the number of sentences in a minibatch.
 The reader arranges same-length input sentences, up to the specified limit,
 into each minibatch.
 For recurrent networks, trainer resets hidden layer activities only at
 the beginning of sentences.
 Activities of hidden layers are carried over to the next minibatch if an
 end of sentence is not reached.
 Using multiple sentences in a minibatch can speed up training processes.
 
\end_layout

\begin_layout Itemize

\emph on
unk
\begin_inset Index idx
status open

\begin_layout Plain Layout
unk
\end_layout

\end_inset

 :
\emph default
 this specifies the symbol to represent unseen input symbols.
 Usually, this symbol is <unk>.
 Unseen words will be mapped to the symbol.
 
\end_layout

\begin_layout Itemize

\emph on
wordclass
\begin_inset Index idx
status open

\begin_layout Plain Layout
wordclass
\end_layout

\end_inset

 :
\emph default
 this specifies the word class information.
 This is used for class-based language modeling.
 An example of the class information is below.
 The first column is the word index.
 The second column is the number of occurrences, the third column is the
 word, and the last column is the class id of the word.
 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

    0       42068      </s>    0
\end_layout

\begin_layout Plain Layout

    1       50770      the     0
\end_layout

\begin_layout Plain Layout

    2       45020      <unk>   0
\end_layout

\begin_layout Plain Layout

    3       32481      N       0
\end_layout

\begin_layout Plain Layout

    4       24400      of      0
\end_layout

\begin_layout Plain Layout

    5       23638      to      0
\end_layout

\begin_layout Plain Layout

    6       21196      a       0
\end_layout

\begin_layout Plain Layout

    7       18000      in      1
\end_layout

\begin_layout Plain Layout

    8       17474      and     1
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\emph on
file
\begin_inset Index idx
status open

\begin_layout Plain Layout
file
\end_layout

\end_inset

 : 
\emph default
the file contains text strings.
 An example is below
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

</s> pierre <unk> N years old will join the board as a nonexecutive director
 nov.
 N </s> 
\end_layout

\begin_layout Plain Layout

</s> mr.
 <unk> is chairman of <unk> n.v.
 the dutch publishing group </s>  
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example you can also notice one sub-blocks named labelIn.
 
\end_layout

\begin_layout Itemize

\emph on
labelIn
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
labelIn
\end_layout

\end_inset

: the section for input label.
 It contains the following setups 
\end_layout

\begin_deeper
\begin_layout Itemize
beginSequence  the sentence beginning symbol 
\end_layout

\begin_layout Itemize
endSequence  the sentence ending symbol 
\end_layout

\begin_layout Itemize
labelDim  the dimension of labels.
 This usually means the vocabulary size.
 
\end_layout

\end_deeper
\begin_layout Subsection
LUSequenceReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
LUSequenceReader
\end_layout

\end_inset


\end_layout

\begin_layout Standard
LUSequenceReader is similar to SequenceReader.
 It however is used for language understanding tasks which have input and
 output strings that are different.
 An example of setting up LUSequenceReader is as follows
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

  reader=[ 
\end_layout

\begin_layout Plain Layout

    readerType="LUSequenceReader" 
\end_layout

\begin_layout Plain Layout

    randomize="none"
\end_layout

\begin_layout Plain Layout

    wordContext=0:1:2
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    nbrUttsInEachRecurrentIter=10
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    unk="<unk>"
\end_layout

\begin_layout Plain Layout

    wordMap="$DataDir$
\backslash
inputmap.txt"
\end_layout

\begin_layout Plain Layout

    file="$DataDir$
\backslash
atis.train.IOB"
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    labelIn=[ 
\end_layout

\begin_layout Plain Layout

      useWordMap=true
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

      beginSequence="BOS"
\end_layout

\begin_layout Plain Layout

      endSequence="EOS"
\end_layout

\begin_layout Plain Layout

      token="$DataDir$
\backslash
input.txt"
\end_layout

\begin_layout Plain Layout

    ] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    labels=[ 
\end_layout

\begin_layout Plain Layout

      beginSequence="O"
\end_layout

\begin_layout Plain Layout

      endSequence="O"
\end_layout

\begin_layout Plain Layout

      token="$DataDir$
\backslash
output.txt"
\end_layout

\begin_layout Plain Layout

    ] 
\end_layout

\begin_layout Plain Layout

 ] 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example, the LUSequenceReader has following parameters: 
\end_layout

\begin_layout Itemize

\emph on
wordContext
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
wordContext
\end_layout

\end_inset

: this specifies a context window.
 For example, wordContext=0:1:2 specifies a context window of 3.
 In this context window, it reads input at a current time, the next time
 and the time after the next time.
 Another example would be wordContext=0:-1.
 In such case, LUSequencReader reads a context window of 2 that consist
 of the current input and the immediate last input.
 .
\end_layout

\begin_layout Itemize

\emph on
randomize
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
randomize
\end_layout

\end_inset

: it is either None or Auto.
 This specifies the mode of whether doing sentence randomization of the
 whole corpus.
 
\end_layout

\begin_layout Itemize

\emph on
nbrUttsInEachRecurrentIter
\begin_inset Index idx
status open

\begin_layout Plain Layout

\emph on
nbrUttsInEachRecurrentIter
\end_layout

\end_inset

 
\emph default
: this specifies the limit of the number of sentences in a minibatch.
 The reader arranges same-length input sentences, up to the specified limit,
 into each minibatch.
 For recurrent networks, trainer resets hidden layer activities only at
 the beginning of sentences.
 Activities of hidden layers are carried over to the next minibatch if an
 end of sentence is not reached.
 Using multiple sentences in a minibatch can speed up training processes.
 
\end_layout

\begin_layout Itemize

\emph on
unk
\begin_inset Index idx
status open

\begin_layout Plain Layout
unk
\end_layout

\end_inset

 :
\emph default
 this specifies the symbol to represent unseen input symbols.
 Usually, this symbol is <unk>.
\end_layout

\begin_layout Itemize

\emph on
wordMap
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout

\emph on
wordMap
\end_layout

\end_inset

: this specifies a file that maps inputs to other inputs.
 This is useful if the user wants to map some inputs to unknown symbols.
 An example of the word mapping file is as follows
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

 buy buy 
\end_layout

\begin_layout Plain Layout

 trans <unk> 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\emph on
file
\begin_inset Index idx
status open

\begin_layout Plain Layout
file
\end_layout

\end_inset


\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\emph on
 : 
\emph default
the file contains input and its labels.
 The last column is the label, and the other columns contain inputs.
 An example of training file is below.
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

 BOS O 
\end_layout

\begin_layout Plain Layout

 flight O 
\end_layout

\begin_layout Plain Layout

 from O 
\end_layout

\begin_layout Plain Layout

 charlotte B-fromloc.city_name 
\end_layout

\begin_layout Plain Layout

 to O 
\end_layout

\begin_layout Plain Layout

 las B-toloc.city_name 
\end_layout

\begin_layout Plain Layout

 vegas I-toloc.city_name 
\end_layout

\begin_layout Plain Layout

 EOS O
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this example you can also notice two sub-blocks named features and labels.
 
\end_layout

\begin_layout Itemize

\emph on
labelIn
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
labelIn
\end_layout

\end_inset

: the section for input label.
 It contains the following setups 
\end_layout

\begin_deeper
\begin_layout Itemize
useWordMap  [True, False] specifies if using word map to map input words
 to other input words.
 
\end_layout

\begin_layout Itemize
beginSequence  the sentence beginning symbol 
\end_layout

\begin_layout Itemize
endSequence  the sentence ending symbol 
\end_layout

\begin_layout Itemize
token  token file contains a list of input words.
 Their orders are not important.
\end_layout

\end_deeper
\begin_layout Itemize

\emph on
label
\emph default
s
\begin_inset Index idx
status open

\begin_layout Plain Layout
labels
\end_layout

\end_inset

: the section for output label.
 
\end_layout

\begin_deeper
\begin_layout Itemize
token  token file contains a list of output labels.
 Their order is not important as long as the tokens are unique.
 
\end_layout

\end_deeper
\begin_layout Section
Top-Level Commands
\end_layout

\begin_layout Standard
Besides the 
\emph on
train
\emph default
 command, CNTK supports a list of other top-level commands.
 We list all the top-level commands and corresponding parameters in this
 section.
 
\end_layout

\begin_layout Subsection
Train
\begin_inset Index idx
status open

\begin_layout Plain Layout
Train Command
\end_layout

\end_inset

 Command
\end_layout

\begin_layout Standard
This command asks CNTK to train a model.
 The related parameters are:
\end_layout

\begin_layout Itemize
reader
\begin_inset Index idx
status open

\begin_layout Plain Layout
reader
\end_layout

\end_inset

  the reader configuration block used to determine how to load input data.
 We have covered this parameter block in previous sections.
\end_layout

\begin_layout Itemize
SGD
\begin_inset Index idx
status open

\begin_layout Plain Layout
SGD
\end_layout

\end_inset

  the SGD training setup.
\end_layout

\begin_layout Itemize
NDLNetworkBuilder
\begin_inset Index idx
status open

\begin_layout Plain Layout
NDLNetworkBuilder
\end_layout

\end_inset

  the NDL network builder configuration block.
\end_layout

\begin_layout Itemize
simpleNetworkBuilder
\begin_inset Index idx
status open

\begin_layout Plain Layout
simpleNetworkBuilder
\end_layout

\end_inset

  the simple network builder configuration block.
 
\end_layout

\begin_layout Itemize
cvReader
\begin_inset Index idx
status open

\begin_layout Plain Layout
cvReader
\end_layout

\end_inset

  (optional) the reader configuration block for cross-validation data
\end_layout

\begin_layout Itemize
makeMode
\begin_inset Index idx
status open

\begin_layout Plain Layout
makeMode
\end_layout

\end_inset

  if true (default) the training will continue from whatever epoch interrupted.
 If false the training will restart from scratch.
\end_layout

\begin_layout Itemize
numMBsToShowResult
\begin_inset Index idx
status open

\begin_layout Plain Layout
numMBsToShowResult
\end_layout

\end_inset

  indicates after how many minibatches the intermediate results should
 be shown.
\end_layout

\begin_layout Subsection
Adapt
\begin_inset Index idx
status open

\begin_layout Plain Layout
Adapt Command
\end_layout

\end_inset

 Command
\end_layout

\begin_layout Standard
This command adapts an already trained model using KL divergence regularization.
 It is advised that all other adaptations should be carried out using MEL.
 The adapt command is very similar to the train command except that it carries
 two more parameters:
\end_layout

\begin_layout Itemize
originalModelFileName  the file name of the model that will be adapted.
\end_layout

\begin_layout Itemize
refNodeName  the name of the node in the computational network which will
 be used for KL divergence regularization.
\end_layout

\begin_layout Subsection
Test
\begin_inset Index idx
status open

\begin_layout Plain Layout
Test Command
\end_layout

\end_inset

 or 
\begin_inset Index idx
status open

\begin_layout Plain Layout
Eval Command
\end_layout

\end_inset

Eval Command
\end_layout

\begin_layout Standard
This command evaluate/test a model for accuracy, usually with a test dataset.
 The related parameters are
\end_layout

\begin_layout Itemize
reader
\begin_inset Index idx
status open

\begin_layout Plain Layout
reader
\end_layout

\end_inset

  the reader configuration block to read the test data.
\end_layout

\begin_layout Itemize
modelPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
modelPath
\end_layout

\end_inset

  the path to the model to be evaluated.
\end_layout

\begin_layout Itemize
minibatchSize
\begin_inset Index idx
status open

\begin_layout Plain Layout
minibatchSize
\end_layout

\end_inset

  the minibatch size to use when reading and processing the dataset.
\end_layout

\begin_layout Itemize
epochSize
\begin_inset Index idx
status open

\begin_layout Plain Layout
epochSize
\end_layout

\end_inset

  the size of dataset.
 Default is 0.
 The entire dataset will be evaluated if it's set to 0.
 
\end_layout

\begin_layout Itemize
numMBsToShowResult
\begin_inset Index idx
status open

\begin_layout Plain Layout
numMBsToShowResult
\end_layout

\end_inset

  indicates after how many minibatches the intermediate results should
 be shown.
\end_layout

\begin_layout Itemize
evalNodeNames
\begin_inset Index idx
status open

\begin_layout Plain Layout
evalNodeNames
\end_layout

\end_inset

  an array of one or more node names to evaluate.
\end_layout

\begin_layout Subsection
CV
\begin_inset Index idx
status open

\begin_layout Plain Layout
CV Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command evaluates a series of models from different epochs on a development
 (or cross validation) set and displays the information of the best model.
 Besides the parameters used for the test command, this command also use
 the parameters
\end_layout

\begin_layout Itemize
crossValidationInterval
\begin_inset Index idx
status open

\begin_layout Plain Layout
crossValidationInterval
\end_layout

\end_inset

  the array of 3 integers identifying the starting epoch, epoch increment
 and final epoch to evaluate.
 For example, 3:2:9 means the models 3,5,7, and 9 will be evaluated.
\end_layout

\begin_layout Itemize
sleepTimeBetweenRuns
\begin_inset Index idx
status open

\begin_layout Plain Layout
sleepTimeBetweenRuns
\end_layout

\end_inset

  how many seconds to wait between runs.
 This is needed only if your GPU is too hot.
\end_layout

\begin_layout Subsection
Write Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
Write Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command writes the value of an output node to a file.
 The related parameters are
\end_layout

\begin_layout Itemize
reader
\begin_inset Index idx
status open

\begin_layout Plain Layout
reader
\end_layout

\end_inset

  the reader configuration block to read the input data.
 
\end_layout

\begin_layout Itemize
writer
\begin_inset Index idx
status open

\begin_layout Plain Layout
writer
\end_layout

\end_inset

  the writer configuration block to determine how to write the output data.
 If this value is not specified the outputPath parameter will be used.
 
\end_layout

\begin_layout Itemize
minibatchSize
\begin_inset Index idx
status open

\begin_layout Plain Layout
minibatchSize
\end_layout

\end_inset

  the minibatch size to use when reading and processing the dataset.
\end_layout

\begin_layout Itemize
epochSize
\begin_inset Index idx
status open

\begin_layout Plain Layout
epochSize
\end_layout

\end_inset

  the size of dataset.
 Default is 0.
 The entire dataset will be evaluated if it's set to 0.
 
\end_layout

\begin_layout Itemize
modelPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
modelPath
\end_layout

\end_inset

  the path to the model to be used to compute the output.
\end_layout

\begin_layout Itemize
outputPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputPath
\end_layout

\end_inset

  the path to the file to write the output in a text format.
 If the writer block exists this parameter will be ignored.
 Either outputPath or writer must exist.
\end_layout

\begin_layout Itemize
outputNodeNames
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputNodeNames
\end_layout

\end_inset

  an array of one or more output node names to be written to a file.
\end_layout

\begin_layout Subsection
Edit Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
Edit Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command executes a model editing language (MEL) script.
 We will describe MEL in detail in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:CNTK_Adv"

\end_inset

.
 Here we list the associated parameter blocks.
\end_layout

\begin_layout Itemize
editPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
editPath
\end_layout

\end_inset

  the path to the MEL script to be executed.
\end_layout

\begin_layout Itemize
ndlMacros
\begin_inset Index idx
status open

\begin_layout Plain Layout
ndlMacros
\end_layout

\end_inset

  the path to the NDL macros file that will be loaded and used in the MEL
 script.
 
\end_layout

\begin_layout Subsection
SVD Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
SVD Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command conducts SVD decomposition for learnable parameters.
 For example:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

svd=[
\end_layout

\begin_layout Plain Layout

  action="SVD" 
\end_layout

\begin_layout Plain Layout

  modelPath="train
\backslash
lstm.model.67"
\end_layout

\begin_layout Plain Layout

  outputModelPath="train
\backslash
lstm.model.svd"
\end_layout

\begin_layout Plain Layout

  SVDConfig="nodeConfigs"
\end_layout

\begin_layout Plain Layout

] 
\end_layout

\end_inset

Here we list the associated parameter blocks.
\end_layout

\begin_layout Itemize
\begin_inset Index idx
status open

\begin_layout Plain Layout
modelPath
\end_layout

\end_inset

modelPath  specifies where to load the initial model 
\end_layout

\begin_layout Itemize
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputModelPath
\end_layout

\end_inset

outputModelPath  specifies where to save the revised model 
\end_layout

\begin_layout Itemize
\begin_inset Index idx
status open

\begin_layout Plain Layout
SVDConfig
\end_layout

\end_inset

SVDConfig  a config file which specifies how the SVD is performed for different
 groups of nodes.
 This config is in a two-column format:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

<NodeNameRegex> <float>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The first column is a regular expression which will match the node name
 in one group.
 The second column is a float indicating what percentage of SVD-energy will
 be kept after SVD, where SVD-energy is defined as the sum of singular values.
 For example, the config
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

LSTMoutput[1-3].Wx[ifco] 0.4 
\end_layout

\begin_layout Plain Layout

LSTMoutput[1-3].Wh[ifco] 0.6
\end_layout

\end_inset


\end_layout

\begin_layout Standard
will cause a more aggressive SVD (0.4) decomposition for the non-recurrent
 connections (from x to i,f,c,o gates) in LSTM and less aggressive SVD decomposi
tion for recurrent connections (from h to i,f,c,o gates), where the parameter
 names are defined in the NDL.
\end_layout

\end_deeper
\begin_layout Subsection
Dumpnode Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
DumpNode Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command dumps the information of node(s) to an output file, which can
 also be accomplished in MEL with greater control.
 The related parameters are:
\end_layout

\begin_layout Itemize
modelPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
modelPath
\end_layout

\end_inset

  the path to the model file containing the nodes to dump.
\end_layout

\begin_layout Itemize
nodeName
\begin_inset Index idx
status open

\begin_layout Plain Layout
nodeName
\end_layout

\end_inset

  the name of the node to be written to a file.
 If not specified all nodes will be dumped.
\end_layout

\begin_layout Itemize
outputFile
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputFile
\end_layout

\end_inset

  the path to the output file.
 If not specified a file name will be automatically generated based on the
 modelPath.
 
\end_layout

\begin_layout Itemize
printValues
\begin_inset Index idx
status open

\begin_layout Plain Layout
printValues
\end_layout

\end_inset

  determines whether to print the values associated with a node if the
 node's values are persisted in the model.
 Default is true.
\end_layout

\begin_layout Itemize
printMetadata
\begin_inset Index idx
status open

\begin_layout Plain Layout
printMetadata
\end_layout

\end_inset

  determines whether to print the metadata (node name, dimensions, etc.)
 associated with a node.
 Default is true.
\end_layout

\begin_layout Subsection
WriteWordAndClass Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
WriteWordAndClass Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command reads the text training data, counts the number of occurrences
 of each each word in the training set, sorts the words in the descending
 order of the counts, gives each word a unique id, assign each word to a
 class, and generates a four column vocabulary file and a word-to-id mapping
 file to be used by the LMSequenceReader to train class-based language models.
 The related parameters are:
\end_layout

\begin_layout Itemize
inputFile
\begin_inset Index idx
status open

\begin_layout Plain Layout
inputFile
\end_layout

\end_inset

  the path to the text training file.
\end_layout

\begin_layout Itemize
outputVocabFile
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputVocabFile
\end_layout

\end_inset

  the name of the generated four-column vocabulary file.
 The first column is the word id, the second column is the count of the
 word, the third column is the word, and the fourth column is the class
 id.
\end_layout

\begin_layout Itemize
outputWord2Cls
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputWord2Cls
\end_layout

\end_inset

  the path to the generated word-to-class mapping file.
 
\end_layout

\begin_layout Itemize
outputCls2Index
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputCls2Index
\end_layout

\end_inset

  the path to the generated class-to-wordId mapping file.
\end_layout

\begin_layout Itemize
vocabSize
\begin_inset Index idx
status open

\begin_layout Plain Layout
vocabSize
\end_layout

\end_inset

  the desired vocabulary size.
 
\end_layout

\begin_layout Itemize
nbrClass
\begin_inset Index idx
status open

\begin_layout Plain Layout
nbrClass
\end_layout

\end_inset

  the desired number of classes .
\end_layout

\begin_layout Itemize
cutoff
\begin_inset Index idx
status open

\begin_layout Plain Layout
cutoff
\end_layout

\end_inset

  the cutoff count.
 When a word's count is below or equal to this value the word will be treated
 as <unk>.
 Default is 2.
 Note that this parameter is used only if the desired vocabulary size is
 larger than the actual number of words exist in the training set.
\end_layout

\begin_layout Subsection
CreateLabelMap Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
CreateLabelMap Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Often it is easy to manually craft the label mapping file.
 Sometimes, however, you would prefer to have the label mapping file generated
 automatically which is the purpose of the CreateLabelMap command.
 Currently UCIFastReader is the only reader that supports this action.
 The related parameters are
\end_layout

\begin_layout Itemize
section
\begin_inset Index idx
status open

\begin_layout Plain Layout
section
\end_layout

\end_inset

  the parameter block name (usually a train block) which has the reader
 sub-block that will be used to generate the label mapping file.
 The generated label mapping file will be saved to the labelMappingFile
 specified in the reader sub-block of this parameter block.
 
\end_layout

\begin_layout Itemize
minibatchSize
\begin_inset Index idx
status open

\begin_layout Plain Layout
minibatchSize
\end_layout

\end_inset

  the minibatch size to use when creating the label mapping file.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Subsection
DoEncoderDecoder Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
DoEncoderDecoder Command
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
Neural networks can be used to form a chain of networks.
 The first several networks can work as encoders and the following networks
 can serve as decoders.
 A special node, PairNetworkNode, is used in each network to serve a socket
 to be connected by other networks.
 The related parameters are 
\end_layout

\begin_layout Itemize
section
\begin_inset Index idx
status open

\begin_layout Plain Layout
section
\end_layout

\end_inset

  the encoderReader and decoderReader are the readers for encoder and decoder.
 Similarly for encoderCVReader and decoderCVReader for validation set.
 
\end_layout

\begin_layout Itemize
encoderNetworkBuilder and decoderNetworkBuilder: These specify the simple
 network builder to used.
 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Plot Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
Plot Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command loads a given computation network and describes the network
 topology (usually a DAG) using the DOT (http://www.graphviz.org/doc/info/lang.html
) language.
 It can also optionally call a third-part tool (http://www.graphviz.org/Documentat
ion/dotguide.pdf) to render the network topology.
 Note that many DOT rendering tools are optimized for DAG, and the rendered
 graph becomes quite messy if the network structure is non-DAG.
 The non-DAG structure is usually caused by the use of delay node.
 In this case, this command will treat all the delay nodes as leaf nodes.
 
\end_layout

\begin_layout Standard
The related parameters of this command are: 
\end_layout

\begin_layout Itemize
modelPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
modelPath
\end_layout

\end_inset

  the path to the computation network.
 
\end_layout

\begin_layout Itemize
outputDOTFile
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputdotFile
\end_layout

\end_inset

  the path to the output DOT file.
 If user does not specify it, ${modelPath}.dot will be assumed.
\end_layout

\begin_layout Standard
Optionally, if uses want to convert the dot description to a figure, he
 or she needs to specify the following parameters: 
\end_layout

\begin_layout Itemize
outputFile
\begin_inset Index idx
status open

\begin_layout Plain Layout
outputfile
\end_layout

\end_inset

  the path to the output figure.
 
\end_layout

\begin_layout Itemize
renderCmd
\begin_inset Index idx
status open

\begin_layout Plain Layout
renderCmd
\end_layout

\end_inset

  A string indicates how the third party tool is used to convert from a
 DOT file to a figure.
 For example, if graphviz is used, the RenderCmd can be written as 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

RenderCmd="d:
\backslash
Tools
\backslash
graphviz
\backslash
bin
\backslash
dot.exe -Tjpg <IN> -o<OUT>"
\end_layout

\end_inset


\end_layout

\begin_layout Standard
where the plot command will substitute <IN>  with the outputDOTFile command
 parameters and <OUT> with the outputFile command parameters; -Tjpg indicates
 a jpg file format is selected.
 
\end_layout

\end_deeper
\begin_layout Standard
Here is a complete example:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

command=topoplot     
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

topoplot=[
\end_layout

\begin_layout Plain Layout

  action="plot"
\end_layout

\begin_layout Plain Layout

  modelPath="train
\backslash
lstm.model.0"
\end_layout

\begin_layout Plain Layout

  outputdotFile="train
\backslash
lstm.model.dot" # this specifies the dot file to output         
\end_layout

\begin_layout Plain Layout

  # if user does not specify this, it will be ${modelPath}.dot         
\end_layout

\begin_layout Plain Layout

  outputFile="train
\backslash
lstm.model.jpg"  # this specifies the rendered image
\end_layout

\begin_layout Plain Layout

  renderCmd="d:
\backslash
Tools
\backslash
graphviz
\backslash
bin
\backslash
dot.exe  -Tjpg   <IN> -o<OUT>"         
\end_layout

\begin_layout Plain Layout

  # if RenderCmd is specified, CNTK will call the plot command after replacing
         
\end_layout

\begin_layout Plain Layout

  # <IN> with ${outputdotFile} and <OUT> with ${outputfile}     
\end_layout

\begin_layout Plain Layout

] 
\end_layout

\end_inset


\end_layout

\begin_layout Section
ConvertDBN Command
\begin_inset Index idx
status open

\begin_layout Plain Layout
ConvertDBN Command
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This command is used to convert a model generated by Microsoft's dbn.exe
 tool to a CNTK model.
 This command is useful when you want to compare the performance of these
 two tools (dbn.exe only supports simple fully connected deep neural networks),
 port existing models trained with dbn.exe to CNTK, or if you want to use
 the RBM pre-training which is available in dbn.exe but not in CNTK right
 now.
 The related parameters are
\end_layout

\begin_layout Itemize
modelPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
modelPath
\end_layout

\end_inset

  the full path of the generated CNTK model.
 
\end_layout

\begin_layout Itemize
dbnModelPath
\begin_inset Index idx
status open

\begin_layout Plain Layout
dbnModelPath
\end_layout

\end_inset

  the full path of the model to be converted.
\end_layout

\begin_layout Section
Additional Top-Level Configurations
\end_layout

\begin_layout Standard
Besides commands, there are several top-level configurations that you can
 set.
\end_layout

\begin_layout Subsection
Stderr
\begin_inset Index idx
status open

\begin_layout Plain Layout
Stderr
\end_layout

\end_inset


\end_layout

\begin_layout Standard
All logging information in CNTK is sent to standard error, and will appear
 on the console screen.
 You can redirect the logging information to a file with the stderr parameter.
 The value of the stderr parameter defines the folder and the prefix of
 the log file.
 The suffix is determined by the command block executed.
 For example, if you set
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

stderr="c:
\backslash
cntk
\backslash
log
\backslash
cntk"
\end_layout

\end_inset

and the command block mnistTrain is to be executed then the full log file
 name will be c:
\backslash
cntk
\backslash
log
\backslash
cntk_mnistTrain.log.
 It is important to note that this file is overwritten on subsequent executions
 if the stderr parameter and the command being run are identical.
\end_layout

\begin_layout Subsection
DeviceId
\begin_inset Index idx
status open

\begin_layout Plain Layout
deviceId
\end_layout

\end_inset


\end_layout

\begin_layout Standard
CNTK supports CPU and GPU computation.
 Users can determine what device to use by setting the deviceId parameter.
 The possible values are
\end_layout

\begin_layout Itemize
auto: choose the best device available.If multiple GPUs are available, it
 will choose the fastest, least busy GPU.
 If no usable GPUs can be found CPU will be used.
\end_layout

\begin_layout Itemize
cpu or -1: use the CPU for all computation
\end_layout

\begin_layout Itemize
a non-negative integer: use the GPU identified by this number to carry out
 all computation.
\end_layout

\begin_layout Itemize
all : use all the available GPU devices (currently this flag is not supported)
\end_layout

\begin_layout Subsection
Precision
\begin_inset Index idx
status open

\begin_layout Plain Layout
Precision
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The precision of the floating point values determines the tradeoff between
 the numerical accuracy and the training and testing speed.
 In CNTK the precision is specified at the top-level as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

precision="float"
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The valid values are float (default) and double.
 For most experiments you should use float since it's significantly faster,
 esp.
 when running under GPUs.
 To run gradient check you should set precision to double.
\end_layout

\begin_layout Subsection
NumCPUThreads
\begin_inset Index idx
status open

\begin_layout Plain Layout
NumCPUThreads
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The NumCPUThreads parameter determines the maximum CPU threads used for
 CPU math operations.
 In CNTK it is specified at the top-level as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

NumCPUThreads=4
\end_layout

\end_inset


\end_layout

\begin_layout Standard
By default the value is 0, which means using what ever number of threads
 determined by MKL, ACML, or OpenMP.
 If a positive number is specified, the number of threads will not be larger
 than the value specified and the number of hardware concurrency.
 If a negative number is specified, the actual number of thread will be
 the sum of the number of hardware concurrency and this value floored at
 1.
\end_layout

\begin_layout Subsection
TraceLevel
\begin_inset Index idx
status open

\begin_layout Plain Layout
traceLevel
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The traceLevel parameter is usually used by the code in CNTK to specify
 how much extra output (verbosity) is desired as in
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

traceLevel=0 # larger values mean more output
\end_layout

\end_inset

The default value is 0 and specifies minimal output.
 The higher the number the more output can be expected.
 Currently 0 (limited output), 1 (medium output) and 2 (verbose output)
 are the only values supported.
\end_layout

\begin_layout Subsection
ShareNodeValueMatrices
\begin_inset Index idx
status open

\begin_layout Plain Layout
ShareNodeValueMatrices
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The ShareNodeValueMatrices parameter is used to indicate whether to turn
 on the sharing of forward computation result matrices to further reduce
 memory usage.
 To turn it on, use
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

ShareNodeValueMatrices=true
\end_layout

\end_inset

The default value is false, which means the memory sharing only happens
 during the backward phase.
 You should decide whether to turn it on by running the same setup with
 this flag on and off.
 If the results are the same, you are safe to turn it on.
 In the future, we will make it on by default after more extensive test.
\end_layout

\begin_layout Section
Advanced Command Line Parsing Rules
\end_layout

\begin_layout Standard
Here we describe CNTK command line parsing rules.
 As described above, CNTK consists of a number of components to complete
 a task.
 Most of these components need some configuration information available
 in order to function, and these configuration parameters are provided through
 configuration files in CNTK.
\end_layout

\begin_layout Standard
Configuration files are collections of name-value pairs.
 The configuration data can be one of the following types:
\end_layout

\begin_layout Itemize
Simple: a single value is assigned to the configuration parameter.
 For example, deviceId=auto.
\end_layout

\begin_layout Itemize
Array: a configuration parameter is assigned an array of values which need
 not be of a uniform type.
 ':' is the default separator for arrays.
 The separator may be changed by enclosing the array values in parenthesis
 and placing the new separator character immediately following the open
 parenthesis.
 The '*' character allows a particular value to be repeated multiple times
 in the array.
 For example, minibatchSize=256:512:512:512:1024 equals to minibatchSize=256:512
*3:1024.
\end_layout

\begin_layout Itemize
Set: parameter sets contain sets of configuration parameters of any type.
 Parameter sets can be nested.
 The default separator for parameter sets is ';' if multiple items are included
 on one line.
 Line separators also serve as separators for items.
 For example, 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

block1=[id=1;size=256]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

block2=[
\end_layout

\begin_layout Plain Layout

  subblock=[string="hi";num=5] 
\end_layout

\begin_layout Plain Layout

  value=1e-10 
\end_layout

\begin_layout Plain Layout

  array=10:"this is a test":1.25
\end_layout

\begin_layout Plain Layout

]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In CNTK, configuration files are organized in a hierarchical fashion.
 The actual data values are not evaluated until a CNTK component requests
 the value.
 When a value is requested by a component, CNTK will search inside the component
 block first.
 If the value is not found, it will continue looking in the parent and grandpare
nt parameter set until the parameter is found, or the top level of the configura
tion hierarchy is reached without a match.
 This allows the share of the same parameter values easier across different
 blocks.
\end_layout

\begin_layout Standard
As we mentioned earlier, to run CNTK you need to specify the configuration
 file in the command line as 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp.cntk
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This will load the requested configuration file, and execute any command
 block listed in the command parameters in the configuration file.
\end_layout

\begin_layout Subsection
Commands and Actions
\end_layout

\begin_layout Standard
There must be a top-level command parameter, which defines the commands
 (separated with ':') that will be executed in the configuration file.
 Each command references a command block in the file, which must contain
 an action parameter defining the operation that block will perform.
 For example the following command will execute the mnistTrain block, which
 executes the train action, followed by the mnistTest block, which evaluates
 the model.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

command=mnistTrain:mnistTest 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mnistTrain=[ 
\end_layout

\begin_layout Plain Layout

  action="train" 
\end_layout

\begin_layout Plain Layout

  ...
\end_layout

\begin_layout Plain Layout

] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mnistTest=[ 
\end_layout

\begin_layout Plain Layout

  action="eval" 
\end_layout

\begin_layout Plain Layout

  ...
\end_layout

\begin_layout Plain Layout

] 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Configuration Overloads In the Command Line
\begin_inset Index idx
status open

\begin_layout Plain Layout
Command Line
\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is common to have a configuration that can be used as a base configuration,
 and modify only a few parameters for each experimental run.
 This can be done in a few different ways, one of which is to override settings
 on the command line.
 For example, to override the model file path, one can simply modify the
 command line as follows: 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp.cntk stderr="c:
\backslash
temp
\backslash
newpath"
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This will override the current setting for stderr, which is defined at the
 root level of the configuration file, with the new value.
 If a parameter inside a command block needs to be modified, the block also
 needs to be specified.
 For example, I can change the minibatchSize for an experiment in the command
 line as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp.cntk mnistTrain=[minibatchSize=256] 
\end_layout

\end_inset

or modify the data file used for an experiment as
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp.cntk mnistTrain=[reader=[file="mynewfile.txt"]] 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Layered Configuration Files
\begin_inset Index idx
status open

\begin_layout Plain Layout
Layered Configuration Files
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Instead of overriding some parts of a configuration file using command line
 parameters, one can also specify multiple configuration files, where the
 latter files override the earlier ones.
 This allows a user to have a 
\emph on
master
\emph default
 configuration file, and then specify, in a separate configuration file,
 which parameters of the master they would like to override for a given
 run of CNTK.
 This can be accomplished by either specifying a '+' separated list of configura
tion files, or by using the "configFile=" tag multiple times.
 The following are equivalent.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp1.cntk+yourExp2.cntk 
\end_layout

\begin_layout Plain Layout

cntk configFile=yourExp1.cntk configFile=yourExp2.cntk 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If yourExp2.cntk only contains the string "mnistTrain=[reader=[file=mynewfile.txt]
]", then both of these commands would be equivalent to: 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp1.cntk mnistTrain=[reader=[file="mynewfile.txt"]] 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that the value of a variable is always determined by the last time
 it is assigned.
 It is also possible to mix command-line parameters, and layered configuration
 files, in arbitrary combinations.
 For example,
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp1.cntk+yourExp2.cntk var1=value configFile=yourExp3.config
 
\end_layout

\end_inset

would process these configuration parameters in the order they appear on
 the command line and whatever value assigned last is the value used.
\end_layout

\begin_layout Standard
In addition being able to specify multiple configuration files at the command
 line, a user can 
\emph on
include
\emph default
 one configuration file within another.
  For example, if the first line of yourExp2.cntk was
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

include=yourExp1.cntk
\end_layout

\end_inset

then simply running 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp2.cntk
\end_layout

\end_inset

would be equivalent to running 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

cntk configFile=yourExp1.cntk+yourExp2.cntk
\end_layout

\end_inset

where in this latter case, yourExp2.cntk doesn't contain the 
\emph on
include
\emph default
 statement.
  Note that these 
\emph on
include
\emph default
 statements can appear anywhere inside a configuration file; wherever the
 include statement appears, that is where the specified configuration file
 will be 
\emph on
included
\emph default
.
  Including a configuration file is equivalent to pasting the contents of
 that file at the location of the include statement.
 Include statements are resolved recursively (using a depth-first search),
 meaning that if yourExpA.cntk includes yourExpB.cntk, and yourExpB.cntk includes
 yourExpC.cntk, then the full chain will be resolved, and yourExpC.config
 will effectively be included in yourExpA.cntk.
  If a configuration file is included multiple times (eg, 'A' includes 'B'
 and 'C', and 'B' also includes 'C'), then it will effectively only be included
 the first time it is encountered.
\end_layout

\begin_layout Subsection
Stringize Variables
\begin_inset Index idx
status open

\begin_layout Plain Layout
Stringize Variables
\end_layout

\end_inset


\end_layout

\begin_layout Standard
While layered configuration files allow users to reuse configuration files
 across experiments, this can still be a cumbersome process.
 For each experiment, a user might have to override several parameters,
 some of which might be long file paths (eg, 'stderr', 'modelPath', 'file',
 etc).
 The "stringize" functionality can make this process much easier.
 It allows a user to specify configuration like the following: 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

command=SpeechTrain 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

stderr="$Root$
\backslash
$RunName$.log" 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

speechTrain=[ 
\end_layout

\begin_layout Plain Layout

  modelPath="$Root$
\backslash
$RunName$.cn" 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  SGD=[ 
\end_layout

\begin_layout Plain Layout

	reader=[ 
\end_layout

\begin_layout Plain Layout

	  features=[ 
\end_layout

\begin_layout Plain Layout

  	  type="real"
\end_layout

\begin_layout Plain Layout

  	  dim="$DataSet1_Dim$" 
\end_layout

\begin_layout Plain Layout

  	  file="$DataSet1_Features$" 
\end_layout

\begin_layout Plain Layout

	  ]
\end_layout

\begin_layout Plain Layout

    ]
\end_layout

\begin_layout Plain Layout

  ]
\end_layout

\begin_layout Plain Layout

] 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here, "Root","RunName", "DataSet1_Dim", and "DataSet1_Features" are variables
 specified elsewhere in the configuration (at a scope visible from the point
 at which they are used).
 When interpreting this configuration file, the parser would replace every
 string of the form "$VarName$" with the string "VarValue", where "VarValue"
 represents the value of the variable called "VarName".
 The variable resolution process is recursive; for example, if A=$B$, B=$C$,
 and C=HelloWorld.txt, then A would be resolved as "HelloWorld.txt".
 Please make sure there is no reference loop in your configuration file.
 Otherwise the parser will go into infinite loop at this moment.
\end_layout

\begin_layout Standard
Notice that because it is equivalent for a user to specify the value of
 a variable in a configuration file vs.
 at the command line, the values for these variables can be specified in
 either location.
 Recall that the value of a variable is determined by the last time it is
 assigned, whether that happens to be in a configuration file, or on the
 command line.
 Thus, if "Root" is defined in config1.txt, but overridden at the command-line,
 then the value specified at the command-line would be the one used to resolve
 instances of $Root$ in configFile1.txt.
 One useful feature is that if 'stderr' or 'modelPath' point to directories
 which do not exist, these directories will be created by CNTK; this allows
 you to specify something like: "stderr=$Root$
\backslash
$RunName$
\backslash
$RunName$.log", even if the directory "$Root$
\backslash
$RunName$" doesn't exist.
 
\end_layout

\begin_layout Subsection
Default, Repeated Values, and Comments
\end_layout

\begin_layout Standard
Most parameters in configuration files have a default value which will be
 used if no configuration value is specified.
 If there is no default value and the value cannot be found in a search,
 an exception will be displayed and the program will exit.
\end_layout

\begin_layout Standard
If a parameter name is specified more than once, the last value set to that
 value is the one that will be maintained.
 The only exception to this is in parameter sets, which are surrounded by
 '[' square braces ']', in these cases the values inside the braces are
 considered to be a parameter set, and will be added to the currently existing
 parameter set.
 For example:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

params=[a=1;b=2;c=3]
\end_layout

\begin_layout Plain Layout

params=[c=5;d=6;e=7]
\end_layout

\end_inset

is effectively equal to:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

params=[a=1;b=2;c=5;d=6;e=7]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that this 
\emph on
append
\emph default
 processing is NOT used for array elements, and the entire array will be
 replaced if it is set multiple times.
\end_layout

\begin_layout Standard
The '#' character signifies the beginning of a comment, everything that
 occurs after the '#' is ignored.
 The '#' must be preceded by whitespace or be at the beginning of the line
 to be interpreted as a comment.
 The following is valid comment
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

stderr="c:
\backslash
cntk
\backslash
log
\backslash
cntk" # "_mnistTrain_mnistTest.log" 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The following is an example of a value that will not be interpreted as a
 comment.
 It sets a parameter 
\begin_inset Quotes eld
\end_inset

var
\begin_inset Quotes erd
\end_inset

 to infinity as the '#' in '1#INF' is not a comment marker
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

var=1#INF
\end_layout

\end_inset


\end_layout

\end_body
\end_document

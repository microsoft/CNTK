% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/losses.R
\name{loss_cross_entropy_with_softmax}
\alias{loss_cross_entropy_with_softmax}
\title{Cross Entropy Loss with Softmax for Multiclass Classification}
\usage{
loss_cross_entropy_with_softmax(output_vector, target_vector, axis = -1,
  name = "")
}
\arguments{
\item{output_vector}{unscaled computed output values from the network}

\item{target_vector}{one-hot encoded vector of target values}

\item{axis}{integer (optional) for axis to compute cross-entropy}

\item{name}{string (optional) - the name of the Function instance in the network}
}
\description{
This operation computes the cross entropy between the \code{target_vector} and the softmax of the \code{output_vector}.
The elements of \code{target_vector} have to be non-negative and should sum to 1.
The \code{output_vector} can contain any values.
The function will internally compute the softmax of the \code{output_vector}.
}
\references{
\url{https://www.cntk.ai/pythondocs/cntk.losses.html#cntk.losses.cross_entropy_with_softmax}
}

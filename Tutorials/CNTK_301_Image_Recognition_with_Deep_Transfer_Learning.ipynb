{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CNTK 301: Image Recognition with Deep Transfer Learning\n",
    "\n",
    "This hands-on tutorial shows how to use [Transfer Learning](https://en.wikipedia.org/wiki/Inductive_transfer) to take an existing trained model and adapt it to your own specialized domain. Note: This notebook will run only if you have a GPU-enabled machine.\n",
    "\n",
    "### Problem\n",
    "\n",
    "What if you've been given the task to identify different types of flowers, and a set of labeled flower images with all of the types of flowers you need to identify. The image below shows a small subset of the data you've been given. \n",
    "\n",
    "![Flower image set](http://www.cntk.ai/jup/cntk301_Flowers.jpg)\n",
    "\n",
    "What if the number of images you've been given is far fewer than you need in order to train a state-of-the-art classifier (such as a [Residual Network](https://github.com/KaimingHe/deep-residual-networks)). You're not out of luck, you can rely on an existing network - one trained with a rich set of images, one which might contain flowers, but also contains many more types of objects and has been trained to recognise them all. An example of that rich data-set is shown below (courtesy [t-SNE visualization site](http://cs.stanford.edu/people/karpathy/cnnembed/)).\n",
    "\n",
    "![AlexNet data sample](http://www.cntk.ai/jup/cntk301_imagenet.jpg)\n",
    "\n",
    "This tutorial introduces deep transfer learning as a means to leverage multiple data sources to overcome this data scarcity problem.\n",
    "\n",
    "### Why Transfer Learning?\n",
    "\n",
    "As stated above, Transfer Learning is a useful technique when, for instance, you know you need to classify incoming images into different categories, but you do not have enough data to train a Deep Neural Network (DNN) from scratch. Training DNNs takes a lot of data, all of it labeled, and often you will not have that kind of data on hand. If your problem is similar to one for which a network has already been trained, though, you can use Transfer Learning to modify that network to your problem with a fraction of the labeled images (we are talking tens instead of thousands). \n",
    "\n",
    "### What is Transfer Learning?\n",
    "\n",
    "With Transfer Learning, we use an existing trained model and adapt it to our own problem. We are essentially building upon the features and concepts that were learned during the training of the base model. With a Convolutional DNN (ResNet_18 in this case), we are using the features learned from ImageNet data and _cutting off_ the final classification layer, replacing it with a new dense layer that will predict the class labels of our new domain. \n",
    "\n",
    "The input to the old and the new prediction layer is the same, we simply reuse the trained features. Then we train this modified network, either only the new weights of the new prediction layer or all weights of the entire network.\n",
    "\n",
    "This can be used, for instance, when we have a small set of images that are in a similar domain to an existing trained model. Training a Deep Neural Network from scratch requires tens of thousands of images, but training one that has already learned features in the domain you are adapting it to requires far fewer. \n",
    "\n",
    "\n",
    "In our case, this means adapting a network trained on ImageNet images (dogs, cats, birds, etc.) to flowers, or sheep/wolves. However, Transfer Learning has also been successfully used to adapt existing neural models for translation, speech synthesis, and many other domains - it is a convenient way to bootstrap your learning process.\n",
    "\n",
    "### Importing CNTK and other useful libraries\n",
    "\n",
    "Microsoft's Cognitive Toolkit comes in Python form as `cntk`, and contains many useful submodules for IO, defining layers, training models, and interrogating trained models. We will need many of these for Transfer Learning, as well as some other common libraries for downloading files, unpacking/unzipping them, working with the file system, and loading matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Some of the flowers data is stored as .mat files\n",
    "from scipy.io import loadmat\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "try: \n",
    "    from urllib.request import urlretrieve \n",
    "except ImportError: \n",
    "    from urllib import urlretrieve\n",
    "    \n",
    "import zipfile\n",
    "\n",
    "# Useful for being able to dump images into the Notebook\n",
    "import IPython.display as D\n",
    "\n",
    "# Import CNTK and helpers\n",
    "import cntk\n",
    "# Load and convert data\n",
    "from cntk.io import MinibatchSource, ImageDeserializer, StreamDefs, StreamDef\n",
    "import cntk.io.transforms as xforms\n",
    "from cntk import load_model, Trainer, UnitType\n",
    "from cntk.layers import Dense, Placeholder, Constant\n",
    "from cntk.learner import momentum_sgd, learning_rate_schedule, momentum_schedule\n",
    "from cntk.ops import input_variable, cross_entropy_with_softmax, classification_error, combine, softmax\n",
    "from cntk.ops.functions import CloneMethod\n",
    "from cntk.utils import log_number_of_parameters, ProgressPrinter\n",
    "# Interrogate the Compute Graph to find the right layer in the trained model\n",
    "from cntk.graph import find_by_name, get_node_outputs\n",
    "\n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "envvar = 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY'\n",
    "def is_test(): return envvar in os.environ\n",
    "\n",
    "# Select the right target device when this notebook is being tested\n",
    "# Currently supported only for GPU \n",
    "\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        raise ValueError('This notebook is currently not support on CPU') \n",
    "    else:\n",
    "        cntk.device.set_default_device(cntk.device.gpu(0))\n",
    "cntk.device.set_default_device(cntk.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are two run modes:\n",
    "- *Fast mode*: `isFast` is set to `True`. This is the default mode for the notebooks, which means we train for fewer iterations or train / test on limited data. This ensures functional correctness of the notebook though the models produced are far from what a completed training would produce.\n",
    "\n",
    "- *Slow mode*: We recommend the user to set this flag to `False` once the user has gained familiarity with the notebook content and wants to gain insight from running the notebooks for a longer period with different parameters for training. \n",
    "\n",
    "For *Fast mode* we train the model for 100 epochs and results have low accuracy but is good enough for development. The model yields good accuracy after 1000-2000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "isFast = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Downloading Data \n",
    "\n",
    "Now, let us download our datasets. We use two datasets in this tutorial - one containing _a bunch_ of flowers images, and the other containing _just a few_ sheep and wolves. They're described in more detail below, but what we are doing here is just downloading and unpacking them.\n",
    "\n",
    "Note that we are setting the data root to coincide with the CNTK examples, so if you have run those some of the data might already exist. Alter the data root if you would like all of the input and output data to go elsewhere (i.e. if you have copied this notebook to your own space). The `download_unless_exists` method will try to download several times, but if that fails you might see an exception. It and the `write_to_file` method both - write to files, so if the data_root is not writeable or fills up you'll see exceptions there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# By default, we store data in the Examples/Image directory under CNTK\n",
    "# If you're running this _outside_ of CNTK, consider changing this\n",
    "data_root = os.path.join('..', 'Examples', 'Image')\n",
    "\n",
    "if is_test():\n",
    "    #data_root = os.path.join(os.environment[envvar], 'Tutorials', 'data')\n",
    "    data_root = os.path.join('..', 'Examples', 'Image')\n",
    "    \n",
    "datasets_path = os.path.join(data_root, 'DataSets')\n",
    "# output_path = os.path.join(data_root, 'Output')\n",
    "output_path = os.path.join('.', 'temp', 'Output')\n",
    "\n",
    "def ensure_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def write_to_file(file_path, img_paths, img_labels):\n",
    "    with open(file_path, 'w+') as f:\n",
    "        for i in range(0, len(img_paths)):\n",
    "            f.write('%s\\t%s\\n' % (os.path.abspath(img_paths[i]), img_labels[i]))\n",
    "\n",
    "def download_unless_exists(url, filename, max_retries=3):\n",
    "    '''Download the file unless it already exists, with retry. Throws if all retries fail.'''\n",
    "    if os.path.exists(filename):\n",
    "        print('Reusing locally cached: ', filename)\n",
    "    else:\n",
    "        print('Starting download of {} to {}'.format(url, filename))\n",
    "        retry_cnt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                urlretrieve(url, filename)\n",
    "                print('Download completed.')\n",
    "                return\n",
    "            except:\n",
    "                retry_cnt += 1\n",
    "                if retry_cnt == max_retries:\n",
    "                    print('Exceeded maximum retry count, aborting.')\n",
    "                    raise\n",
    "                print('Failed to download, retrying.')\n",
    "                time.sleep(np.random.randint(1,10))\n",
    "        \n",
    "def download_model(model_root = os.path.join(data_root, 'PretrainedModels')):\n",
    "    ensure_exists(model_root)\n",
    "    resnet18_model_uri = 'https://www.cntk.ai/Models/ResNet/ResNet_18.model'\n",
    "    resnet18_model_local = os.path.join(model_root, 'ResNet_18.model')\n",
    "    download_unless_exists(resnet18_model_uri, resnet18_model_local)\n",
    "    return resnet18_model_local\n",
    "\n",
    "def download_flowers_dataset(dataset_root = os.path.join(datasets_path, 'Flowers')):\n",
    "    ensure_exists(dataset_root)\n",
    "    flowers_uris = [\n",
    "        'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz',\n",
    "        'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat',\n",
    "        'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat'\n",
    "    ]\n",
    "    flowers_files = [\n",
    "        os.path.join(dataset_root, '102flowers.tgz'),\n",
    "        os.path.join(dataset_root, 'imagelabels.mat'),\n",
    "        os.path.join(dataset_root, 'setid.mat')\n",
    "    ]\n",
    "    for uri, file in zip(flowers_uris, flowers_files):\n",
    "        download_unless_exists(uri, file)\n",
    "    tar_dir = os.path.join(dataset_root, 'extracted')\n",
    "    if not os.path.exists(tar_dir):\n",
    "        print('Extracting {} to {}'.format(flowers_files[0], tar_dir))\n",
    "        os.makedirs(tar_dir)\n",
    "        tarfile.open(flowers_files[0]).extractall(path=tar_dir)\n",
    "    else:\n",
    "        print('{} already extracted to {}, using existing version'.format(flowers_files[0], tar_dir))\n",
    "\n",
    "    flowers_data = {\n",
    "        'data_folder': dataset_root,\n",
    "        'training_map': os.path.join(dataset_root, '6k_img_map.txt'),\n",
    "        'testing_map': os.path.join(dataset_root, '1k_img_map.txt'),\n",
    "        'validation_map': os.path.join(dataset_root, 'val_map.txt')\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(flowers_data['training_map']):\n",
    "        print('Writing map files ...')\n",
    "        # get image paths and 0-based image labels\n",
    "        image_paths = np.array(sorted(glob.glob(os.path.join(tar_dir, 'jpg', '*.jpg'))))\n",
    "        image_labels = loadmat(flowers_files[1])['labels'][0]\n",
    "        image_labels -= 1\n",
    "\n",
    "        # read set information from .mat file\n",
    "        setid = loadmat(flowers_files[2])\n",
    "        idx_train = setid['trnid'][0] - 1\n",
    "        idx_test = setid['tstid'][0] - 1\n",
    "        idx_val = setid['valid'][0] - 1\n",
    "\n",
    "        # Confusingly the training set contains 1k images and the test set contains 6k images\n",
    "        # We swap them, because we want to train on more data\n",
    "        write_to_file(flowers_data['training_map'], image_paths[idx_train], image_labels[idx_train])\n",
    "        write_to_file(flowers_data['testing_map'], image_paths[idx_test], image_labels[idx_test])\n",
    "        write_to_file(flowers_data['validation_map'], image_paths[idx_val], image_labels[idx_val])\n",
    "        print('Map files written, dataset download and unpack completed.')\n",
    "    else:\n",
    "        print('Using cached map files.')\n",
    "        \n",
    "    return flowers_data\n",
    "    \n",
    "def download_animals_dataset(dataset_root = os.path.join(datasets_path, 'Animals')):\n",
    "    ensure_exists(dataset_root)\n",
    "    animals_uri = 'https://www.cntk.ai/DataSets/Animals/Animals.zip'\n",
    "    animals_file = os.path.join(dataset_root, 'Animals.zip')\n",
    "    download_unless_exists(animals_uri, animals_file)\n",
    "    if not os.path.exists(os.path.join(dataset_root, 'Test')):\n",
    "        with zipfile.ZipFile(animals_file) as animals_zip:\n",
    "            print('Extracting {} to {}'.format(animals_file, dataset_root))\n",
    "            animals_zip.extractall(path=os.path.join(dataset_root, '..'))\n",
    "            print('Extraction completed.')\n",
    "    else:\n",
    "        print('Using previously extracted Animals data.')\n",
    "        \n",
    "    return {\n",
    "        'training_folder': os.path.join(dataset_root, 'Train'),\n",
    "        'testing_folder': os.path.join(dataset_root, 'Test')\n",
    "    }\n",
    "\n",
    "print('Downloading flowers and animals data-set, this might take a while...')\n",
    "flowers_data = download_flowers_dataset()\n",
    "animals_data = download_animals_dataset()\n",
    "print('Downloading the data completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Trained Model Architecture\n",
    "\n",
    "For this task, we have chosen ResNet_18 as our trained model and  will it as the base model. This model will be adapted using Transfer Learning for classification of flowers and animals. This model is a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) built using [Residual Network](https://github.com/KaimingHe/deep-residual-networks) techniques. Convolutional Neural Networks build up layers of convolutions, transforming an input image and distilling it down until they start recognizing composite features, with deeper layers of convolutions recognizing complex patterns are made possible. The author of Keras has a [fantastic post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) where he describes how Convolutional Networks \"see the world\" which gives a much more detailed explanation.\n",
    "\n",
    "Residual Deep Learning is a technique that originated in Microsoft Research and involves \"passing through\" the main signal of the input data, so that the network winds up \"learning\" on just the residual portions that differ between layers. This has proven, in practice, to allow the training of much deeper networks by avoiding issues that plague gradient descent on larger networks. These cells bypass convolution layers and then come back in later before ReLU (see below), but some have argued that even deeper networks can be built by avoiding even more nonlinearities in the bypass channel. This is an area of hot research right now, and one of the most exciting parts of Transfer Learning is that you get to benefit from all of the improvements by just integrating new trained models.\n",
    "\n",
    "![A ResNet Block](https://adeshpande3.github.io/assets/ResNet.png)\n",
    "\n",
    "For visualizations of some of the deeper ResNet architectures, see [Kaiming He's GitHub](https://github.com/KaimingHe/deep-residual-networks) where he links off to visualizations of 50, 101, and 152-layer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Downloading pre-trained model. Note: this might take a while...')\n",
    "base_model_file = download_model()\n",
    "print('Downloading pre-trained model complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Interrogating the Model\n",
    "\n",
    "We print out all of the layers in ResNet_18 to show you how you can interrogate a model - to use a different model than ResNet_18 you would just need to discover the appropriate last hidden layer and feature layer to use. CNTK provides a convenient `get_node_outputs` method under `cntk.graph` to allow you to dump all of the model details. We can recognize the final hidden layer as the one before we start computing the final classification into the 1000 ImageNet classes (so in this case, `z.x`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define base model location and characteristics\n",
    "base_model = {\n",
    "    'model_file': base_model_file,\n",
    "    'feature_node_name': 'features',\n",
    "    'last_hidden_node_name': 'z.x',\n",
    "    # Channel Depth x Height x Width\n",
    "    'image_dims': (3, 224, 224)\n",
    "}\n",
    "\n",
    "# Print out all layers in the model\n",
    "print('Loading {} and printing all layers:'.format(base_model['model_file']))\n",
    "node_outputs = get_node_outputs(load_model(base_model['model_file']))\n",
    "for l in node_outputs: print(\"  {0} {1}\".format(l.name, l.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The Flowers Dataset\n",
    "\n",
    "The Flowers dataset comes from the Oxford Visual Geometry Group, and contains 102 different categories of flowers common to the UK. It has roughly 8000 images split between train, test, and validation sets. The [VGG homepage for the dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) contains more details.\n",
    "\n",
    "The data comes in the form of a huge [tarball](https://en.wikipedia.org/wiki/Tar_(computing)) of images, and two matrices in `.mat` format. These are 1-based matrices containing label IDs and the train/test/validation split. We convert them to 0-based labels, and write out the train, test, and validation index files in the format CNTK expects (see `write_to_file` above) of image/label pairs (tab-delimited, one per line).\n",
    "\n",
    "Let's take a look at some of the data we'll be working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flowers_image_dir = os.path.join(flowers_data['data_folder'], 'extracted', 'jpg')\n",
    "\n",
    "for image in ['08093', '08084', '08081', '08058']:\n",
    "    D.display(D.Image(os.path.join(flowers_image_dir, 'image_{}.jpg'.format(image)), width=100, height=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the Transfer Learning Model\n",
    "\n",
    "In the code below, we load up the pre-trained ResNet_18 model and clone it, while stripping off the final `features` layer. We clone the model so that we can re-use the same trained model multiple times, trained for different things - it is not strictly necessary if you are just training it for a single task, but this is why we would not use `CloneMethod.share`, we want to learn new parameters. If `freeze_weights` is true, we will freeze weights on all layers we clone and only learn weights on the final new features layer. This can often be useful if you are cloning higher up the tree (e.g., cloning after the first convolutional layer to just get basic image features).\n",
    "\n",
    "We find the final hidden layer (`z.x`) using `find_by_name`, clone it and all of its predecessors, then attach a new `Dense` layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ensure_exists(output_path)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Creates a minibatch source for training or testing\n",
    "def create_mb_source(map_file, image_dims, num_classes, randomize=True):\n",
    "    transforms = [xforms.scale(width=image_dims[2], height=image_dims[1], channels=image_dims[0], interpolations='linear')]\n",
    "    return MinibatchSource(ImageDeserializer(map_file, StreamDefs(\n",
    "            features =StreamDef(field='image', transforms=transforms),\n",
    "            labels   =StreamDef(field='label', shape=num_classes))),\n",
    "            randomize=randomize)\n",
    "\n",
    "# Creates the network model for transfer learning\n",
    "def create_model(model_details, num_classes, input_features, new_prediction_node_name='prediction', freeze=False):\n",
    "    # Load the pretrained classification net and find nodes\n",
    "    base_model   = load_model(model_details['model_file'])\n",
    "    feature_node = find_by_name(base_model, model_details['feature_node_name'])\n",
    "    last_node    = find_by_name(base_model, model_details['last_hidden_node_name'])\n",
    "\n",
    "    # Clone the desired layers with fixed weights\n",
    "    cloned_layers = combine([last_node.owner]).clone(\n",
    "        CloneMethod.freeze if freeze else CloneMethod.clone,\n",
    "        {feature_node: Placeholder(name='features')})\n",
    "\n",
    "    # Add new dense layer for class prediction\n",
    "    feat_norm  = input_features - Constant(114)\n",
    "    cloned_out = cloned_layers(feat_norm)\n",
    "    z          = Dense(num_classes, activation=None, name=new_prediction_node_name) (cloned_out)\n",
    "\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will now train the model just like any other CNTK model training - instantiating an input source (in this case a `MinibatchSource` from our image data), defining the loss function, and training for a number of epochs. Since we are training a multi-class classifier network, the final layer is a cross-entropy Softmax, and the error function is classification error - both conveniently provided by utility functions in `cntk.ops`.\n",
    "\n",
    "When training a pre-trained model, we are adapting the existing weights to suit our domain. Since the weights are likely already close to correct (especially for earlier layers that find more primitive features), fewer examples and fewer epochs are typically required to get good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Trains a transfer learning model\n",
    "def train_model(model_details, num_classes, train_map_file,\n",
    "                learning_params, max_images=-1):\n",
    "    num_epochs = learning_params['max_epochs']\n",
    "    epoch_size = sum(1 for line in open(train_map_file))\n",
    "    if max_images > 0:\n",
    "        epoch_size = min(epoch_size, max_images)\n",
    "    minibatch_size = learning_params['mb_size']\n",
    "    \n",
    "    # Create the minibatch source and input variables\n",
    "    minibatch_source = create_mb_source(train_map_file, model_details['image_dims'], num_classes)\n",
    "    image_input = input_variable(model_details['image_dims'])\n",
    "    label_input = input_variable(num_classes)\n",
    "\n",
    "    # Define mapping from reader streams to network inputs\n",
    "    input_map = {\n",
    "        image_input: minibatch_source['features'],\n",
    "        label_input: minibatch_source['labels']\n",
    "    }\n",
    "\n",
    "    # Instantiate the transfer learning model and loss function\n",
    "    tl_model = create_model(model_details, num_classes, image_input, freeze=learning_params['freeze_weights'])\n",
    "    ce = cross_entropy_with_softmax(tl_model, label_input)\n",
    "    pe = classification_error(tl_model, label_input)\n",
    "\n",
    "    # Instantiate the trainer object\n",
    "    lr_schedule = learning_rate_schedule(learning_params['lr_per_mb'], unit=UnitType.minibatch)\n",
    "    mm_schedule = momentum_schedule(learning_params['momentum_per_mb'])\n",
    "    learner = momentum_sgd(tl_model.parameters, lr_schedule, mm_schedule, \n",
    "                           l2_regularization_weight=learning_params['l2_reg_weight'])\n",
    "    trainer = Trainer(tl_model, (ce, pe), learner)\n",
    "\n",
    "    # Get minibatches of images and perform model training\n",
    "    print(\"Training transfer learning model for {0} epochs (epoch_size = {1}).\".format(num_epochs, epoch_size))\n",
    "    log_number_of_parameters(tl_model)\n",
    "    progress_printer = ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    for epoch in range(num_epochs):       # loop over epochs\n",
    "        sample_count = 0\n",
    "        while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "            data = minibatch_source.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map)\n",
    "            trainer.train_minibatch(data)                                    # update model with it\n",
    "            sample_count += trainer.previous_minibatch_sample_count          # count samples processed so far\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)  # log progress\n",
    "            if sample_count % (100 * minibatch_size) == 0:\n",
    "                print (\"Processed {0} samples\".format(sample_count))\n",
    "\n",
    "        trainer.summarize_training_progress()\n",
    "\n",
    "    return tl_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When we evaluate the trained model on an image, we have to massage that image into the expected format. In our case we use `Image` to load the image from its path, resize it to the size expected by our model, reverse the color channels (RGB to BGR), and convert to a contiguous array along height, width, and color channels. This corresponds to the 224x224x3 flattened array on which our model was trained.\n",
    "\n",
    "The model with which we are doing the evaluation has not had the Softmax and Error layers added, so is complete up to the final feature layer. To evaluate the image with the model, we send the input data to the `model.eval` method, `softmax` over the results to produce probabilities, and use Numpy's `argmax` method to determine the predicted class. We can then compare that against the true labels to get the overall model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluates a single image using the re-trained model\n",
    "def eval_single_image(loaded_model, image_path, image_dims):\n",
    "    # load and format image (resize, RGB -> BGR, CHW -> HWC)\n",
    "    img = Image.open(image_path)\n",
    "    if image_path.endswith(\"png\"):\n",
    "        temp = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "        temp.paste(img, img)\n",
    "        img = temp\n",
    "    resized = img.resize((image_dims[2], image_dims[1]), Image.ANTIALIAS)\n",
    "    bgr_image = np.asarray(resized, dtype=np.float32)[..., [2, 1, 0]]\n",
    "    hwc_format = np.ascontiguousarray(np.rollaxis(bgr_image, 2))\n",
    "\n",
    "    # compute model output\n",
    "    arguments = {loaded_model.arguments[0]: [hwc_format]}\n",
    "    output = loaded_model.eval(arguments)\n",
    "\n",
    "    # return softmax probabilities\n",
    "    sm = softmax(output[0, 0])\n",
    "    return sm.eval()\n",
    "\n",
    "\n",
    "# Evaluates an image set using the provided model\n",
    "def eval_test_images(loaded_model, output_file, test_map_file, image_dims, max_images=-1, column_offset=0):\n",
    "    num_images = sum(1 for line in open(test_map_file))\n",
    "    if max_images > 0:\n",
    "        num_images = min(num_images, max_images)\n",
    "    if isFast:\n",
    "        num_images = min(num_images, 300) #We will run through fewer images for test run\n",
    "        \n",
    "    print(\"Evaluating model output node '{0}' for {1} images.\".format('prediction', num_images))\n",
    "\n",
    "    pred_count = 0\n",
    "    correct_count = 0\n",
    "    np.seterr(over='raise')\n",
    "    with open(output_file, 'wb') as results_file:\n",
    "        with open(test_map_file, 'r') as input_file:\n",
    "            for line in input_file:\n",
    "                tokens = line.rstrip().split('\\t')\n",
    "                img_file = tokens[0 + column_offset]\n",
    "                probs = eval_single_image(loaded_model, img_file, image_dims)\n",
    "\n",
    "                pred_count += 1\n",
    "                true_label = int(tokens[1 + column_offset])\n",
    "                predicted_label = np.argmax(probs)\n",
    "                if predicted_label == true_label:\n",
    "                    correct_count += 1\n",
    "\n",
    "                np.savetxt(results_file, probs[np.newaxis], fmt='%.3f')\n",
    "                if pred_count % 100 == 0:\n",
    "                    print('Processed {0} samples ({1:.2%} correct)'.format(pred_count, (correct_count / float(pred_count))))\n",
    "                if pred_count >= num_images:\n",
    "                    break\n",
    "    print ('{0} of {1} prediction were correct'.format(correct_count, pred_count))\n",
    "    return correct_count, pred_count, (correct_count / float(pred_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, with all of these helper functions in place we can train the model and evaluate it on our flower dataset.\n",
    "\n",
    "Feel free to adjust the `learning_params` below and observe the results. You can tweak the `max_epochs` to train for longer, `mb_size` to adjust the size of each minibatch, or `lr_per_mb` to play with the speed of convergence (learning rate). \n",
    "\n",
    "**Note that if you've already trained the model, you will want to set `force_retraining` to `True` to force the Notebook to re-train your model with the new parameters.** \n",
    "\n",
    "You should see the model train and evaluate, with a final accuracy somewhere in the realm of 94%. At this point you could choose to train longer, or consider taking a look at the confusion matrix to determine if certain flowers are mis-predicted at a greater rate. You could also easily swap out to a different model and see if that performs better, or potentially learn from an earlier point in the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "force_retraining = True\n",
    "\n",
    "max_training_epochs = 3 if isFast else 20\n",
    "\n",
    "learning_params = {\n",
    "    'max_epochs': max_training_epochs,\n",
    "    'mb_size': 50,\n",
    "    'lr_per_mb': [0.2]*10 + [0.1],\n",
    "    'momentum_per_mb': 0.9,\n",
    "    'l2_reg_weight': 0.0005,\n",
    "    'freeze_weights': False\n",
    "}\n",
    "\n",
    "flowers_model = {\n",
    "    'model_file': os.path.join(output_path, 'FlowersTransferLearning.model'),\n",
    "    'results_file': os.path.join(output_path, 'FlowersPredictions.txt'),\n",
    "    'num_classes': 102\n",
    "}\n",
    "\n",
    "# Train only if no model exists yet or if force_retraining is set to True\n",
    "if os.path.exists(flowers_model['model_file']) and not force_retraining:\n",
    "    print(\"Loading existing model from %s\" % flowers_model['model_file'])\n",
    "    trained_model = load_model(flowers_model['model_file'])\n",
    "else:\n",
    "    trained_model = train_model(base_model,\n",
    "                                flowers_model['num_classes'], flowers_data['training_map'],\n",
    "                                learning_params)\n",
    "    trained_model.save(flowers_model['model_file'])\n",
    "    print(\"Stored trained model at %s\" % flowers_model['model_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the test set\n",
    "predict_correct, predict_total, predict_accuracy = \\\n",
    "   eval_test_images(trained_model, flowers_model['results_file'], flowers_data['testing_map'], base_model['image_dims'])\n",
    "print(\"Done. Wrote output to %s\" % flowers_model['results_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test: Accuracy on flower data\n",
    "print (\"Prediction accuracy: {0:.2%}\".format(predict_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Animals Dataset\n",
    "\n",
    "With the Flowers dataset, we had hundreds of classes with hundreds of images. What if we had a smaller set of classes and images to work with, would transfer learning still work? Let us examine the Animals dataset we have downloaded, consisting of nothing but sheep and wolves and a much smaller set of images to work with (on the order of a dozen per class). Let us take a look at a few..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sheep = ['738519_d0394de9.jpg', 'Pair_of_Icelandic_Sheep.jpg']\n",
    "wolves = ['European_grey_wolf_in_Prague_zoo.jpg', 'Wolf_je1-3.jpg']\n",
    "for image in [os.path.join('Sheep', f) for f in sheep] + [os.path.join('Wolf', f) for f in wolves]:\n",
    "    D.display(D.Image(os.path.join(animals_data['training_folder'], image), width=100, height=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The images are stored in `Train` and `Test` folders with the nested folder giving the class name (i.e. `Sheep` and `Wolf` folders). This is quite common, so it is useful to know how to convert that format into one that can be used for constructing the mapping files CNTK expects. `create_class_mapping_from_folder` looks at all nested folders in the root and turns their names into labels, and returns this as an array used by `create_map_file_from_folder`. That method walks those folders and writes their paths and label indices into a `map.txt` file in the root (e.g. `Train`, `Test`). Note the use of `abspath`, allowing you to specify relative \"root\" paths to the method, and then move the resulting map files or run from different directories without issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_map_file_from_folder(root_folder, class_mapping, include_unknown=False, valid_extensions=['.jpg', '.jpeg', '.png']):\n",
    "    map_file_name = os.path.join(root_folder, \"map.txt\")\n",
    "    with open(map_file_name , 'w') as map_file:\n",
    "        for class_id in range(0, len(class_mapping)):\n",
    "            folder = os.path.join(root_folder, class_mapping[class_id])\n",
    "            if os.path.exists(folder):\n",
    "                for entry in os.listdir(folder):\n",
    "                    filename = os.path.abspath(os.path.join(folder, entry))\n",
    "                    if os.path.isfile(filename) and os.path.splitext(filename)[1].lower() in valid_extensions:\n",
    "                        map_file.write(\"{0}\\t{1}\\n\".format(filename, class_id))\n",
    "\n",
    "        if include_unknown:\n",
    "            for entry in os.listdir(root_folder):\n",
    "                filename = os.path.abspath(os.path.join(root_folder, entry))\n",
    "                if os.path.isfile(filename) and os.path.splitext(filename)[1].lower() in valid_extensions:\n",
    "                    map_file.write(\"{0}\\t-1\\n\".format(filename))\n",
    "\n",
    "    return map_file_name\n",
    "\n",
    "\n",
    "def create_class_mapping_from_folder(root_folder):\n",
    "    classes = []\n",
    "    for _, directories, _ in os.walk(root_folder):\n",
    "        for directory in directories:\n",
    "            classes.append(directory)\n",
    "    return np.asarray(classes)\n",
    "\n",
    "animals_data['class_mapping'] = create_class_mapping_from_folder(animals_data['training_folder'])\n",
    "animals_data['training_map'] = create_map_file_from_folder(animals_data['training_folder'], animals_data['class_mapping'])\n",
    "# Since the test data includes some birds, set include_unknown\n",
    "animals_data['testing_map'] = create_map_file_from_folder(animals_data['testing_folder'], animals_data['class_mapping'], \n",
    "                                                          include_unknown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can now train our model on our small domain and evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "animals_model = {\n",
    "    'model_file': os.path.join(output_path, 'AnimalsTransferLearning.model'),\n",
    "    'results_file': os.path.join(output_path, 'AnimalsPredictions.txt'),\n",
    "    'num_classes': len(animals_data['class_mapping'])\n",
    "}\n",
    "\n",
    "if os.path.exists(animals_model['model_file']) and not force_retraining:\n",
    "    print(\"Loading existing model from %s\" % animals_model['model_file'])\n",
    "    trained_model = load_model(animals_model['model_file'])\n",
    "else:\n",
    "    trained_model = train_model(base_model, \n",
    "                                animals_model['num_classes'], animals_data['training_map'],\n",
    "                                learning_params)\n",
    "    trained_model.save(animals_model['model_file'])\n",
    "    print(\"Stored trained model at %s\" % animals_model['model_file'])\n",
    "\n",
    "# evaluate test images\n",
    "with open(animals_data['testing_map'], 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        tokens = line.rstrip().split('\\t')\n",
    "        img_file = tokens[0]\n",
    "        true_label = int(tokens[1])\n",
    "        probs = eval_single_image(trained_model, img_file, base_model['image_dims'])\n",
    "\n",
    "        class_probs = np.column_stack((probs, animals_data['class_mapping'])).tolist()\n",
    "        class_probs.sort(key=lambda x: float(x[0]), reverse=True)\n",
    "        predictions = ' '.join(\n",
    "            ['%s:%.3f' % (class_probs[i][1], float(class_probs[i][0])) for i in range(0, animals_model['num_classes'])])\n",
    "        true_class_name = animals_data['class_mapping'][true_label] if true_label >= 0 else 'unknown'\n",
    "        print('Class: %s, predictions: %s, image: %s' % (true_class_name, predictions, img_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The Known Unknown\n",
    "\n",
    "Note the `include_unknown=True` in the `test_map_file` creation. This is because we have a few unlabeled images in that directory - these get tagged with label `-1`, which will never be matched by the evaluator. This is just to show that if you train a classifier to only find sheep and wolves, it will always find sheep and wolves. Showing it pictures of birds like our unknown examples will only result in confusion, as you can see above where the images of birds are falsely predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "images = ['Bird_in_flight_wings_spread.jpg', 'quetzal-bird.jpg', 'Weaver_bird.jpg']\n",
    "for image in images:\n",
    "    D.display(D.Image(os.path.join(animals_data['testing_folder'], image), width=100, height=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Final Thoughts, and Caveats\n",
    "\n",
    "Transfer Learning has limitations. If you noticed, we re-trained a model that had been trained on ImageNet images. This meant it already _knew_ what \"images\" were, and had a good idea on concepts from low-level (stripes, circles) to high-level (dog's noses, cat's ears). Re-training such a model to detect sheep or wolves makes sense, but re-training it to detect vehicles from aerial imagery would be more difficult. You can still use Transfer Learning in these cases, but you might want to just re-use earlier layers of the model (i.e. the early Convolutional layers that have learned more primitive concepts), and you will likely require much more training data.\n",
    "\n",
    "Adding a catch-all category can be a good idea, but only if the training data for that category contains images that are again sufficiently similar to the images you expect at scoring time. As in the above example, if we train a classifier with images of sheep and wolf and use it to score an image of a bird, the classifier can still only assign a sheep or wolf label, since it does not know any other categories. If we were to add a catch-all category and add training images of birds to it then the classifier might predict the class correctly for the bird image. However, if we present it, e.g., an image of a car, it faces the same problem as before as it knows only sheep, wolf and bird (which we just happened to call called catch-all). Hence, your training data, even for your catch-all, needs to cover sufficiently those concepts and images that you expect later on at scoring time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# CNTK 102:  Feed Forward Network with Simulated Data\n",
    "\n",
    "The purpose of this tutorial is to familiarize you with quickly combining components from the CNTK python library to perform a **classification** task. You may skip *Introduction* section, if you have already completed the Logistic Regression tutorial or are familiar with machine learning. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Problem** (recap from the CNTK 101):\n",
    "\n",
    "A cancer hospital has provided data and wants us to determine if a patient has a fatal [malignant][] cancer vs. a benign growth. This is known as a classification problem. To help classify each patient, we are given their age and the size of the tumor. Intuitively, one can imagine that younger patients and/or patient with small tumor size are less likely to have malignant cancer. The data set simulates this application where the each observation is a patient represented as a dot where red color indicates malignant and blue indicates a benign disease. Note: This is a toy example for learning, in real life there are large number of features from different tests/examination sources and doctors'  experience that play into the diagnosis/treatment decision for a patient.\n",
    "\n",
    "<img src=\"https://www.cntk.ai/jup/cancer_data_plot.jpg\", width=400, height=400>\n",
    "\n",
    "\n",
    "**Goal**:\n",
    "Our goal is to learn a classifier that classifies any patient into either benign or malignant category given two features (age, tumor size). \n",
    "\n",
    "In CNTK 101 tutorial, we learnt a linear classifier using Logistic Regression which misclassified some data points. Often in real world problems, linear classifiers cannot accurately model the data in situations where there is little to no knowledge of how to construct good features. This often results in accuracy limitations and requires models that have more complex decision boundaries. In this tutorial, we will combine multiple linear units (from the CNTK 101 tutorial - Logistic Regression) to a non-linear classifier. The other aspect of such classifiers where the feature encoders are automatically learnt from the data will be covered in later tutorials.  \n",
    "\n",
    "**Approach**:\n",
    "Any learning algorithm has typically five stages. These are Data reading, Data preprocessing, Creating a model, Learning the model parameters, and Evaluating (a.k.a. testing/prediction) the model. \n",
    "\n",
    "We keep everything same as CNTK 101 except for the third (Model creation) step where we use a feed forward network instead.\n",
    " \n",
    "\n",
    "## Feed forward network model\n",
    "\n",
    "The data set used is similar to the one used in the Logistic Regression tutorial. The model combines multiple logistic classifiers to be able to classify data when the decision boundary needed to properly categorize the data is more complex than a simple linear model (like Logistic Regression). The figure below illustrates the general shape of the network.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif\",width=200, height=200> \n",
    "\n",
    "A feedforward neural network is an artificial neural network where connections between the units **do not** form a cycle.\n",
    "The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network\n",
    "\n",
    "In this tutorial, we will go through the different steps needed to complete the five steps for training and testing a model on the toy data.\n",
    "\n",
    "[malignant]: https://en.wikipedia.org/wiki/Malignancy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "# Import the relevant components\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import cntk as C\n",
    "from cntk import Trainer, learning_rate_schedule, UnitType\n",
    "from cntk.blocks import default_options, Input\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk.layers import Dense\n",
    "from cntk.learner import sgd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    import cntk\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        cntk.device.set_default_device(cntk.device.cpu())\n",
    "    else:\n",
    "        cntk.device.set_default_device(cntk.device.gpu(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "This section can be *skipped* (next section titled <a href='#Model Creation'>Model Creation</a>) if you have gone through CNTK 101. \n",
    "\n",
    "Let us generate some synthetic data emulating the cancer example using `numpy` library. We have two features (represented in two-dimensions)  each either being to one of the two classes (benign:blue dot or malignant:red dot). \n",
    "\n",
    "In our example, each observation in the training data has a label (blue or red) corresponding to each observation (set of features - age and size). In this example, we have two classes represented by labels 0 or 1, thus a  binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the data dimensions\n",
    "input_dim = 2\n",
    "num_output_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Labels\n",
    "\n",
    "In this tutorial we are generating synthetic data using `numpy` library. In real world problems, one would use a reader, that would read feature values (`features`: *age* and *tumor size*) corresponding to each observation (patient).  Note, each observation can reside in a higher dimension space (when more features are available) and will be represented as a tensor in CNTK. More advanced tutorials shall introduce the handling of high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to generate a random data sample\n",
    "def generate_random_data_sample(sample_size, feature_dim, num_classes):\n",
    "    # Create synthetic data using NumPy. \n",
    "    Y = np.random.randint(size=(sample_size, 1), low=0, high=num_classes)\n",
    "\n",
    "    # Make sure that the data is separable\n",
    "    X = (np.random.randn(sample_size, feature_dim)+3) * (Y+1)\n",
    "    X = X.astype(np.float32)    \n",
    "    # converting class 0 into the vector \"1 0 0\", \n",
    "    # class 1 into vector \"0 1 0\", ...\n",
    "    class_ind = [Y==class_number for class_number in range(num_classes)]\n",
    "    Y = np.asarray(np.hstack(class_ind), dtype=np.float32)\n",
    "    return X, Y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the input variables denoting the features and the label data. Note: the input_variable \n",
    "# does not need additional info on number of observations (Samples) since CNTK first create only \n",
    "# the network tooplogy first \n",
    "mysamplesize = 64\n",
    "features, labels = generate_random_data_sample(mysamplesize, input_dim, num_output_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the input data. \n",
    "\n",
    "**Caution**: If the import of `matplotlib.pyplot` fails, please run `conda install matplotlib` which will fix the `pyplot` version dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# given this is a 2 class \n",
    "colors = ['r' if l == 0 else 'b' for l in labels[:,0]]\n",
    "\n",
    "plt.scatter(features[:,0], features[:,1], c=colors)\n",
    "plt.xlabel(\"Scaled age (in yrs)\")\n",
    "plt.ylabel(\"Tumor size (in cm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Model Creation'></a>\n",
    "## Model Creation\n",
    "\n",
    "Our feed forward network will be relatively simple with 2 hidden layers (`num_hidden_layers`) with each layer having 50 hidden nodes (`hidden_layers_dim`). \n",
    "\n",
    "<img src=\"http://cntk.ai/jup/feedforward_network.jpg\",width=200, height=200>\n",
    "\n",
    "\n",
    "The number of green nodes (refer to picture above) in each hidden layer is set to 50 in the example and the number of hidden layers (refer to the number of layers of green nodes) is 2. Fill in the following values:\n",
    "- num_hidden_layers\n",
    "- hidden_layers_dim\n",
    "\n",
    "Note: In this illustration, we have not shown the bias node (introduced in the logistic regression tutorial). Each hidden layer would have a bias node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden_layers = 2\n",
    "hidden_layers_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network input and output: \n",
    "- **input** variable (a key CNTK concept): \n",
    ">An **input** variable is a container in which we fill different observations (data point or sample, equivalent to a blue/red dot in our example) during model learning (a.k.a.training) and model evaluation (a.k.a. testing). Thus, the shape of the `input_variable` must match the shape of the data that will be provided.  For example, when data are images each of  height 10 pixels  and width 5 pixels, the input feature dimension will be two (representing image height and width). Similarly, in our examples the dimensions are age and tumor size, thus `input_dim` = 2). More on data and their dimensions to appear in separate tutorials.\n",
    "\n",
    "\n",
    "**Question** What is the input dimension of your chosen model? This is fundamental to our understanding of variables in a network or model representation in CNTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The input variable (representing 1 observation, in our example of age and size) $\\bf{x}$ which \n",
    "# in this case has a dimension of 2. \n",
    "#\n",
    "# The label variable has a dimensionality equal to the number of output classes in our case 2. \n",
    "# We use the Input block object equivalent to input_variable.\n",
    "\n",
    "input = Input(input_dim)\n",
    "label = Input(num_output_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network setup\n",
    "Let us define the feedforward network one step at a time. The first layer takes an input feature vector ($\\bf{x}$) with dimensions (`input_dim`) say $m$) and emits the output a.k.a. *evidence* (first hidden layer $\\bf{z_1}$ with dimension (`hidden_layer_dim`) say $n$). Each feature in the input layer is connected with a node in the output later by the weight which is represented by a matrix $\\bf{W}$ with dimensions ($m \\times n$). The first step is to compute the evidence for the entire feature set. Note: we use **bold** notations to denote matrix / vectors: \n",
    "\n",
    "$$\\bf{z_1} = \\bf{W} \\cdot \\bf{x} + \\bf{b}$$ \n",
    "\n",
    "where $\\bf{b}$ is a bias vector of dimension $n$. \n",
    "\n",
    "In the `linear_layer` function, we perform two operations:\n",
    "0. multiply the weights ($\\bf{W}$) with the features ($\\bf{x}$) and add individual features' contribution,\n",
    "1. add the bias term $\\bf{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_layer(input_var, output_dim):\n",
    "\n",
    "    input_dim = input_var.shape[0]\n",
    "    times_param = C.ops.parameter(shape=(input_dim, output_dim))\n",
    "    bias_param = C.ops.parameter(shape=(output_dim))\n",
    "\n",
    "    t = C.ops.times(input_var, times_param)\n",
    "    return bias_param + t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the *evidence* (the output of the linear layer) through a non-linear function a.k.a. *activation functions* of your choice that would squash the evidence to activations using a choice of functions ([found here][]). **Sigmoid** or **Tanh** are historically popular. We will use **sigmoid** function in this tutorial. The output of the sigmoid function often is the input to the next layer or the output of the final layer. \n",
    "[found here]: https://github.com/Microsoft/CNTK/wiki/Activation-Functions\n",
    "\n",
    "**Question**: Try different activation functions by passing different them to `nonlinearity` value and get familiarized with using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_layer(input, output_dim, nonlinearity):\n",
    "    r = linear_layer(input, output_dim)\n",
    "    r = nonlinearity(r)\n",
    "    return r;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created one hidden layer, we need to iterate through the layers to create a fully connected classifier. Output of the first layer $\\bf{h_1}$ becomes the input to the next layer.\n",
    "\n",
    "In this example we have only 2 layers, hence one could conceivably write the code as:\n",
    "\n",
    ">`h1 = fully_connected_layer(input, hidden_layer_dim, sigmoid)`\n",
    "\n",
    ">`h2 = fully_connected_layer(h1, hidden_layer_dim, sigmoid)`\n",
    "\n",
    "However, this code becomes very quickly difficult to read and update when the number of layers or blocks (in convolutional or recurrent networks) that we will see in later tutorials. CNTK provides a programming construct shown below that greatly eases the burden on the programmer. \n",
    "\n",
    ">`h = fully_connected_layer(input, hidden_layer_dim, sigmoid)`\n",
    "\n",
    ">`for i in range(1, num_hidden_layers):`\n",
    "       \n",
    ">>`    h = fully_connected_layer(h, hidden_layer_dim, sigmoid)`\n",
    "\n",
    "This construct is very attractive to write compact representation of large repetitive network components and will be used in many of the subsequent tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a multilayer feedforward classification model\n",
    "def fully_connected_classifier_net(input, num_output_classes, hidden_layer_dim, \n",
    "                                   num_hidden_layers, nonlinearity):\n",
    "    \n",
    "    h = dense_layer(input, hidden_layer_dim, nonlinearity)\n",
    "    for i in range(1, num_hidden_layers):\n",
    "        h = dense_layer(h, hidden_layer_dim, nonlinearity)\n",
    "    r = linear_layer(h, num_output_classes)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network output: `z` will be used to represent the output of a network across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the fully connected classfier\n",
    "z = fully_connected_classifier_net(input, num_output_classes, hidden_layers_dim, \n",
    "                                   num_hidden_layers, C.ops.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned network helps us better understand how to implement a network using CNTK primitives. We would encourage CNTK users to leverage the convenience of using the [layers library](https://www.cntk.ai/pythondocs/layerref.html). This library provides predefined commonly used “layers,” (lego like blocks)  which makes it very easy to write simple networks that consist of standard layers layered on top of each other. We will use [`Dense`](https://www.cntk.ai/pythondocs/layerref.html#dense) layer function to compose our deep model. We can pass the input variable (`Input`) to this model to get the network output. \n",
    "\n",
    "**Suggested task**: Please go through the model defined above and the output of the `create_model` function and convince that the implmentation below encapsulates the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with default_options(init = glorot_uniform(), activation = C.ops.sigmoid):\n",
    "            h = features\n",
    "            for _ in range(num_hidden_layers):\n",
    "                h = Dense(hidden_layers_dim)(h)\n",
    "            r = Dense(num_output_classes, activation = None)(h)\n",
    "            return r\n",
    "        \n",
    "z = create_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning model parameters\n",
    "\n",
    "Now that the network is setup, we would like to learn the parameters $\\bf w$ and $\\bf b$ for each of the layers in our network. To do so we convert, the computed evidence ($\\bf z_{final~layer}$) into a set of predicted probabilities ($\\textbf p$) using a `softmax` function.\n",
    "\n",
    "$$ \\textbf{p} = \\mathrm{softmax}(\\bf{z_{final~layer}})$$ \n",
    "\n",
    "One can see the `softmax` function as an activation function that maps the accumulated evidences to a probability distribution over the classes (Details of the [softmax function][]). Other choices of activation function can be [found here][].\n",
    "\n",
    "[softmax function]: https://www.cntk.ai/pythondocs/cntk.ops.html#cntk.ops.softmax\n",
    "\n",
    "[found here]: https://github.com/Microsoft/CNTK/wiki/Activation-Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "If you have already gone through CNTK101, please skip this section and jump to the section titled,\n",
    "<a href='#Run the trainer'>Run the trainer'</a>.\n",
    "\n",
    "The output of the `softmax` is a probability of observations belonging to the respective classes. For training the classifier, we need to determine what behavior the model needs to mimic. In other words, we want the generated probabilities to be as close as possible to the observed labels. This function is called the *cost* or *loss* function and shows what is the difference between the learnt model vs. that generated by the training set.\n",
    "\n",
    "$$ H(p) = - \\sum_{j=1}^C y_j \\log (p_j) $$  \n",
    "\n",
    "where $p$ is our predicted probability from `softmax` function and $y$ represents the label. This label provided with the data for training is also called the ground-truth label. In the two-class example, the `label` variable has dimensions of two (equal to the `num_output_classes` or $C$). Generally speaking, if the task in hand requires classification into $C$ different classes, the label variable will have $C$ elements with 0 everywhere except for the class represented by the data point where it will be 1.  Understanding the [details][] of this cross-entropy function is highly recommended.\n",
    "\n",
    "[`cross-entropy`]: http://cntk.ai/pythondocs/cntk.ops.html#cntk.ops.cross_entropy_with_softmax\n",
    "[details]: http://colah.github.io/posts/2015-09-Visual-Information/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = C.ops.cross_entropy_with_softmax(z, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "In order to evaluate the classification, one can compare the output of the network which for each observation emits a vector of evidences (can be converted into probabilities using `softmax` functions) with dimension equal to number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_error = C.ops.classification_error(z, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training\n",
    "\n",
    "The trainer strives to reduce the `loss` function by different optimization approaches, [Stochastic Gradient Descent][] (`sgd`) being one of the most popular one. Typically, one would start with random initialization of the model parameters. The `sgd` optimizer would calculate the `loss` or error between the predicted label against the corresponding ground-truth label and using [gradient-decent][] generate a new set model parameters in a single iteration. \n",
    "\n",
    "The aforementioned model parameter update using a single observation at a time is attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations. An intermediate ground is to load a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\n",
    "\n",
    "With minibatches we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\n",
    "\n",
    "One of the key parameter for optimization is called the `learning_rate`. For now, we can think of it as a scaling factor that modulates how much we change the parameters in any iteration. We will be covering more details in later tutorial. \n",
    "With this information, we are ready to create our trainer.\n",
    "\n",
    "[optimization]: https://en.wikipedia.org/wiki/Category:Convex_optimization\n",
    "[Stochastic Gradient Descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "[gradient-decent]: http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-with-Gradient-Descent.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the trainer object to drive the model training\n",
    "learning_rate = 0.5\n",
    "lr_schedule = learning_rate_schedule(learning_rate, UnitType.minibatch) \n",
    "learner = C.learner.sgd(z.parameters, lr_schedule)\n",
    "trainer = Trainer(z, loss, eval_error, [learner])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets create some helper functions that will be needed to visualize different functions associated with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cntk.utils import get_train_eval_criterion, get_train_loss\n",
    "\n",
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=10):\n",
    "    \n",
    "    if len(a) < w: \n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    \n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = get_train_loss(trainer)\n",
    "        eval_error = get_train_eval_criterion(trainer)\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {}, Train Loss: {}, Train Error: {}\".format(mb, training_loss, eval_error))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Run the trainer'></a>\n",
    "### Run the trainer\n",
    "\n",
    "We are now ready to train our fully connected neural net. We want to decide what data we need to feed into the training engine.\n",
    "\n",
    "In this example, each iteration of the optimizer will work on 25 samples (25 dots w.r.t. the plot above) a.k.a. `minibatch_size`. We would like to train on say 20000 observations. Note: In real world case, we would be given a certain amount of labeled data (in the context of this example, observation (age, size) and what they mean (benign / malignant)). We would use a large number of observations for training say 70% and set aside the remainder for evaluation of the trained model.\n",
    "\n",
    "With these parameters we can proceed with training our simple feed forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters for the trainer\n",
    "minibatch_size = 25\n",
    "num_samples = 20000\n",
    "num_minibatches_to_train = num_samples / minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the trainer and perform model training\n",
    "training_progress_output_freq = 20\n",
    "\n",
    "plotdata = {\"batchsize\":[], \"loss\":[], \"error\":[]}\n",
    "\n",
    "for i in range(0, int(num_minibatches_to_train)):\n",
    "    features, labels = generate_random_data_sample(minibatch_size, input_dim, num_output_classes)\n",
    "    \n",
    "    # Specify the input variables mapping in the model to actual minibatch data for training\n",
    "    trainer.train_minibatch({input : features, label : labels})\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, \n",
    "                                                     training_progress_output_freq, verbose=0)\n",
    "    \n",
    "    if not (loss == \"NA\" or error ==\"NA\"):\n",
    "        plotdata[\"batchsize\"].append(batchsize)\n",
    "        plotdata[\"loss\"].append(loss)\n",
    "        plotdata[\"error\"].append(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the errors over the different training minibatches. Note that as we iterate the training loss decreases though we do see some intermediate bumps. The bumps indicate that during that iteration the model came across observations that it predicted incorrectly. This can happen with observations that are novel during model training.\n",
    "\n",
    "One way to smoothen the bumps is by increasing the minibatch size. One could conceptually use the entire data set in every iteration. This would ensure the loss keeps consistently decreasing over iterations. However, this approach  requires the gradient computations over all data points in the dataset and repeat those after locally updating the model parameters for a large number of iterations. For this toy example it is not a big deal. However with real world example, making multiple passes over the entire data set for each iteration of parameter update becomes computationally prohibitive. \n",
    "\n",
    "Hence, we use smaller minibatches and using `sgd` enables us to have a great scalability while being performant for large data sets. There are advanced variants of the optimizer unique to CNTK that enable harnessing computational efficiency for real world data sets and will be introduced in advanced tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the moving average loss to smooth out the noise in SGD\n",
    "\n",
    "plotdata[\"avgloss\"] = moving_average(plotdata[\"loss\"])\n",
    "plotdata[\"avgerror\"] = moving_average(plotdata[\"error\"])\n",
    "\n",
    "# Plot the training loss and the training error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"avgloss\"], 'b--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Minibatch run vs. Training loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"avgerror\"], 'r--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Label Prediction Error')\n",
    "plt.title('Minibatch run vs. Label Prediction Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation / Testing \n",
    "\n",
    "Now that we have trained the network, let us evaluate the trained network on data that hasn't been used for training. This is often called **testing**. Let us create some new data set and evaluate the average error and loss on this set. This is done using `trainer.test_minibatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate new data\n",
    "test_minibatch_size = 25\n",
    "features, labels = generate_random_data_sample(test_minibatch_size, input_dim, num_output_classes)\n",
    "\n",
    "trainer.test_minibatch({input : features, label : labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this error is very comparable to our training error indicating that our model has good \"out of sample\" error a.k.a. generalization error. This implies that our model can very effectively deal with previously unseen observations (during the training process). This is key to avoid the phenomenon of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so far been dealing with aggregate measures of error. Lets now get the probabilities associated with individual data points. For each observation, the `eval` function returns the probability distribution across all the classes. If you used the default parameters in this tutorial, then it would be a vector of 2 elements per observation. First let us route the network output through a softmax function.\n",
    "\n",
    "#### Why do we need to route the network output `netout` via `softmax`?\n",
    "\n",
    "<img src=\"http://cntk.ai/jup/feedforward_network.jpg\",width=200, height=200>\n",
    "\n",
    "The way we have configured the network includes the output of all the activation nodes (e.g., the green layer in the figure). The output nodes (the orange layer in the figure), converts the activations into a probability. A simple and effective way is to route the activations via a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = C.ops.softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test on previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_label_probs =out.eval({input : features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Label    :\", [np.argmax(label) for label in labels])\n",
    "print(\"Predicted:\", [np.argmax(row) for row in predicted_label_probs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exploration Suggestion**\n",
    "-  Try exploring how the classifier behaves with different data distributions - suggest changing the `minibatch_size` parameter from 25 to say 64. What happens to the error rate? How does the error compare to the logistic regression classifier?\n",
    "- Try exploring different optimizers such as Adam (`fsadagrad`). \n",
    "    learner = fsadagrad(z.parameters(), 0.02, 0, targetAdagradAvDenom=1)\n",
    "- Can you change the network to reduce the training error rate? When do you see *overfitting* happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Code link\n",
    "\n",
    "If you want to try running the tutorial from python command prompt. Please run the [FeedForwardNet.py][] example.\n",
    "\n",
    "[FeedForwardNet.py]: https://github.com/Microsoft/CNTK/blob/v2.0.beta8.0/Tutorials/NumpyInterop/FeedForwardNet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk-py34]",
   "language": "python",
   "name": "conda-env-cntk-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

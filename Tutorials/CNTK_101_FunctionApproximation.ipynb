{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 101: Function Approximation Primer\n",
    "This tutorial is targeted to individuals who are new to CNTK and to machine learning. In this tutorial, you will train a learning model that can be used to approximate continuious functions. See the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem). Simple models with a few neurons can approximate a wide variety of equations and Function Approximation is a powerful machine learning tool.\n",
    "\n",
    "This notebook is for CNTK Python APIs. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Problem**:\n",
    "A business has a large amount of data they wish to use in a predictive fashion. You can create a function approximation to make predictions on new data based on examples from the past. This toy example uses very simple functions for the model to approximate. It's important to understand how to get a trivial model to train well before attempting to predict more complicated models. The first model has three features (y1, y2, y3) and one label (x) where x = y (y2 and y3 are irrellevant). The model should be able to approximate x = y. The second model is slightly more complex x = 100y1+n +10y2+n +y3+n. The model should be able to approximate this easily as well. \n",
    "\n",
    "\n",
    "**Goal**:\n",
    "Our goal is to learn to create a model than can approximate simple functions. We will also show you to to create and read CNTK formatted data files and how to standardize data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cntk.ai/jup/feedforward_network.jpg\" width=\"200\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 1\n",
    "Image(url= \"https://cntk.ai/jup/feedforward_network.jpg\", width=200, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows a simple feed forward network with two layers and two outputs. Our network will be even simpler with one layer and one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the relevant components\n",
    "import numpy as np\n",
    "import os\n",
    "from cntk.device import cpu, try_set_default_device\n",
    "from cntk import Trainer\n",
    "from cntk.learners import sgd, learning_rate_schedule, UnitType, momentum_sgd, momentum_schedule\n",
    "from cntk.ops import input, sigmoid, tanh, floor, clip, relu, softplus, log\n",
    "from cntk.losses import squared_error\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.io import CTFDeserializer, MinibatchSource, StreamDef, StreamDefs\n",
    "from cntk.io import INFINITELY_REPEAT\n",
    "from cntk.layers import Dense, Sequential\n",
    "\n",
    "# Seed the random number generator to ensure the same dataset each time for consistency \n",
    "np.random.seed(98019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Labels\n",
    "\n",
    "In this tutorial we are generating synthetic data and writing it out to a CNTK formatted data file before reading it back in with CNTK tools in order to demonstrate how the data files are constructed and read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createStandardizedData(path, num_records):\n",
    "    \"\"\"\n",
    "    Create a very simple dataset for testing regression models in the CNTK format\n",
    "    Models should be able to converge quickly\n",
    "    x = y1 + 0 + 0\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as outfile:    \n",
    "        for i in range(num_records):\n",
    "            r = np.random.randint(size=(1, 1), low=0, high=999)[0][0]\n",
    "            label = (r - 500) / 290 # Standardization, pre-calculated mean and standard deviation\n",
    "            feature = [label, 0, 0]\n",
    "            feature = [str(i) for i in feature]\n",
    "            outfile.write(\"|labels {} |features {}\\n\".format(label, \" \".join(feature)))\n",
    "\n",
    "\n",
    "def createStandardizedData2(path, num_records):\n",
    "    \"\"\"\n",
    "    Create a simple equation for testing regression models\n",
    "    x = (y1-n) * 100 + (y2 -n) * 10 + (y3 - n)\n",
    "    \n",
    "    write out raw features and labels as well as a standardized set.\n",
    "    Shows how models converge quickly with standardized data v.s. raw data.\n",
    "    \"\"\"    \n",
    "    # pre-calculated. Otherwise get them from a sample of the data.\n",
    "    feature_mean = 50\n",
    "    feature_std_dist = 10\n",
    "    label_mean = 500\n",
    "    label_std_dist = 290\n",
    "    with open(path, \"w\") as outfile:    \n",
    "        for i in range(num_records):\n",
    "            r = np.random.randint(size=(1, 1), low=0, high=999)[0][0]\n",
    "            label = r\n",
    "            standardized_label = standardize(r,label_mean, label_std_dist)\n",
    "            feature = [ord(c) for c in str(r)]\n",
    "            while(len(feature) < 3):\n",
    "                feature.insert(0, 0)\n",
    "            standardized_feature = [str(i) for i in standardize(feature, feature_mean, feature_std_dist)]\n",
    "            feature = [str(i) for i in feature]\n",
    "            outfile.write(\"|labels {} |features {} |rawlabels {} |rawfeatures {}\\n\".format(standardized_label, \" \".join(standardized_feature), label, \" \".join(feature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "In order for the models to converge the variables should all have a similar range. This is a key point and if overlooked it will result in models that don't seem to be able to learn no matter what hyperparameters are tried. There are various ways to  standardize the ranges. One simple and effective method is to subtract the mean from each data point to \"zero center\" the data and then divide by the standard deviation to scale the data. See also [Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(a, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    0 center and scale data\n",
    "    Standardize an np.array to the array mean and standard deviation or specified parameters\n",
    "    See https://en.wikipedia.org/wiki/Feature_scaling\n",
    "    \"\"\"\n",
    "    if mean == None:\n",
    "        mean = np.mean(a)\n",
    "    \n",
    "    if std == None:\n",
    "        std = np.std(a)\n",
    "    a = np.array(a, np.float32)\n",
    "    n = (a - mean) / std\n",
    "    return n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CNTK data\n",
    "\n",
    "CNTK has built in methods for reading data in the specified format. Here create_reader reads the \"labels\" and \"features\" tags, which were already standardized. create_reader_raw swaps in the raw data to run as a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    \"\"\"\n",
    "    reads CNTK formatted file with 'labels' and 'features'\n",
    "    \"\"\"    \n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        labels = StreamDef(field='labels', shape=num_label_classes),\n",
    "        features   = StreamDef(field='features', shape=input_dim)\n",
    "    )), randomize = is_training, max_sweeps = INFINITELY_REPEAT if is_training else 1)   \n",
    "    \n",
    "    \n",
    "def create_reader_raw(path, is_training, input_dim, num_label_classes):\n",
    "    \"\"\"\n",
    "    Reads in the unstardized values.\n",
    "    \"\"\"\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        labels = StreamDef(field='rawlabels', shape=num_label_classes),\n",
    "        features   = StreamDef(field='rawfeatures', shape=input_dim)\n",
    "    )), randomize = is_training, max_sweeps = INFINITELY_REPEAT if is_training else 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "CNTK has an expressive and high level (layers library)[https://github.com/Microsoft/CNTK/wiki/Layers-Library-Reference]. Our simple model has a single hidden layer and one output layer. The hidden layer uses the tanh activation, sigmoid is also a good choice for this type of model. The second layer doesn't specify and activation function and will therefore output it's raw value. We can't use activations like softmax here, because they work on categories. You can try using softmax and see what it does to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim, output_dim, hidden_dim, feature_input):    \n",
    "    \"\"\"\n",
    "    Create a model with the layers library.\n",
    "    \"\"\"\n",
    "    my_model = Sequential ([\n",
    "            Dense(hidden_dim, tanh),\n",
    "            Dense(output_dim)\n",
    "            ])\n",
    "\n",
    "    netout = my_model(feature)   \n",
    "    return(netout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the important functions have been defined we can create and test the model. First the training data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file_path = r'regression_example_data.txt'\n",
    "    \n",
    "createStandardizedData(data_file_path, num_records =  100000) # a very simple equation\n",
    "#createStandardizedData2(data_file_path, num_records = 100000) # a slightly complex equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    " \n",
    "These parameters control the overall size and shape of the model. Our model will have one hidden layer with ten neurons. The inputs and outputs are fixed for this data. The learning rate and minibatch size can be adjusted to experiment. In order for some more complicated models to work these parameters must be experimented with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "output_dim = 1\n",
    "hidden_dim = 10\n",
    "learning_rate = 0.001\n",
    "minibatch_size = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set the shape of the input and output. We also create the model, reader and map which input to use in the model to the streams in the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "Input and output shapes\n",
    "\"\"\"\n",
    "feature = input((input_dim), np.float32)\n",
    "label = input((output_dim), np.float32)\n",
    "\n",
    "\"\"\"\n",
    "Create model, reader and map\n",
    "\"\"\"\n",
    "netout = create_model(input_dim, output_dim, hidden_dim, feature)\n",
    "training_reader = create_reader(data_file_path, True, input_dim, output_dim)\n",
    "input_map = {\n",
    "label  : training_reader.streams.labels,\n",
    "feature  : training_reader.streams.features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to set the loss function. The evaluation function is optional and is used to show how well the model is predicting the data. For function approximation squared_error is a great choice. We will also set the learning rate per mini-batch. CNTK creates a schedule to optimize learning as the minibatches are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = squared_error(netout, label)    \n",
    "evaluation = squared_error(netout, label)\n",
    "lr_per_minibatch=learning_rate_schedule(learning_rate, UnitType.minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning functions\n",
    "\n",
    "CNTK has a variety of built in (learning functions)[https://www.cntk.ai/pythondocs/cntk.learners.html]. sgd is Stochastic Gradient Descent which will iteratively attempt to tune the model to match the input to the output. We pass that and the other functions to the trainer. In order to see how the model is doing we will need a progress printer that reports every n iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = sgd(netout.parameters, lr=lr_per_minibatch)    \n",
    "\n",
    "# Other learners to try\n",
    "#learner = momentum_sgd(netout.parameters, lr=lr_per_minibatch, momentum = momentum_schedule(0.9))\n",
    "#learner = adagrad(netout.parameters, lr=lr_per_minibatch) \n",
    "progress_printer = ProgressPrinter(minibatch_size)\n",
    "trainer = Trainer(netout, (loss, evaluation), learner, progress_printer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the minibatches\n",
    "\n",
    "Now we use CNTK to run minibatches of data through the learner. As it loops through the iterations the progress_printer will show how it's doing. We also capture the loss for later examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[   1- 120]: loss = 1.765789 * 14400, metric = 176.58% * 14400;\n",
      " Minibatch[ 121- 240]: loss = 0.556902 * 14400, metric = 55.69% * 14400;\n",
      " Minibatch[ 241- 360]: loss = 0.181210 * 14400, metric = 18.12% * 14400;\n",
      " Minibatch[ 361- 480]: loss = 0.064498 * 14400, metric = 6.45% * 14400;\n",
      " Minibatch[ 481- 600]: loss = 0.028165 * 14400, metric = 2.82% * 14400;\n",
      " Minibatch[ 601- 720]: loss = 0.017087 * 14400, metric = 1.71% * 14400;\n",
      " Minibatch[ 721- 840]: loss = 0.013209 * 14400, metric = 1.32% * 14400;\n",
      " Minibatch[ 841- 960]: loss = 0.012035 * 14400, metric = 1.20% * 14400;\n",
      " Minibatch[ 961-1080]: loss = 0.011610 * 14400, metric = 1.16% * 14400;\n",
      " Minibatch[1081-1200]: loss = 0.011386 * 14400, metric = 1.14% * 14400;\n",
      " Minibatch[1201-1320]: loss = 0.011287 * 14400, metric = 1.13% * 14400;\n",
      " Minibatch[1321-1440]: loss = 0.011075 * 14400, metric = 1.11% * 14400;\n",
      " Minibatch[1441-1560]: loss = 0.011016 * 14400, metric = 1.10% * 14400;\n",
      " Minibatch[1561-1680]: loss = 0.010858 * 14400, metric = 1.09% * 14400;\n",
      " Minibatch[1681-1800]: loss = 0.010741 * 14400, metric = 1.07% * 14400;\n",
      " Minibatch[1801-1920]: loss = 0.010630 * 14400, metric = 1.06% * 14400;\n",
      " Minibatch[1921-2040]: loss = 0.010493 * 14400, metric = 1.05% * 14400;\n",
      " Minibatch[2041-2160]: loss = 0.010443 * 14400, metric = 1.04% * 14400;\n",
      " Minibatch[2161-2280]: loss = 0.010327 * 14400, metric = 1.03% * 14400;\n",
      " Minibatch[2281-2400]: loss = 0.010208 * 14400, metric = 1.02% * 14400;\n",
      " Minibatch[2401-2520]: loss = 0.009922 * 14400, metric = 0.99% * 14400;\n",
      " Minibatch[2521-2640]: loss = 0.010084 * 14400, metric = 1.01% * 14400;\n",
      " Minibatch[2641-2760]: loss = 0.009731 * 14400, metric = 0.97% * 14400;\n",
      " Minibatch[2761-2880]: loss = 0.009910 * 14400, metric = 0.99% * 14400;\n",
      " Minibatch[2881-3000]: loss = 0.009611 * 14400, metric = 0.96% * 14400;\n",
      " Minibatch[3001-3120]: loss = 0.009527 * 14400, metric = 0.95% * 14400;\n",
      " Minibatch[3121-3240]: loss = 0.009508 * 14400, metric = 0.95% * 14400;\n",
      " Minibatch[3241-3360]: loss = 0.009277 * 14400, metric = 0.93% * 14400;\n",
      " Minibatch[3361-3480]: loss = 0.009334 * 14400, metric = 0.93% * 14400;\n",
      " Minibatch[3481-3600]: loss = 0.009229 * 14400, metric = 0.92% * 14400;\n",
      " Minibatch[3601-3720]: loss = 0.009031 * 14400, metric = 0.90% * 14400;\n",
      " Minibatch[3721-3840]: loss = 0.008914 * 14400, metric = 0.89% * 14400;\n",
      " Minibatch[3841-3960]: loss = 0.008942 * 14400, metric = 0.89% * 14400;\n",
      " Minibatch[3961-4080]: loss = 0.008868 * 14400, metric = 0.89% * 14400;\n",
      " Minibatch[4081-4200]: loss = 0.008705 * 14400, metric = 0.87% * 14400;\n",
      " Minibatch[4201-4320]: loss = 0.008558 * 14400, metric = 0.86% * 14400;\n",
      " Minibatch[4321-4440]: loss = 0.008656 * 14400, metric = 0.87% * 14400;\n",
      " Minibatch[4441-4560]: loss = 0.008402 * 14400, metric = 0.84% * 14400;\n",
      " Minibatch[4561-4680]: loss = 0.008478 * 14400, metric = 0.85% * 14400;\n",
      " Minibatch[4681-4800]: loss = 0.008407 * 14400, metric = 0.84% * 14400;\n",
      " Minibatch[4801-4920]: loss = 0.008232 * 14400, metric = 0.82% * 14400;\n",
      " Minibatch[4921-5040]: loss = 0.008181 * 14400, metric = 0.82% * 14400;\n",
      " Minibatch[5041-5160]: loss = 0.008084 * 14400, metric = 0.81% * 14400;\n",
      " Minibatch[5161-5280]: loss = 0.008148 * 14400, metric = 0.81% * 14400;\n",
      " Minibatch[5281-5400]: loss = 0.007949 * 14400, metric = 0.79% * 14400;\n",
      " Minibatch[5401-5520]: loss = 0.007733 * 14400, metric = 0.77% * 14400;\n",
      " Minibatch[5521-5640]: loss = 0.007870 * 14400, metric = 0.79% * 14400;\n",
      " Minibatch[5641-5760]: loss = 0.007636 * 14400, metric = 0.76% * 14400;\n",
      " Minibatch[5761-5880]: loss = 0.007746 * 14400, metric = 0.77% * 14400;\n",
      " Minibatch[5881-6000]: loss = 0.007539 * 14400, metric = 0.75% * 14400;\n",
      " Minibatch[6001-6120]: loss = 0.007513 * 14400, metric = 0.75% * 14400;\n",
      " Minibatch[6121-6240]: loss = 0.007635 * 14400, metric = 0.76% * 14400;\n",
      " Minibatch[6241-6360]: loss = 0.007301 * 14400, metric = 0.73% * 14400;\n",
      " Minibatch[6361-6480]: loss = 0.007429 * 14400, metric = 0.74% * 14400;\n",
      " Minibatch[6481-6600]: loss = 0.007189 * 14400, metric = 0.72% * 14400;\n",
      " Minibatch[6601-6720]: loss = 0.007214 * 14400, metric = 0.72% * 14400;\n",
      " Minibatch[6721-6840]: loss = 0.007193 * 14400, metric = 0.72% * 14400;\n",
      " Minibatch[6841-6960]: loss = 0.007087 * 14400, metric = 0.71% * 14400;\n",
      " Minibatch[6961-7080]: loss = 0.007150 * 14400, metric = 0.71% * 14400;\n",
      " Minibatch[7081-7200]: loss = 0.006953 * 14400, metric = 0.70% * 14400;\n",
      " Minibatch[7201-7320]: loss = 0.006833 * 14400, metric = 0.68% * 14400;\n",
      " Minibatch[7321-7440]: loss = 0.006851 * 14400, metric = 0.69% * 14400;\n",
      " Minibatch[7441-7560]: loss = 0.006841 * 14400, metric = 0.68% * 14400;\n",
      " Minibatch[7561-7680]: loss = 0.006684 * 14400, metric = 0.67% * 14400;\n",
      " Minibatch[7681-7800]: loss = 0.006562 * 14400, metric = 0.66% * 14400;\n",
      " Minibatch[7801-7920]: loss = 0.006650 * 14400, metric = 0.66% * 14400;\n",
      " Minibatch[7921-8040]: loss = 0.006618 * 14400, metric = 0.66% * 14400;\n",
      " Minibatch[8041-8160]: loss = 0.006614 * 14400, metric = 0.66% * 14400;\n",
      " Minibatch[8161-8280]: loss = 0.006558 * 14400, metric = 0.66% * 14400;\n",
      " Minibatch[8281-8400]: loss = 0.006402 * 14400, metric = 0.64% * 14400;\n",
      " Minibatch[8401-8520]: loss = 0.006500 * 14400, metric = 0.65% * 14400;\n",
      " Minibatch[8521-8640]: loss = 0.006343 * 14400, metric = 0.63% * 14400;\n",
      " Minibatch[8641-8760]: loss = 0.006316 * 14400, metric = 0.63% * 14400;\n",
      " Minibatch[8761-8880]: loss = 0.006236 * 14400, metric = 0.62% * 14400;\n",
      " Minibatch[8881-9000]: loss = 0.006101 * 14400, metric = 0.61% * 14400;\n",
      " Minibatch[9001-9120]: loss = 0.006048 * 14400, metric = 0.60% * 14400;\n",
      " Minibatch[9121-9240]: loss = 0.006094 * 14400, metric = 0.61% * 14400;\n",
      " Minibatch[9241-9360]: loss = 0.006067 * 14400, metric = 0.61% * 14400;\n",
      " Minibatch[9361-9480]: loss = 0.006066 * 14400, metric = 0.61% * 14400;\n",
      " Minibatch[9481-9600]: loss = 0.006006 * 14400, metric = 0.60% * 14400;\n",
      " Minibatch[9601-9720]: loss = 0.005883 * 14400, metric = 0.59% * 14400;\n",
      " Minibatch[9721-9840]: loss = 0.005817 * 14400, metric = 0.58% * 14400;\n",
      " Minibatch[9841-9960]: loss = 0.005890 * 14400, metric = 0.59% * 14400;\n"
     ]
    }
   ],
   "source": [
    "plotdata = {\"loss\":[]}\n",
    "for i in range(10000):\n",
    "    data = training_reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    \"\"\"\n",
    "    # This is how to get the Numpy typed data from the reader\n",
    "    ldata = data[label].asarray()\n",
    "    fdata = data[feature].asarray()\n",
    "    \"\"\"\n",
    "    trainer.train_minibatch(data)\n",
    "    loss = trainer.previous_minibatch_loss_average\n",
    "    if not (loss == \"NA\"):\n",
    "        plotdata[\"loss\"].append(loss)       \n",
    "    if np.abs(trainer.previous_minibatch_loss_average) < 0.0015: #stop once the model is good.\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the result\n",
    "\n",
    "Now we take a look at the training summary and try out some \"hold-out\" data on the model to see how well it performs. We also take a peek at ten rows to see what the model came up with. _Note that in order to use the data you will have to reverse the standardization done on it previously._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[1 of 300]: loss = 0.039021 * 1200000, metric = 3.90% * 1200000 22.222s (54000.5 samples/s);\n",
      "Error rate on an unseen minibatch 0.005606\n",
      "data 0.937931, 0.0, 0.0,\tevaluation [ 1.01246774],\texpected [ 0.93793106]\n",
      "data 0.603448, 0.0, 0.0,\tevaluation [ 0.68987054],\texpected [ 0.60344827]\n",
      "data 0.0275862, 0.0, 0.0,\tevaluation [ 0.03323707],\texpected [ 0.02758621]\n",
      "data -0.882759, 0.0, 0.0,\tevaluation [-0.96400577],\texpected [-0.88275862]\n",
      "data 1.63793, 0.0, 0.0,\tevaluation [ 1.48890424],\texpected [ 1.63793099]\n",
      "data 1.08621, 0.0, 0.0,\tevaluation [ 1.13622749],\texpected [ 1.08620691]\n",
      "data 1.55862, 0.0, 0.0,\tevaluation [ 1.44824409],\texpected [ 1.55862069]\n",
      "data 0.67931, 0.0, 0.0,\tevaluation [ 0.76795447],\texpected [ 0.67931032]\n",
      "data 0.793103, 0.0, 0.0,\tevaluation [ 0.87984943],\texpected [ 0.79310346]\n",
      "data 0.851724, 0.0, 0.0,\tevaluation [ 0.93489009],\texpected [ 0.85172415]\n"
     ]
    }
   ],
   "source": [
    "trainer.summarize_training_progress()\n",
    "test_data = training_reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "avg_error = trainer.test_minibatch(test_data)\n",
    "print(\"Error rate on an unseen minibatch %f\" % avg_error)\n",
    "\n",
    "ntldata = data[label].asarray()\n",
    "ntfdata = data[feature].asarray()\n",
    "for i in range(10):            \n",
    "        print(\"data {},\\tevaluation {},\\texpected {}\".format(\n",
    "                \", \".join(str(v) for v in ntfdata[i][0]),\n",
    "                netout.eval({feature: ntfdata[i]})[0],\n",
    "                ntldata[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Figure 2\n",
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the loss\n",
    "\n",
    "Visualizing the loss graph is a good way to see if your model is performing correctly. The above figure shows a typical loss progression for models that converge properly.\n",
    "Here we will smooth our loss data and examine a graph to see how well the model is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pyplot and define a moving average function\n",
    "import matplotlib.pyplot as plt\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAACfCAYAAADXno+tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd482be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the data\n",
    "plotdata[\"avgloss\"] = moving_average(plotdata[\"loss\"], 100)\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(plotdata[\"avgloss\"])\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Minibatch run vs. Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions\n",
    "\n",
    "  - Go back and run the model with the both data types and compare the loss rate.\n",
    "  - Try the different learners and compare the loss rate.\n",
    "  - Use create_reader_raw to pass in non-standardized data to the model and compare the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

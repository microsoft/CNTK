{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 206:  Part B - Time series prediction with LSTM (IOT Data)\n",
    "\n",
    "In  [part A of this tutorial](CNTK_106A_LSTM_Timeseries_with_Simulated_Data.ipynb) we developed a simple LSTM network to predict future values in a time series. In part B we want to use the model on some real world internet-of-things ([IOT](https://en.wikipedia.org/wiki/Internet_of_things)) data. As an example we want to predict the daily output of a solar panel base on the initial readings of a the day. \n",
    "\n",
    "[Solar power forecasting](https://en.wikipedia.org/wiki/Solar_power_forecasting) is a challenging and important problem. The solar energy generation forecasting problem is closely linked to the problem of weather variables forecasting. Indeed, this problem is usually split into two parts, on one hand focusing on the forecasting of solar PV or any other meteorological variable and on the other hand estimating the amount of energy that a concrete power plant will produce with the estimated meteorological resource. In general, the way to deal with this difficult problem is usually related to the spatial and temporal scales we are interested in. This tutorial focusses on a simplified forecasting model using previously generated data from solar panel to predict the future. \n",
    "\n",
    "**Goal**\n",
    "\n",
    "We will be using the LSTM based time series prediction model developed in part A to predict the daily output of a solar panel based on the initial readings of a the day. \n",
    "\n",
    "![rooftop-solar](https://www.cntk.ai/jup/rooftop-solar-power.jpg)\n",
    "\n",
    "We train the model with historical data of the solar panel. In our example we want to predict the total power production of the solar panel array for the day starting with the initial readings of the day. We start predicting after the first 2 readings and adjust the prediction with each new reading.\n",
    "\n",
    "In this tutorial, we will use the LSTM model introduced in the CNTK 106A. This tutorial has the following sub-sections:\n",
    "- Setup\n",
    "- Data generation\n",
    "- LSTM network modeling\n",
    "- Model training and evaluation\n",
    "\n",
    "For more details on how LSTMs work, see [this excellent post](http://colah.github.io/posts/2015-08-Understanding-LSTMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup\n",
    "We need a few imports and constants throughout the tutorial that we define here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "import cntk as C\n",
    "import cntk.axis\n",
    "from cntk.blocks import Input\n",
    "from cntk.layers import Dense, Dropout, Recurrence \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there are 14 lstm cells, 1 for each possible reading we get per day\n",
    "TIMESTEPS = 14\n",
    "\n",
    "# 20000 is the maximum total output in our dataset. We normalize all values with \n",
    "# this so our inputs are between 0.0 and 1.0 range.\n",
    "NORMALIZE = 20000 \n",
    "\n",
    "# process batches of 10 days\n",
    "BATCH_SIZE = TIMESTEPS * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make things reproduceable, seed random\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the notebook runtime environment devices / settings\n",
    "\n",
    "Set the device to cpu / gpu for the test environment. If you have both CPU and GPU on your machine, you can optionally switch the devices. By default we choose the best available device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two run modes:\n",
    "- *Fast mode*: `isFast` is set to `True`. This is the default mode for the notebooks, which means we train for fewer iterations or train / test on limited data. This ensures functional correctness of the notebook though the models produced are far from what a completed training would produce.\n",
    "\n",
    "- *Slow mode*: We recommend the user to set this flag to `False` once the user has gained familiarity with the notebook content and wants to gain insight from running the notebooks for a longer period with different parameters for training. \n",
    "\n",
    "For *Fast mode* we train the model for 100 epochs and results have low accuracy but is good enough for development. <br/>\n",
    "The model yields good accuracy after 1000-2000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isFast = True\n",
    "\n",
    "# we need around 2000 epochs to see good accuracy. For testing 100 epochs will do.\n",
    "EPOCHS = 100 if isFast else 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "The model we build in our example is going to do the following:\n",
    "\n",
    "Starting with the initial readings of the day, we want to predict the total power production of the solar panel array for the day. We start predicting after the first 2 readings and adjust the prediction with each new reading.\n",
    "\n",
    "The training data we are going to use comes as a csv file and has the following format:\n",
    "```\n",
    "time,solar.current,solar.total\n",
    "7am,6.3,1.7\n",
    "7:30am,44.3,11.4\n",
    "...\n",
    "\n",
    "solar.current is the current production in Watt\n",
    "solar.total is the total produced for the day so far in Watt/hour\n",
    "1 reading is taken every 30 minutes\n",
    "```\n",
    "\n",
    "The training dataset we use contains data captured for 3 years and can be found [here](https://guschmueds.blob.core.windows.net/datasets/solar.csv). \n",
    "The dataset is not pre-processed: it is raw data and contains smaller gabs and errors (like a panel failed to report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code in this example is related to data preparation. Thankfully the pandas library make this easy.\n",
    "\n",
    "generate_solar_data() reads raw data into a pandas dataframe, normalizes the data, groups by day and appends the columns \"solar.current.max\" and \"solar.total.max\" for each day.\n",
    "It than generates the sequences for each day as we describe below. All sequences are concatenated into a single list of sequences. There is no more notion of timestamp in our train input and only the sequences matter.\n",
    "\n",
    "The raw data is sorted by time and we should randomize it before splitting into training, validation and test datasets but this  would make it unpractical to visualize results in the tutorial.\n",
    "For this tutorial we split the dataset the following way: pick in sequence 8 values for training, 1 for validation and 1 for test until there is no more data. This will spread training, validation and test datasets across the full timeline but still preserves time order.\n",
    "\n",
    "next_batch() yields the next batch for training. We use variable size sequences supported by CNTK and batches are a list of numpy arrays where the numpy arrays have variable length. \n",
    "\n",
    "A standard practice is to shuffle batches with each epoch. We don't do this here because we want to be able to graph the data easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_solar_data(input_url, time_steps, normalize=1, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    generate sequences to feed to rnn based on data frame with solar panel data\n",
    "    the csv has has the format: time ,solar.current, solar.total\n",
    "     (solar.current is the current output in Watt, solar.total is the total production\n",
    "      for the day so far in Watt hours)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_url, index_col=\"time\", parse_dates=['time'])\n",
    "    df[\"date\"] = df.index.date\n",
    "    \n",
    "    # normalize data\n",
    "    df['solar.current'] /= normalize\n",
    "    df['solar.total'] /= normalize\n",
    "    \n",
    "    # group by day, find the max for a day and add a new column .max\n",
    "    grouped = df.groupby(df.index.date).max()\n",
    "    grouped.columns = [\"solar.current.max\", \"solar.total.max\", \"date\"]\n",
    "\n",
    "    # merge continuous readings and daily max values into a single frame\n",
    "    df_merged = pd.merge(df, grouped, right_index=True, on=\"date\")\n",
    "    df_merged = df_merged[[\"solar.current\", \"solar.total\", \"solar.current.max\", \"solar.total.max\"]]\n",
    "    # we group by day so we can process a day at a time.\n",
    "    grouped = df_merged.groupby(df_merged.index.date)\n",
    "    per_day = []\n",
    "    for _, group in grouped:\n",
    "        per_day.append(group)\n",
    "\n",
    "    # split the dataset into train, validatation and test sets on day boundaries\n",
    "    val_size = int(len(per_day) * val_size)\n",
    "    test_size = int(len(per_day) * test_size)\n",
    "    next_val = 0\n",
    "    next_test = 0\n",
    "\n",
    "    result_x = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    result_y = {\"train\": [], \"val\": [], \"test\": []}    \n",
    "\n",
    "    # generate sequences a day at a time\n",
    "    for i, day in enumerate(per_day):\n",
    "        # if we have less than 8 datapoints for a day we skip over the\n",
    "        # day assuming something is missing in the raw data\n",
    "        total = day[\"solar.total\"].values\n",
    "        if len(total) < 8:\n",
    "            continue\n",
    "        if i >= next_val:\n",
    "            current_set = \"val\"\n",
    "            next_val = i + int(len(per_day) / val_size)\n",
    "        elif i >= next_test:\n",
    "            current_set = \"test\"\n",
    "            next_test = i + int(len(per_day) / test_size)\n",
    "        else:\n",
    "            current_set = \"train\"\n",
    "        max_total_for_day = np.array(day[\"solar.total.max\"].values[0])\n",
    "        for j in range(2, len(total)):\n",
    "            result_x[current_set].append(total[0:j])\n",
    "            result_y[current_set].append([max_total_for_day])\n",
    "            if j >= time_steps:\n",
    "                break\n",
    "    # make result_y a numpy array\n",
    "    for ds in [\"train\", \"val\", \"test\"]:\n",
    "        result_y[ds] = np.array(result_y[ds])\n",
    "    return result_x, result_y\n",
    "\n",
    "\n",
    "def next_batch(x, y, ds):\n",
    "    \"\"\"get the next batch for training\"\"\"\n",
    "\n",
    "    def as_batch(data, start, count):\n",
    "        return data[start:start + count]\n",
    "\n",
    "    for i in range(0, len(x[ds]), BATCH_SIZE):\n",
    "        yield as_batch(X[ds], i, BATCH_SIZE), as_batch(Y[ds], i, BATCH_SIZE)\n",
    "        \n",
    "\n",
    "X, Y = generate_solar_data(\"https://guschmueds.blob.core.windows.net/datasets/solar.csv\",\n",
    "                           TIMESTEPS, normalize=NORMALIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the sequence we are going to feed to the LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X['train'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y['train'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM network setup\n",
    "Our LSTM model has the following design:\n",
    "![lstm](https://guschmueds.blob.core.windows.net/datasets/2.png)\n",
    "\n",
    "We use 14 LSTM cell, 1 cell for each data point we take during the day.\n",
    "With a reading taken every 30 minutes we get around 14 data points a day. We truncate if we receive more than 14 readings.\n",
    "CNTK does support variable sequences as input to a LSTM so we can feed our sequences as they are with no padding needed.\n",
    "\n",
    "The output of the neural network is the total output for the day and each sequence for a given day has the same total output.\n",
    "\n",
    "For example:\n",
    "```\n",
    "1.7,11.4 -> 10300\n",
    "1.7,11.4,67.5 -> 10300\n",
    "1.7,11.4,67.5,250.5 ... -> 10300\n",
    "1.7,11.4,67.5,250.5,573.5 -> 10300\n",
    "```\n",
    "\n",
    "The outputs from the LSTMs are feed into a dense layer and we randomly dropout 20% of the values to not overfit the model to the training set. The output of the dense layer becomes the prediction our model generates.\n",
    "\n",
    "You might have noticed that we lost the timestamp altogether; in our model only the sequences of readings matters. \n",
    "\n",
    "Defining the model in CNTK is done with a few lines of python code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x):\n",
    "    \"\"\"Create the model for time series prediction\"\"\"\n",
    "    with C.layers.default_options(initial_state = 0.1):\n",
    "        m = C.layers.Recurrence(C.layers.LSTM(TIMESTEPS))(x)\n",
    "        m = C.ops.sequence.last(m)\n",
    "        m = C.layers.Dropout(0.2)(m)\n",
    "        m = cntk.layers.Dense(1)(m)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Before we can start training we need to bind our input variables for the model and define what optimizer we want to use. \n",
    "For this example we choose the adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input sequences\n",
    "x = C.blocks.Input(1)\n",
    "\n",
    "# create the model\n",
    "z = create_model(x)\n",
    "\n",
    "# expected output (label), also the dynamic axes of the model output\n",
    "# is specified as the model of the label input\n",
    "l = C.blocks.Input(1, dynamic_axes=z.dynamic_axes, name=\"y\")\n",
    "\n",
    "# the learning rate\n",
    "learning_rate = 0.005\n",
    "lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "\n",
    "# loss function\n",
    "loss = C.ops.squared_error(z, l)\n",
    "\n",
    "# use squared error to determine error for now\n",
    "error = C.ops.squared_error(z, l)\n",
    "\n",
    "# use adam optimizer\n",
    "momentum_time_constant = C.learner.momentum_as_time_constant_schedule(BATCH_SIZE / -math.log(0.9)) \n",
    "learner = C.learner.adam_sgd(z.parameters, \n",
    "                             lr = lr_schedule, \n",
    "                             momentum = momentum_time_constant)\n",
    "trainer = C.Trainer(z, loss, error, [learner])\n",
    "\n",
    "loss_summary = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to start trainig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "start = time.time()\n",
    "for epoch in range(0, EPOCHS):\n",
    "    for x_batch, l_batch in next_batch(X, Y, \"train\"):\n",
    "        trainer.train_minibatch({x: x_batch, l: l_batch})\n",
    "        \n",
    "    if epoch % (EPOCHS / 10) == 0:\n",
    "        training_loss = C.utils.get_train_loss(trainer)\n",
    "        loss_summary.append(training_loss)\n",
    "        print(\"epoch: {}, loss: {:.4f}\".format(epoch, training_loss))\n",
    "\n",
    "print(\"training took {} sec\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look how the loss function shows how the model is converging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_summary, label='training loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the training validation and test dataset. We use mean squared error as measure which might be a little simplistic. A method that would define a ratio how many predictions have been inside a given tolerance would make a better measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate\n",
    "def get_mse(X,Y,labeltxt):\n",
    "    result = 0.0\n",
    "    for x1, y1 in next_batch(X, Y, labeltxt):\n",
    "        eval_error = trainer.test_minibatch({x : x1, l : y1})\n",
    "        result += eval_error\n",
    "    return result/len(X[labeltxt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the train and validation errors\n",
    "for labeltxt in [\"train\", \"val\", \"test\"]:\n",
    "    print(\"mse for {}: {:.6f}\".format(labeltxt, get_mse(X, Y, labeltxt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. <br/>\n",
    "The nice thing with time series data is that we can visualize the results. <br/>\n",
    "Let's take our newly created model, make predictions and plot them against the actual readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "f, a = plt.subplots(2, 1, figsize=(12, 8))\n",
    "for j, ds in enumerate([\"val\", \"test\"]):\n",
    "    results = []\n",
    "    for x_batch, _ in next_batch(X, Y, ds):\n",
    "        pred = z.eval({x: x_batch})\n",
    "        results.extend(pred[:, 0])\n",
    "    # because we normalized the input data we need to multiply the prediction\n",
    "    # with SCALER to get the real values.\n",
    "    a[j].plot((Y[ds] * NORMALIZE).flatten(), label=ds + ' raw');\n",
    "    a[j].plot(np.array(results) * NORMALIZE, label=ds + ' pred');\n",
    "    a[j].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are close to the actual data and definitely follow the right pattern. If we let the model train for 2000 epochs, the accuracy of the results on validation and test datasets is reasonable good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So what we do with this model? A practical application would be to generate alerts if the actual output is not in line with the prediction, for example if one of the panels is failing. The solar array that goes with our dataset has 16 panels. If we'd want to detect failure without generating false alerts, the accuracy of our prediction would need to be at least 1 - 1/16, around 94%. Our model is close to this but would most likely generate occasional false alerts.\n",
    "\n",
    "To improve the model we could let it train for more epochs, cleanup the training set or use a more sophisticated. The biggest improvement on accuracy would most likely comes from more, higher resolution training data, for example reading a data point every 5 minutes instead of every 30 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We hope this tutorial gets you started on time series prediction with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk-py34]",
   "language": "python",
   "name": "conda-env-cntk-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

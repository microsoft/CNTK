{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# CNTK 103: Part C - Multi Layer Perceptron with MNIST\n",
    "\n",
    "We assume that you have successfully completed CNTK 103 Part A.\n",
    "\n",
    "In this tutorial, we train a multi-layer perceptron on MNIST data. This notebook provides the recipe using Python APIs. If you are looking for this example in BrainScript, please look [here](https://github.com/Microsoft/CNTK/tree/v2.0/Examples/Image/GettingStarted).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Problem** \n",
    "As in CNTK 103B, we will continue to work on the same problem of recognizing digits in MNIST data. The MNIST data comprises of hand-written digits with little background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 1\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**:\n",
    "Our goal is to train a classifier that will identify the digits in the MNIST dataset. Additionally, we aspire to achieve a lower error rate with a Multi-layer perceptron compared to the Multi-class logistic regression from 103B. \n",
    "\n",
    "**Approach**:\n",
    "The same 5 stages we have used in the previous tutorial are applicable: Data reading, Data preprocessing, Creating a model, Learning the model parameters and Evaluating (a.k.a. testing/prediction) the model. \n",
    "- Data reading: We will use the CNTK Text reader \n",
    "- Data preprocessing: Covered in part A (suggested extension section). \n",
    "\n",
    "There is a substantial overlap with CNTK 102. However, in this tutorial we adjust the model to work on MNIST data with ten classes instead of the two classes we used in CNTK 102.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import cntk as C\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below, we check if we are running this notebook in the CNTK internal test machines by looking for environment variables defined there. We then select the right target device (GPU vs CPU) to test this notebook. In other cases, we use CNTK's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "In this section, we will read the data we downloaded in CNTK 103 Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the data dimensions\n",
    "input_dim = 784\n",
    "num_output_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "The downloaded MNIST dataset has 60,000 training images and 10,000 test images with each image being 28 x 28 pixels. Thus the number of features is equal to 784 (= 28 x 28 pixels), 1 per pixel. The variable `num_output_classes` is set to 10 corresponding to the number of digits (0-9) in the dataset.\n",
    "\n",
    "The data is in the following format:\n",
    "\n",
    "    |labels 0 0 0 0 0 0 0 1 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "    \n",
    "In this tutorial we are going to use labels corresponding to the integer stream \"labels\" and the image pixels corresponding to the integer stream named \"features\". We define a `create_reader` function to read the training and test data using the [CTF deserializer](https://cntk.ai/pythondocs/cntk.io.html?highlight=ctfdeserializer#cntk.io.CTFDeserializer). The labels are [1-hot encoded](https://en.wikipedia.org/wiki/One-hot). Refer to CNTK 103A tutorial for data format visualizations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is ..\\Examples\\Image\\DataSets\\MNIST\n"
     ]
    }
   ],
   "source": [
    "# Ensure the training and test data is generated and available for this tutorial.\n",
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "data_found = False\n",
    "for data_dir in [os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                 os.path.join(\"data\", \"MNIST\")]:\n",
    "    train_file = os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file = os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found = True\n",
    "        break\n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing CNTK 103 Part A\")\n",
    "print(\"Data directory is {0}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Model Creation'></a>\n",
    "## Model Creation\n",
    "\n",
    "Our multi-layer perceptron will be relatively simple with 2 hidden layers (`num_hidden_layers`). The number of nodes in the hidden layer is specified by `hidden_layers_dim`. The figure below illustrates the entire model we will use in this tutorial.\n",
    "\n",
    "![model-mlp](http://cntk.ai/jup/cntk103c_MNIST_MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with the terms *hidden_layer* and *number of hidden layers*, please refer back to the CNTK 102 tutorial.\n",
    "\n",
    "Each Dense layer (as illustrated below) shows the input dimensions, output dimensions, and the activation function that layer uses. Specifically, the layer below shows: input dimension = 784 (1 for each input pixel), output dimension = 400 (number of hidden nodes, a parameter specified by the user) and the activation function is [relu](https://cntk.ai/pythondocs/cntk.ops.html?highlight=relu#cntk.ops.relu).\n",
    "\n",
    "![model-dense](http://www.cntk.ai/jup/cntk103c_MNIST_dense.png)\n",
    "\n",
    "In this model we have two dense layers called the hidden layers each with a `relu` activation function and one output layer with no activation.  \n",
    "\n",
    "The output dimension (a.k.a. number of hidden nodes) in the two hidden layers is set to 400 and 200 in the illustration above. In the code below  we set both layers to have the same number of hidden nodes (400). \n",
    "\n",
    "The final output layer emits a vector of 10 values. Since we will be using softmax to normalize the output of the model, we do not use an activation function in this layer. The softmax operation comes bundled with the [loss function](https://cntk.ai/pythondocs/cntk.losses.html) we will be using later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden_layers = 2\n",
    "hidden_layers_dim = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network input and output: \n",
    "- **input** variable (a key CNTK concept): \n",
    ">An **input** variable is a container in which we put different observations. In this case, it contains image pixels. Thus, the shape of the `input` must match the shape of the data that will be provided.  For example, when data are images each of  height 10 pixels  and width 5 pixels, the input feature dimension will be 50 (representing the total number of image pixels). More on data and their dimensions will appear in separate tutorials.\n",
    "\n",
    "\n",
    "**Question** What is the input dimension of your chosen model? This is fundamental to our understanding of variables in a network or model representation in CNTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = C.input_variable(input_dim)\n",
    "label = C.input_variable(num_output_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron setup\n",
    "\n",
    "The cell below is a direct translation of the model illustration shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "            h = features\n",
    "            for _ in range(num_hidden_layers):\n",
    "                h = C.layers.Dense(hidden_layers_dim)(h)\n",
    "            r = C.layers.Dense(num_output_classes, activation = None)(h)\n",
    "            return r\n",
    "        \n",
    "z = create_model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`z` will be used to represent the output of the network.\n",
    "\n",
    "We introduced the sigmoid function in CNTK 102. In this tutorial you should try different activation functions in the hidden layer. You may choose to do this right away and take a peek into the performance later in the tutorial or run the preset tutorial and then choose to perform the suggested activity.\n",
    "\n",
    "\n",
    "** Suggested Activity **\n",
    "- Record the training error you get with `sigmoid` as the activation function\n",
    "- Now change to `relu` as the activation function and see if you can improve your training error\n",
    "\n",
    "*Quiz*: Name some of the different supported activation functions? Which activation function gives the smallest training error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the input to 0-1 range by dividing each pixel by 255.\n",
    "z = create_model(input/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning model parameters\n",
    "\n",
    "As in the previous tutorial, we use the `softmax` function to map the accumulated evidences or activations to a probability distribution over the classes (details of the [softmax function][]).\n",
    "\n",
    "[softmax function]: http://cntk.ai/pythondocs/cntk.ops.html#cntk.ops.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Similar to CNTK 102, we strive to minimize the cross-entropy between the label and predicted probability by the network. If this terminology sounds strange to you, please refer to the CNTK 102 for a refresher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = C.cross_entropy_with_softmax(z, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "To evaluate the classification, we compare the output of the network with the labels. The networks emits for each observation a vector of evidences (can be converted into probabilities using `softmax` functions) with dimension equal to the number of labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_error = C.classification_error(z, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training\n",
    "\n",
    "The trainer strives to reduce the `loss` function by different optimization approaches, [Stochastic Gradient Descent][] (`sgd`) being a basic one. Typically, one would start with random initialization of the model's weight parameters. The `sgd` optimizer  calculates the `loss` or error between the predicted label and the corresponding ground-truth label. It then generate a new set of the model's weight parameters in a single iteration using [gradient-decent][]. \n",
    "\n",
    "The aforementioned model parameter update using a single observation at a time is attractive since it does not require the entire data set (all observation) to be loaded in memory. In addition it requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the parameter updates that are generated by using a single observation sample at a time, can vary wildly between iterations. An intermediate approach is to load a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This approach is called *minibatch*.\n",
    "\n",
    "With minibatches, we often sample observations from the larger training dataset. We repeat the process of updating the model parameters by using different combinations of training samples. This will after a number of iterations minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after going through a preset number of minibatches, we claim that our model is trained.\n",
    "\n",
    "One of the key parameters for optimization is called the `learning_rate`. For now, we can think of it as a scaling factor that indicates how much we change the parameters in any iteration. We will be covering this parameter in more detail in a later tutorial. \n",
    "\n",
    "With this information, we are ready to create our trainer. \n",
    "\n",
    "[optimization]: https://en.wikipedia.org/wiki/Category:Convex_optimization\n",
    "[Stochastic Gradient Descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "[gradient-decent]: http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-with-Gradient-Descent.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the trainer object to drive the model training\n",
    "learning_rate = 0.2\n",
    "lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "learner = C.sgd(z.parameters, lr_schedule)\n",
    "trainer = C.Trainer(z, (loss, label_error), [learner])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create some helper functions to visualize the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A more efficient implementation of the moving average is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Run the trainer'></a>\n",
    "### Run the trainer\n",
    "\n",
    "We are now ready to train our fully connected neural net. We need to decide what data we want to feed into the training engine.\n",
    "\n",
    "In this example, each iteration of the optimizer will work on `minibatch_size` sized samples. We would like to train on all 60000 observations. Additionally we will make multiple passes through the data specified by the variable `num_sweeps_to_train_with`. With these parameters we can proceed with training our simple multi-layer perceptron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters for the trainer\n",
    "minibatch_size = 64\n",
    "num_samples_per_sweep = 60000\n",
    "num_sweeps_to_train_with = 10\n",
    "num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.3106, Error: 81.25%\n",
      "Minibatch: 500, Loss: 0.2775, Error: 9.38%\n",
      "Minibatch: 1000, Loss: 0.0982, Error: 1.56%\n",
      "Minibatch: 1500, Loss: 0.1353, Error: 6.25%\n",
      "Minibatch: 2000, Loss: 0.0090, Error: 0.00%\n",
      "Minibatch: 2500, Loss: 0.0369, Error: 1.56%\n",
      "Minibatch: 3000, Loss: 0.0227, Error: 0.00%\n",
      "Minibatch: 3500, Loss: 0.0496, Error: 1.56%\n",
      "Minibatch: 4000, Loss: 0.0170, Error: 0.00%\n",
      "Minibatch: 4500, Loss: 0.0124, Error: 0.00%\n",
      "Minibatch: 5000, Loss: 0.0065, Error: 0.00%\n",
      "Minibatch: 5500, Loss: 0.0048, Error: 0.00%\n",
      "Minibatch: 6000, Loss: 0.0032, Error: 0.00%\n",
      "Minibatch: 6500, Loss: 0.0226, Error: 0.00%\n",
      "Minibatch: 7000, Loss: 0.0146, Error: 0.00%\n",
      "Minibatch: 7500, Loss: 0.0040, Error: 0.00%\n",
      "Minibatch: 8000, Loss: 0.0039, Error: 0.00%\n",
      "Minibatch: 8500, Loss: 0.0019, Error: 0.00%\n",
      "Minibatch: 9000, Loss: 0.0004, Error: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Create the reader to training data set\n",
    "reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "\n",
    "# Map the data streams to the input and labels.\n",
    "input_map = {\n",
    "    label  : reader_train.streams.labels,\n",
    "    input  : reader_train.streams.features\n",
    "} \n",
    "\n",
    "# Run the trainer on and perform model training\n",
    "training_progress_output_freq = 500\n",
    "\n",
    "plotdata = {\"batchsize\":[], \"loss\":[], \"error\":[]}\n",
    "\n",
    "for i in range(0, int(num_minibatches_to_train)):\n",
    "    \n",
    "    # Read a mini batch from the training data file\n",
    "    data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    \n",
    "    trainer.train_minibatch(data)\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "    \n",
    "    if not (loss == \"NA\" or error ==\"NA\"):\n",
    "        plotdata[\"batchsize\"].append(batchsize)\n",
    "        plotdata[\"loss\"].append(loss)\n",
    "        plotdata[\"error\"].append(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the errors over the different training minibatches. Note that as we iterate the training loss decreases although we do see some intermediate bumps. \n",
    "\n",
    "So, using minibatches with `sgd` enables us to have good scalability while being performant for large data sets. There are advanced variants of the optimizer unique to CNTK that enable harnessing computational efficiency for real world data sets. This will be introduced in advanced tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAACgCAYAAAAPbNcqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n51xC/D/cidy33hD52gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ec6b543e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAACgCAYAAAAfIFuzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ec7ebfa8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the moving average loss to smooth out the noise in SGD\n",
    "plotdata[\"avgloss\"] = moving_average(plotdata[\"loss\"])\n",
    "plotdata[\"avgerror\"] = moving_average(plotdata[\"error\"])\n",
    "\n",
    "# Plot the training loss and the training error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"avgloss\"], 'b--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Minibatch run vs. Training loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"avgerror\"], 'r--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Label Prediction Error')\n",
    "plt.title('Minibatch run vs. Label Prediction Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation / Testing \n",
    "\n",
    "Now that we have trained the network, let us evaluate the trained network on the test data. This is done using `trainer.test_minibatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test error: 1.82%\n"
     ]
    }
   ],
   "source": [
    "# Read the training data\n",
    "reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "\n",
    "test_input_map = {\n",
    "    label  : reader_test.streams.labels,\n",
    "    input  : reader_test.streams.features,\n",
    "}\n",
    "\n",
    "# Test data for trained model\n",
    "test_minibatch_size = 512\n",
    "num_samples = 10000\n",
    "num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "test_result = 0.0\n",
    "\n",
    "for i in range(num_minibatches_to_test):\n",
    "    \n",
    "    # We are loading test data in batches specified by test_minibatch_size\n",
    "    # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "    # with one pixel per dimension that we will encode / decode with the \n",
    "    # trained model.\n",
    "    data = reader_test.next_minibatch(test_minibatch_size,\n",
    "                                      input_map = test_input_map)\n",
    "\n",
    "    eval_error = trainer.test_minibatch(data)\n",
    "    test_result = test_result + eval_error\n",
    "\n",
    "# Average of evaluation errors of all test minibatches\n",
    "print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this error is very comparable to our training error. This indicates that our model has a good \"out of sample\" error (a.k.a. generalization error). It implies that our model can very effectively deal with previously unseen observations (during the training process). This is key to avoid the phenomenon of overfitting.\n",
    "\n",
    "Also not the **huge** reduction in error compared to the multi-class LR from CNTK 103B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so far been dealing with aggregate measures of error. Let us now get the probabilities associated with individual data points. For each observation, the `eval` function returns the probability distribution across all ten classes. The classifier is trained to recognize digits, hence has 10 classes. First let us route the network output through a `softmax` function. This maps the aggregated activations across the network to probabilities over the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = C.softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sample a small minibatch from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data for evaluation\n",
    "reader_eval = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "\n",
    "eval_minibatch_size = 25\n",
    "eval_input_map = {input: reader_eval.streams.features} \n",
    "\n",
    "data = reader_test.next_minibatch(eval_minibatch_size, input_map = test_input_map)\n",
    "\n",
    "img_label = data[label].asarray()\n",
    "img_data = data[input].asarray()\n",
    "predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the index with the maximum value for both predicted as well as the ground truth\n",
    "pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label    : [4, 5, 6, 7, 8, 9, 7, 4, 6, 1, 4, 0, 9, 9, 3, 7, 8, 4, 7, 5, 8, 5, 3, 2, 2]\n",
      "Predicted: [4, 6, 6, 7, 8, 9, 7, 4, 6, 1, 4, 0, 9, 9, 3, 7, 8, 0, 7, 5, 8, 5, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Label    :\", gtlabel[:25])\n",
    "print(\"Predicted:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxf89MQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ec6e00c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a random image\n",
    "sample_number = 2\n",
    "plt.imshow(img_data[sample_number].reshape(28,28), cmap=\"gray_r\")\n",
    "plt.axis('off')\n",
    "\n",
    "img_gt, img_pred = gtlabel[sample_number], pred[sample_number]\n",
    "print(\"Image Label: \", img_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exploration Suggestion**\n",
    "-  Try exploring how the classifier behaves with different parameters - suggest changing the `minibatch_size` parameter from 25 to say 64 or 128. What happens to the error rate? How does the error compare to the logistic regression classifier?\n",
    "- Suggest trying to increase the number of sweeps\n",
    "- Can you change the network to reduce the training error rate? When do you see *overfitting* happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Code link\n",
    "\n",
    "If you want to try running the tutorial from Python command prompt please run the [SimpleMNIST.py](https://github.com/Microsoft/CNTK/tree/v2.0/Examples/Image/Classification/MLP/Python) example."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
